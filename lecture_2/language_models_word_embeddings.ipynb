{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# –õ–µ–∫—Ü–∏—è 2: –ï–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –Ω–∞ –¥—É–º–∏\n",
    "\n",
    "## –û—Ç –¥—É–º–∏ –¥–æ –∑–Ω–∞—á–µ–Ω–∏–µ: N-–≥—Ä–∞–º–∏, –¥–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –∏ Word2Vec\n",
    "\n",
    "**–ü—Ä–æ–¥—ä–ª–∂–∏—Ç–µ–ª–Ω–æ—Å—Ç:** 2-2.5 —á–∞—Å–∞  \n",
    "**–ü—Ä–µ–¥–ø–æ—Å—Ç–∞–≤–∫–∏:** –õ–µ–∫—Ü–∏—è 1 (ML –æ—Å–Ω–æ–≤–∏, Bag-of-Words, –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)  \n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (BPE, WordPiece, subword –∞–ª–≥–æ—Ä–∏—Ç–º–∏)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives",
   "metadata": {},
   "source": [
    "---\n",
    "## –¶–µ–ª–∏ –Ω–∞ –ª–µ–∫—Ü–∏—è—Ç–∞\n",
    "\n",
    "–°–ª–µ–¥ —Ç–∞–∑–∏ –ª–µ–∫—Ü–∏—è —â–µ –º–æ–∂–µ—Ç–µ:\n",
    "\n",
    "- –û–±—è—Å–Ω—è–≤–∞—Ç–µ –∫–∞–∫–≤–æ –µ –µ–∑–∏–∫–æ–≤ –º–æ–¥–µ–ª –∏ –∑–∞—â–æ –µ –≤–∞–∂–µ–Ω –∑–∞ NLP\n",
    "- –ò–∑–≥—Ä–∞–∂–¥–∞—Ç–µ N-–≥—Ä–∞–º –º–æ–¥–µ–ª –∏ –≥–µ–Ω–µ—Ä–∏—Ä–∞—Ç–µ —Ç–µ–∫—Å—Ç —Å –Ω–µ–≥–æ\n",
    "- –ò–∑—á–∏—Å–ª—è–≤–∞—Ç–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è (perplexity) –∑–∞ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –µ–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏\n",
    "- –†–∞–∑–±–∏—Ä–∞—Ç–µ –¥–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–∞—Ç–∞ —Ö–∏–ø–æ—Ç–µ–∑–∞ –∏ co-occurrence –º–∞—Ç—Ä–∏—Ü–∏\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞—Ç–µ Word2Vec –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ —Å—Ö–æ–¥—Å—Ç–≤–æ –∏ –∞–Ω–∞–ª–æ–≥–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roadmap",
   "metadata": {},
   "source": [
    "### –ü—ä—Ç–Ω–∞ –∫–∞—Ä—Ç–∞ –∑–∞ –¥–Ω–µ—Å\n",
    "\n",
    "```\n",
    "BoW –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è ‚Üí WordNet ‚Üí N-–≥—Ä–∞–º–∏ ‚Üí Perplexity ‚Üí Co-occurrence ‚Üí Word2Vec ‚Üí –ü—Ä–∞–∫—Ç–∏–∫–∞\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# –°–≤–∞–ª—è–º–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–∏—Ç–µ –¥–∞–Ω–Ω–∏\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"‚úì NLTK –¥–∞–Ω–Ω–∏—Ç–µ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim –∑–∞ Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sklearn –∑–∞ PCA –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì –í—Å–∏—á–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏ —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. –û—Ç Bag-of-Words –∫—ä–º –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "\n",
    "### –ö–∞–∫–≤–æ –Ω–∞—É—á–∏—Ö–º–µ –≤ –õ–µ–∫—Ü–∏—è 1?\n",
    "\n",
    "**Bag-of-Words (BoW)** –ø—Ä–µ–¥—Å—Ç–∞–≤—è —Ç–µ–∫—Å—Ç –∫–∞—Ç–æ –≤–µ–∫—Ç–æ—Ä –æ—Ç —á–µ—Å—Ç–æ—Ç–∏ –Ω–∞ –¥—É–º–∏.\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∏:**\n",
    "- –ì—É–±–∏ —Ä–µ–¥–∞ –Ω–∞ –¥—É–º–∏—Ç–µ\n",
    "- –ù—è–º–∞ –ø–æ–Ω—è—Ç–∏–µ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ —Å—Ö–æ–¥—Å—Ç–≤–æ\n",
    "- –í–∏—Å–æ–∫–æ–º–µ—Ä–Ω–∏, —Ä–∞–∑—Ä–µ–¥–µ–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bow-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–ø–æ–º–Ω—è–Ω–µ: BoW –≥—É–±–∏ —Ä–µ–¥ –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\n",
    "    \"king rules the kingdom\",\n",
    "    \"queen rules the kingdom\",\n",
    "    \"car drives on the road\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print(\"üìä Bag-of-Words –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ:\\n\")\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out(), index=sentences)\n",
    "print(df.to_string())\n",
    "\n",
    "print(\"\\nüí° –ü—Ä–æ–±–ª–µ–º: 'king' –∏ 'queen' –∏–∑–≥–ª–µ–∂–¥–∞—Ç –Ω–∞–ø—ä–ª–Ω–æ —Ä–∞–∑–ª–∏—á–Ω–∏,\")\n",
    "print(\"   –Ω–æ 'king' –∏ 'car' - —Å—ä—â–æ. –ù—è–º–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∞ –≤—Ä—ä–∑–∫–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-question",
   "metadata": {},
   "source": [
    "### –¶–µ–Ω—Ç—Ä–∞–ª–µ–Ω –≤—ä–ø—Ä–æ—Å: –ö–∞–∫–≤–æ –æ–∑–Ω–∞—á–∞–≤–∞ –¥—É–º–∏—Ç–µ –¥–∞ –∏–º–∞—Ç \"–∑–Ω–∞—á–µ–Ω–∏–µ\"?\n",
    "\n",
    "**–¢—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞:**\n",
    "\n",
    "| –ü–æ–¥—Ö–æ–¥ | –ò–¥–µ—è | –ü—Ä–∏–º–µ—Ä |\n",
    "|--------|------|--------|\n",
    "| **–°–∏–º–≤–æ–ª–µ–Ω** | –ï–∫—Å–ø–µ—Ä—Ç–Ω–∏ –±–∞–∑–∏ –∑–Ω–∞–Ω–∏—è | WordNet: king IS-A ruler |\n",
    "| **–î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–µ–Ω** | –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞ —Å—ä–≤–º–µ—Å—Ç–Ω–∞ –ø–æ—è–≤–∞ | king —á–µ—Å—Ç–æ —Å–µ —Å—Ä–µ—â–∞ —Å crown, throne |\n",
    "| **–î–∏—Å—Ç—Ä–∏–±—É–∏—Ä–∞–Ω** | –ü–ª—ä—Ç–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏ | king = [0.2, -0.5, 0.8, ...] |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. –°–∏–º–≤–æ–ª–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è: WordNet\n",
    "\n",
    "### –ö–∞–∫–≤–æ –µ WordNet?\n",
    "\n",
    "**WordNet** –µ –ª–µ–∫—Å–∏–∫–∞–ª–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–Ω–∏, –æ—Ä–≥–∞–Ω–∏–∑–∏—Ä–∞–Ω–∞ –æ–∫–æ–ª–æ **synsets** (synonym sets).\n",
    "\n",
    "–í—Å–µ–∫–∏ synset –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–≤–∞ –µ–¥–Ω–æ **–ø–æ–Ω—è—Ç–∏–µ** –∏ —Å—ä–¥—ä—Ä–∂–∞ —Å–∏–Ω–æ–Ω–∏–º–∏ –∑–∞ –Ω–µ–≥–æ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wordnet-synsets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: Synsets –∑–∞ –¥—É–º–∞—Ç–∞ \"dog\"\n",
    "print(\"üêï Synsets –∑–∞ –¥—É–º–∞—Ç–∞ 'dog':\\n\")\n",
    "\n",
    "dog_synsets = wn.synsets('dog')\n",
    "for i, synset in enumerate(dog_synsets[:5]):  # –ü–æ–∫–∞–∑–≤–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ 5\n",
    "    print(f\"{i+1}. {synset.name()}\")\n",
    "    print(f\"   –î–µ—Ñ–∏–Ω–∏—Ü–∏—è: {synset.definition()}\")\n",
    "    print(f\"   –ü—Ä–∏–º–µ—Ä–∏: {synset.examples()[:2]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wordnet-relations",
   "metadata": {},
   "source": [
    "### –°–µ–º–∞–Ω—Ç–∏—á–Ω–∏ —Ä–µ–ª–∞—Ü–∏–∏ –≤ WordNet\n",
    "\n",
    "| –†–µ–ª–∞—Ü–∏—è | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–º–µ—Ä |\n",
    "|---------|----------|--------|\n",
    "| **Synonyms** | –î—É–º–∏ —Å—ä—Å —Å—ä—â–æ—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ | {car, automobile, motorcar} |\n",
    "| **Hypernyms** | –ü–æ-–æ–±—â–æ –ø–æ–Ω—è—Ç–∏–µ (IS-A) | dog ‚Üí canine ‚Üí mammal |\n",
    "| **Hyponyms** | –ü–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –ø–æ–Ω—è—Ç–∏–µ | dog ‚Üí beagle, poodle |\n",
    "| **Meronyms** | –ß–∞—Å—Ç –æ—Ç —Ü—è–ª–æ (HAS-A) | car has wheel, engine |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wordnet-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: –ô–µ—Ä–∞—Ä—Ö–∏—è –Ω–∞ –ø–æ–Ω—è—Ç–∏—è\n",
    "dog = wn.synset('dog.n.01')\n",
    "\n",
    "print(f\"üìä –ô–µ—Ä–∞—Ä—Ö–∏—è –∑–∞ '{dog.name()}':\\n\")\n",
    "\n",
    "# Hypernyms (–ø–æ-–æ–±—â–∏ –ø–æ–Ω—è—Ç–∏—è)\n",
    "print(\"‚¨ÜÔ∏è Hypernyms (–ø–æ-–æ–±—â–∏):\")\n",
    "for hypernym in dog.hypernym_paths()[0][::-1]:  # –û—Ç –Ω–∞–π-–æ–±—â–æ—Ç–æ –∫—ä–º dog\n",
    "    print(f\"   ‚Üí {hypernym.name()}: {hypernym.definition()[:50]}...\")\n",
    "\n",
    "print(\"\\n‚¨áÔ∏è Hyponyms (–ø–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏):\")\n",
    "for hyponym in dog.hyponyms()[:5]:\n",
    "    print(f\"   ‚Üí {hyponym.name()}: {hyponym.definition()[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wordnet-similarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑–º–µ—Ä–≤–∞–Ω–µ –Ω–∞ —Å—Ö–æ–¥—Å—Ç–≤–æ –≤ WordNet\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "car = wn.synset('car.n.01')\n",
    "\n",
    "print(\"üìè –°—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ø–æ–Ω—è—Ç–∏—è (Wu-Palmer similarity):\\n\")\n",
    "\n",
    "pairs = [(dog, cat), (dog, car), (cat, car)]\n",
    "for s1, s2 in pairs:\n",
    "    sim = s1.wup_similarity(s2)\n",
    "    print(f\"   {s1.name()} ‚Üî {s2.name()}: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nüí° dog –∏ cat —Å–∞ –ø–æ-—Å—Ö–æ–¥–Ω–∏ (–∏ –¥–≤–µ—Ç–µ —Å–∞ –∂–∏–≤–æ—Ç–Ω–∏),\")\n",
    "print(\"   –¥–æ–∫–∞—Ç–æ car –µ –æ—Ç —Å—ä–≤—Å–µ–º –¥—Ä—É–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wordnet-limitations",
   "metadata": {},
   "source": [
    "### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ WordNet\n",
    "\n",
    "- **–†—ä—á–Ω–æ —Å—ä–∑–¥–∞–¥–µ–Ω** ‚Üí —Å–∫—ä–ø –∑–∞ –ø–æ–¥–¥—Ä—ä–∂–∫–∞, –Ω–µ–ø—ä–ª–µ–Ω\n",
    "- **–õ–∏–ø—Å–≤–∞—Ç –Ω–æ–≤–∏ –¥—É–º–∏:** \"selfie\", \"bitcoin\", \"COVID\"\n",
    "- **–°—É–±–µ–∫—Ç–∏–≤–µ–Ω:** –≥—Ä–∞–Ω–∏—Ü–∏ –º–µ–∂–¥—É –ø–æ–Ω—è—Ç–∏—è—Ç–∞ —Å–∞ —Ä–∞–∑–º–∏—Ç–∏\n",
    "- **–î–∏—Å–∫—Ä–µ—Ç–Ω–∏ —Ä–µ–ª–∞—Ü–∏–∏:** –∏–ª–∏ –µ —Å–∏–Ω–æ–Ω–∏–º, –∏–ª–∏ –Ω–µ –µ\n",
    "- **–ë–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç:** \"bank\" = —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞ –∏–ª–∏ —Ä–µ—á–Ω–∞?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wordnet-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: WordNet –Ω–µ –ø–æ–∑–Ω–∞–≤–∞ –Ω–æ–≤–∏ –¥—É–º–∏\n",
    "new_words = ['selfie', 'bitcoin', 'emoji', 'hashtag']\n",
    "\n",
    "print(\"‚ùå –ù–æ–≤–∏ –¥—É–º–∏, –∫–æ–∏—Ç–æ WordNet –Ω–µ –ø–æ–∑–Ω–∞–≤–∞:\\n\")\n",
    "for word in new_words:\n",
    "    synsets = wn.synsets(word)\n",
    "    status = \"‚úì –Ω–∞–º–µ—Ä–µ–Ω\" if synsets else \"‚úó –ª–∏–ø—Å–≤–∞\"\n",
    "    print(f\"   {word}: {status}\")\n",
    "\n",
    "print(\"\\nüí° WordNet —Å–µ –æ–±–Ω–æ–≤—è–≤–∞ –±–∞–≤–Ω–æ. –ï–∑–∏–∫—ä—Ç —Å–µ –ø—Ä–æ–º–µ–Ω—è –ø–æ-–±—ä—Ä–∑–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –µ–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏: N-–≥—Ä–∞–º–∏\n",
    "\n",
    "### –ö–∞–∫–≤–æ –µ –µ–∑–∏–∫–æ–≤ –º–æ–¥–µ–ª?\n",
    "\n",
    "**–ï–∑–∏–∫–æ–≤ –º–æ–¥–µ–ª** –ø—Ä–∏—Å–≤–æ—è–≤–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç –Ω–∞ –ø–æ—Ä–µ–¥–∏—Ü–∞ –æ—Ç –¥—É–º–∏.\n",
    "\n",
    "$$P(w_1, w_2, ..., w_n)$$\n",
    "\n",
    "**–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è:**\n",
    "- –†–∞–∑–ø–æ–∑–Ω–∞–≤–∞–Ω–µ –Ω–∞ —Ä–µ—á: \"recognize speech\" vs \"wreck a nice beach\"\n",
    "- –ú–∞—à–∏–Ω–µ–Ω –ø—Ä–µ–≤–æ–¥: –∫–æ–π –ø—Ä–µ–≤–æ–¥ –µ –ø–æ-–≤–µ—Ä–æ—è—Ç–µ–Ω?\n",
    "- –ê–≤—Ç–æ–∫–æ–º–ø–ª–∏–π—Ç: –ø—Ä–µ–¥—Å–∫–∞–∂–∏ —Å–ª–µ–¥–≤–∞—â–∞—Ç–∞ –¥—É–º–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chain-rule",
   "metadata": {},
   "source": [
    "### –í–µ—Ä–∏–∂–Ω–æ –ø—Ä–∞–≤–∏–ª–æ –∑–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "\n",
    "**–ò–Ω—Ç—É–∏—Ü–∏—è:** –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ –Ω–∞ –∏–∑—Ä–µ—á–µ–Ω–∏–µ = –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –Ω–∞ —É—Å–ª–æ–≤–Ω–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞:**\n",
    "$$P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i | w_1, w_2, ..., w_{i-1})$$\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º:** –¢–≤—ä—Ä–¥–µ –º–Ω–æ–≥–æ –≤—ä–∑–º–æ–∂–Ω–∏ –∏—Å—Ç–æ—Ä–∏–∏! –ö–∞–∫ –¥–∞ –æ—Ü–µ–Ω–∏–º $P(w_5 | w_1, w_2, w_3, w_4)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markov-assumption",
   "metadata": {},
   "source": [
    "### N-–≥—Ä–∞–º –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è (–ú–∞—Ä–∫–æ–≤—Å–∫–æ –¥–æ–ø—É—Å–∫–∞–Ω–µ)\n",
    "\n",
    "**–ò–¥–µ—è:** –ü—Ä–∏–µ–º–∞–º–µ, —á–µ –¥—É–º–∞—Ç–∞ –∑–∞–≤–∏—Å–∏ —Å–∞–º–æ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ N-1 –¥—É–º–∏.\n",
    "\n",
    "| –ú–æ–¥–µ–ª | –ê–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è | –ü—Ä–∏–º–µ—Ä |\n",
    "|-------|--------------|--------|\n",
    "| **Unigram** | $P(w_i)$ | –î—É–º–∞—Ç–∞ –Ω–µ –∑–∞–≤–∏—Å–∏ –æ—Ç –Ω–∏—â–æ |\n",
    "| **Bigram** | $P(w_i | w_{i-1})$ | –°–∞–º–æ –ø—Ä–µ–¥–∏—à–Ω–∞—Ç–∞ –¥—É–º–∞ |\n",
    "| **Trigram** | $P(w_i | w_{i-2}, w_{i-1})$ | –ü–æ—Å–ª–µ–¥–Ω–∏—Ç–µ 2 –¥—É–º–∏ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä–µ–Ω –∫–æ—Ä–ø—É—Å –∑–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
    "corpus = \"\"\"\n",
    "I love natural language processing.\n",
    "I love machine learning.\n",
    "Natural language processing is fascinating.\n",
    "Machine learning powers modern AI.\n",
    "I study artificial intelligence.\n",
    "Language models are powerful.\n",
    "I love language models.\n",
    "Natural language is complex.\n",
    "\"\"\"\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ preprocessing\n",
    "def tokenize(text):\n",
    "    \"\"\"Simple tokenization with start/end tokens.\"\"\"\n",
    "    sentences = sent_tokenize(text.lower())\n",
    "    tokenized = []\n",
    "    for sent in sentences:\n",
    "        tokens = ['<s>'] + word_tokenize(sent) + ['</s>']\n",
    "        tokenized.append(tokens)\n",
    "    return tokenized\n",
    "\n",
    "sentences = tokenize(corpus)\n",
    "print(\"üìö –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–∞–Ω –∫–æ—Ä–ø—É—Å (–ø—ä—Ä–≤–∏—Ç–µ 3 –∏–∑—Ä–µ—á–µ–Ω–∏—è):\\n\")\n",
    "for sent in sentences[:3]:\n",
    "    print(f\"   {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mle-estimation",
   "metadata": {},
   "source": [
    "### –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: Maximum Likelihood\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞ –∑–∞ bigram:**\n",
    "$$P(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$$\n",
    "\n",
    "**–ò–Ω—Ç—É–∏—Ü–∏—è:** –ö–æ–ª–∫–æ –ø—ä—Ç–∏ $w_i$ —Å–ª–µ–¥–≤–∞ $w_{i-1}$, –¥–µ–ª–µ–Ω–æ –Ω–∞ –æ–±—â–∏—è –±—Ä–æ–π –Ω–∞ $w_{i-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigram-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑–≥—Ä–∞–∂–¥–∞–Ω–µ –Ω–∞ bigram –º–æ–¥–µ–ª\n",
    "class BigramModel:\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.unigram_counts = Counter()\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w1, w2 = sentence[i], sentence[i+1]\n",
    "                self.bigram_counts[w1][w2] += 1\n",
    "                self.unigram_counts[w1] += 1\n",
    "    \n",
    "    def probability(self, w2, w1):\n",
    "        \"\"\"P(w2 | w1)\"\"\"\n",
    "        if self.unigram_counts[w1] == 0:\n",
    "            return 0\n",
    "        return self.bigram_counts[w1][w2] / self.unigram_counts[w1]\n",
    "\n",
    "# –û–±—É—á–∞–≤–∞–º–µ –º–æ–¥–µ–ª–∞\n",
    "bigram_model = BigramModel()\n",
    "bigram_model.train(sentences)\n",
    "\n",
    "print(\"‚úì Bigram –º–æ–¥–µ–ª –æ–±—É—á–µ–Ω!\")\n",
    "print(f\"   –£–Ω–∏–∫–∞–ª–Ω–∏ unigrams: {len(bigram_model.unigram_counts)}\")\n",
    "print(f\"   –£–Ω–∏–∫–∞–ª–Ω–∏ bigram –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏: {len(bigram_model.bigram_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigram-probabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–∫–∞–∑–≤–∞–º–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥ –¥–∞–¥–µ–Ω–∞ –¥—É–º–∞\n",
    "context = 'i'\n",
    "print(f\"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥ –¥—É–º–∞—Ç–∞ '{context}':\\n\")\n",
    "\n",
    "probs = bigram_model.bigram_counts[context]\n",
    "total = bigram_model.unigram_counts[context]\n",
    "\n",
    "for word, count in probs.most_common(5):\n",
    "    prob = count / total\n",
    "    bar = '‚ñà' * int(prob * 20)\n",
    "    print(f\"   P({word:12s} | {context}) = {prob:.3f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigram-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: Bigram –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–∞—Ç–æ heatmap\n",
    "# –ò–∑–±–∏—Ä–∞–º–µ –Ω—è–∫–æ–ª–∫–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∏ –¥—É–º–∏\n",
    "focus_words = ['<s>', 'i', 'love', 'natural', 'language', 'machine']\n",
    "\n",
    "# –°—ä–∑–¥–∞–≤–∞–º–µ –º–∞—Ç—Ä–∏—Ü–∞ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "prob_matrix = np.zeros((len(focus_words), len(focus_words)))\n",
    "for i, w1 in enumerate(focus_words):\n",
    "    for j, w2 in enumerate(focus_words):\n",
    "        prob_matrix[i, j] = bigram_model.probability(w2, w1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(prob_matrix, xticklabels=focus_words, yticklabels=focus_words,\n",
    "            annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.xlabel('–°–ª–µ–¥–≤–∞—â–∞ –¥—É–º–∞ (w‚ÇÇ)')\n",
    "plt.ylabel('–ö–æ–Ω—Ç–µ–∫—Å—Ç (w‚ÇÅ)')\n",
    "plt.title('Bigram –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ P(w‚ÇÇ | w‚ÇÅ)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ù–µ–Ω—É–ª–µ–≤–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ –ø–æ–∫–∞–∑–≤–∞—Ç –Ω–∞–±–ª—é–¥–∞–≤–∞–Ω–∏ bigram –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ —Ç–µ–∫—Å—Ç —Å bigram –º–æ–¥–µ–ª\n",
    "def generate_text(model, max_length=15):\n",
    "    current = '<s>'\n",
    "    result = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # –í–∑–µ–º–∞–º–µ –≤—ä–∑–º–æ–∂–Ω–∏—Ç–µ —Å–ª–µ–¥–≤–∞—â–∏ –¥—É–º–∏\n",
    "        candidates = model.bigram_counts[current]\n",
    "        if not candidates:\n",
    "            break\n",
    "        \n",
    "        # –°—É–º–ø–ª–∏—Ä–∞–º–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–Ω–æ –Ω–∞ —á–µ—Å—Ç–æ—Ç–∏—Ç–µ\n",
    "        words = list(candidates.keys())\n",
    "        counts = list(candidates.values())\n",
    "        probs = np.array(counts) / sum(counts)\n",
    "        \n",
    "        next_word = np.random.choice(words, p=probs)\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        result.append(next_word)\n",
    "        current = next_word\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "print(\"üé≤ –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω —Ç–µ–∫—Å—Ç –æ—Ç bigram –º–æ–¥–µ–ª:\\n\")\n",
    "np.random.seed(42)\n",
    "for i in range(5):\n",
    "    text = generate_text(bigram_model)\n",
    "    print(f\"   {i+1}. {text}\")\n",
    "\n",
    "print(\"\\nüí° –ú–æ–¥–µ–ª—ä—Ç –≥–µ–Ω–µ—Ä–∏—Ä–∞ –≥—Ä–∞–º–∞—Ç–∏—á–Ω–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏, –Ω–æ –Ω—è–º–∞ –¥—ä–ª–≥–∞ –∫–æ—Ö–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ngram-problems",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–±–ª–µ–º–∏ —Å N-–≥—Ä–∞–º –º–æ–¥–µ–ª–∏\n",
    "\n",
    "**1. Sparsity (—Ä–∞–∑—Ä–µ–¥–µ–Ω–æ—Å—Ç):**\n",
    "- –ú–Ω–æ–≥–æ N-–≥—Ä–∞–º–∏ –∏–º–∞—Ç –Ω—É–ª–µ–≤–∏ —á–µ—Å—Ç–æ—Ç–∏\n",
    "- $P(\\text{language} | \\text{artificial}) = 0$ –∞–∫–æ –Ω–µ –µ –≤ –∫–æ—Ä–ø—É—Å–∞\n",
    "\n",
    "**2. Storage (–ø–∞–º–µ—Ç):**\n",
    "- $V^N$ –≤—ä–∑–º–æ–∂–Ω–∏ N-–≥—Ä–∞–º–∏ (V = —Ä–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞)\n",
    "\n",
    "**3. –ë–µ–∑ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è:**\n",
    "- –ù–µ –∑–Ω–∞–µ, —á–µ \"dog\" –∏ \"puppy\" —Å–∞ —Å—Ö–æ–¥–Ω–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sparsity-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: Sparsity –ø—Ä–æ–±–ª–µ–º\n",
    "test_bigrams = [\n",
    "    ('i', 'love'),           # –í–∏–∂–¥–∞–Ω\n",
    "    ('language', 'models'),  # –í–∏–∂–¥–∞–Ω\n",
    "    ('artificial', 'language'),  # –ù–ï –≤–∏–∂–¥–∞–Ω\n",
    "    ('deep', 'learning'),    # –ù–ï –≤–∏–∂–¥–∞–Ω\n",
    "]\n",
    "\n",
    "print(\"‚ùì Sparsity –ø—Ä–æ–±–ª–µ–º: –Ω—É–ª–µ–≤–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\\n\")\n",
    "for w1, w2 in test_bigrams:\n",
    "    prob = bigram_model.probability(w2, w1)\n",
    "    status = \"‚úì\" if prob > 0 else \"‚úó\"\n",
    "    print(f\"   {status} P({w2} | {w1}) = {prob:.3f}\")\n",
    "\n",
    "print(\"\\nüí° –ù–µ–≤–∏–∂–¥–∞–Ω–∏ bigrams –∏–º–∞—Ç P=0. –¢–æ–≤–∞ –µ –ø—Ä–æ–±–ª–µ–º!\")\n",
    "print(\"   –†–µ—à–µ–Ω–∏–µ: smoothing (Laplace, Kneser-Ney, ...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laplace-smoothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add-one (Laplace) smoothing\n",
    "def laplace_probability(model, w2, w1, vocab_size):\n",
    "    \"\"\"P(w2 | w1) with Laplace smoothing.\"\"\"\n",
    "    count = model.bigram_counts[w1][w2]\n",
    "    total = model.unigram_counts[w1]\n",
    "    return (count + 1) / (total + vocab_size)\n",
    "\n",
    "vocab_size = len(bigram_model.unigram_counts)\n",
    "\n",
    "print(\"‚ú® –° Laplace smoothing:\\n\")\n",
    "for w1, w2 in test_bigrams:\n",
    "    prob_raw = bigram_model.probability(w2, w1)\n",
    "    prob_smooth = laplace_probability(bigram_model, w2, w1, vocab_size)\n",
    "    print(f\"   P({w2:10s} | {w1:10s}): raw={prob_raw:.3f}, smoothed={prob_smooth:.3f}\")\n",
    "\n",
    "print(\"\\nüí° Smoothing –≥–∞—Ä–∞–Ω—Ç–∏—Ä–∞ –Ω–µ–Ω—É–ª–µ–≤–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –Ω–æ –µ –≥—Ä—É–±–æ —Ä–µ—à–µ–Ω–∏–µ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. –û—Ü–µ–Ω–∫–∞ –Ω–∞ –µ–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏: Perplexity\n",
    "\n",
    "### –ö–∞–∫–≤–æ –µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è?\n",
    "\n",
    "**–ò–Ω—Ç—É–∏—Ü–∏—è:** –ö–æ–ª–∫–æ \"–∏–∑–Ω–µ–Ω–∞–¥–∞–Ω\" –µ –º–æ–¥–µ–ª—ä—Ç –æ—Ç —Ç–µ—Å—Ç–æ–≤–∏—Ç–µ –¥–∞–Ω–Ω–∏?\n",
    "\n",
    "**–ü–æ-–Ω–∏—Å–∫–∞ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è = –ø–æ-–¥–æ–±—ä—Ä –º–æ–¥–µ–ª** (–ø–æ-–º–∞–ª–∫–æ –∏–∑–Ω–µ–Ω–∞–¥–∞–Ω)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perplexity-formula",
   "metadata": {},
   "source": [
    "### –§–æ—Ä–º—É–ª–∞ –∑–∞ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è\n",
    "\n",
    "**Cross-entropy:**\n",
    "$$H = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_1, ..., w_{i-1})$$\n",
    "\n",
    "**Perplexity:**\n",
    "$$PP = \\exp(H) = P(w_1, ..., w_N)^{-1/N}$$\n",
    "\n",
    "**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:** –ü–µ—Ä–ø–ª–µ–∫—Å–∏—è—Ç–∞ –µ \"–µ—Ñ–µ–∫—Ç–∏–≤–µ–Ω branching factor\" ‚Äî —Å—Ä–µ–¥–Ω–æ –∫–æ–ª–∫–æ –¥—É–º–∏ —Å–∞ —Ä–∞–≤–Ω–æ–≤–µ—Ä–æ—è—Ç–Ω–∏ –Ω–∞ –≤—Å—è–∫–∞ –ø–æ–∑–∏—Ü–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perplexity-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è\n",
    "def compute_perplexity(model, sentences, vocab_size, use_smoothing=True):\n",
    "    \"\"\"Compute perplexity of bigram model on test sentences.\"\"\"\n",
    "    log_prob_sum = 0\n",
    "    n_tokens = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence) - 1):\n",
    "            w1, w2 = sentence[i], sentence[i+1]\n",
    "            \n",
    "            if use_smoothing:\n",
    "                prob = laplace_probability(model, w2, w1, vocab_size)\n",
    "            else:\n",
    "                prob = model.probability(w2, w1)\n",
    "                if prob == 0:\n",
    "                    prob = 1e-10  # Avoid log(0)\n",
    "            \n",
    "            log_prob_sum += np.log(prob)\n",
    "            n_tokens += 1\n",
    "    \n",
    "    cross_entropy = -log_prob_sum / n_tokens\n",
    "    perplexity = np.exp(cross_entropy)\n",
    "    return perplexity\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è\n",
    "test_corpus = \"\"\"\n",
    "I love artificial intelligence.\n",
    "Machine learning is powerful.\n",
    "\"\"\"\n",
    "test_sentences = tokenize(test_corpus)\n",
    "\n",
    "ppl = compute_perplexity(bigram_model, test_sentences, vocab_size)\n",
    "print(f\"üìä –ü–µ—Ä–ø–ª–µ–∫—Å–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤ –∫–æ—Ä–ø—É—Å: {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perplexity-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: in-domain vs out-of-domain —Ç–µ–∫—Å—Ç\n",
    "in_domain = \"I love natural language processing.\"\n",
    "out_domain = \"The cat sat on the mat.\"\n",
    "\n",
    "in_domain_ppl = compute_perplexity(bigram_model, tokenize(in_domain), vocab_size)\n",
    "out_domain_ppl = compute_perplexity(bigram_model, tokenize(out_domain), vocab_size)\n",
    "\n",
    "print(\"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è:\\n\")\n",
    "print(f\"   In-domain:  '{in_domain}'\")\n",
    "print(f\"   Perplexity: {in_domain_ppl:.2f}\\n\")\n",
    "print(f\"   Out-domain: '{out_domain}'\")\n",
    "print(f\"   Perplexity: {out_domain_ppl:.2f}\")\n",
    "\n",
    "print(\"\\nüí° –ú–æ–¥–µ–ª—ä—Ç –µ –ø–æ-'–∏–∑–Ω–µ–Ω–∞–¥–∞–Ω' –æ—Ç out-of-domain —Ç–µ–∫—Å—Ç (–ø–æ-–≤–∏—Å–æ–∫–∞ PP).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perplexity-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: –ü–µ—Ä–ø–ª–µ–∫—Å–∏—è –∫–∞—Ç–æ \"branching factor\"\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# –õ—è–≤–∞ –≥—Ä–∞—Ñ–∏–∫–∞: –ö–∞–∫–≤–æ –æ–∑–Ω–∞—á–∞–≤–∞ PP\n",
    "pp_values = [2, 10, 50, 100, 1000]\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(pp_values)))\n",
    "\n",
    "axes[0].barh(range(len(pp_values)), pp_values, color=colors)\n",
    "axes[0].set_yticks(range(len(pp_values)))\n",
    "axes[0].set_yticklabels([f'PP={pp}' for pp in pp_values])\n",
    "axes[0].set_xlabel('–ï—Ñ–µ–∫—Ç–∏–≤–µ–Ω branching factor')\n",
    "axes[0].set_title('–ü–µ—Ä–ø–ª–µ–∫—Å–∏—è: –∫–æ–ª–∫–æ –¥—É–º–∏ —Å–∞ \"—Ä–∞–≤–Ω–æ–≤–µ—Ä–æ—è—Ç–Ω–∏\"?')\n",
    "\n",
    "# –î—è—Å–Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∞: Train vs Test PP\n",
    "train_ppl = compute_perplexity(bigram_model, sentences, vocab_size)\n",
    "test_ppl = compute_perplexity(bigram_model, test_sentences, vocab_size)\n",
    "\n",
    "bars = axes[1].bar(['Train corpus', 'Test corpus'], [train_ppl, test_ppl],\n",
    "                   color=['steelblue', 'coral'])\n",
    "axes[1].set_ylabel('Perplexity')\n",
    "axes[1].set_title('Train vs Test Perplexity')\n",
    "\n",
    "for bar, val in zip(bars, [train_ppl, test_ppl]):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                 f'{val:.1f}', ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ü–æ-–Ω–∏—Å–∫–∞ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è = –º–æ–¥–µ–ª—ä—Ç –ø–æ-–¥–æ–±—Ä–µ –ø—Ä–µ–¥—Å–∫–∞–∑–≤–∞ –¥–∞–Ω–Ω–∏—Ç–µ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perplexity-importance",
   "metadata": {},
   "source": [
    "### –ó–∞—â–æ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è—Ç–∞ –µ –≤–∞–∂–Ω–∞?\n",
    "\n",
    "**–£–Ω–∏–≤–µ—Ä—Å–∞–ª–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞** –∑–∞ –µ–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏:\n",
    "- N-–≥—Ä–∞–º –º–æ–¥–µ–ª–∏: PP ~ 50-100\n",
    "- LSTM –º–æ–¥–µ–ª–∏: PP ~ 20-30\n",
    "- GPT-2 (2019): PP ~ 17\n",
    "- GPT-3 (2020): PP ~ 10-12\n",
    "\n",
    "üìå **–©–µ –∏–∑–ø–æ–ª–∑–≤–∞–º–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è –ø—Ä–µ–∑ —Ü–µ–ª–∏—è –∫—É—Ä—Å!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. –î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∞\n",
    "\n",
    "### –î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–∞ —Ö–∏–ø–æ—Ç–µ–∑–∞\n",
    "\n",
    "> \"You shall know a word by the company it keeps.\" ‚Äî J.R. Firth (1957)\n",
    "\n",
    "**–ò–¥–µ—è:** –î—É–º–∏, –∫–æ–∏—Ç–æ —Å–µ —Å—Ä–µ—â–∞—Ç –≤ —Å—Ö–æ–¥–Ω–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏, –∏–º–∞—Ç —Å—Ö–æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ.\n",
    "\n",
    "- \"dog\" –∏ \"cat\" —Å–µ —Å—Ä–µ—â–∞—Ç —Å: pet, food, loves, cute\n",
    "- \"car\" –∏ \"truck\" —Å–µ —Å—Ä–µ—â–∞—Ç —Å: drive, road, engine, wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooccurrence-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ-–≥–æ–ª—è–º –∫–æ—Ä–ø—É—Å –∑–∞ co-occurrence\n",
    "bigger_corpus = \"\"\"\n",
    "The king rules the kingdom with wisdom.\n",
    "The queen rules the kingdom with grace.\n",
    "A king sits on the throne.\n",
    "A queen sits on the throne.\n",
    "The king wears a golden crown.\n",
    "The queen wears a silver crown.\n",
    "The man works in the field.\n",
    "The woman works in the garden.\n",
    "A man drives the car.\n",
    "A woman drives the car.\n",
    "The boy plays with the dog.\n",
    "The girl plays with the cat.\n",
    "Dogs and cats are popular pets.\n",
    "The car has four wheels.\n",
    "The truck has six wheels.\n",
    "\"\"\"\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "words = word_tokenize(bigger_corpus.lower())\n",
    "words = [w for w in words if w.isalpha()]  # –°–∞–º–æ –±—É–∫–≤–∏\n",
    "\n",
    "print(f\"üìö –ö–æ—Ä–ø—É—Å: {len(words)} –¥—É–º–∏, {len(set(words))} —É–Ω–∏–∫–∞–ª–Ω–∏\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooccurrence-matrix",
   "metadata": {},
   "source": [
    "### Co-occurrence –º–∞—Ç—Ä–∏—Ü–∞\n",
    "\n",
    "**–ò–¥–µ—è:** –ë—Ä–æ–∏–º –∫–æ–ª–∫–æ –ø—ä—Ç–∏ –¥–≤–µ –¥—É–º–∏ —Å–µ —Å—Ä–µ—â–∞—Ç –≤ –±–ª–∏–∑–æ—Å—Ç (–≤ –ø—Ä–æ–∑–æ—Ä–µ—Ü –æ—Ç ¬±k –¥—É–º–∏).\n",
    "\n",
    "$$M_{ij} = \\text{–±—Ä–æ–π –ø—ä—Ç–∏ –¥—É–º–∞ } i \\text{ —Å–µ —Å—Ä–µ—â–∞ –±–ª–∏–∑–æ –¥–æ –¥—É–º–∞ } j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-cooccurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑–≥—Ä–∞–∂–¥–∞–Ω–µ –Ω–∞ co-occurrence –º–∞—Ç—Ä–∏—Ü–∞\n",
    "def build_cooccurrence_matrix(words, window_size=2):\n",
    "    \"\"\"Build word-word co-occurrence matrix.\"\"\"\n",
    "    vocab = sorted(set(words))\n",
    "    word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "    \n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    \n",
    "    for i, center in enumerate(words):\n",
    "        # –ì–ª–µ–¥–∞–º–µ –ø—Ä–æ–∑–æ—Ä–µ—Ü –æ–∫–æ–ª–æ –¥—É–º–∞—Ç–∞\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(words), i + window_size + 1)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if i != j:  # –ù–µ –±—Ä–æ–∏–º –¥—É–º–∞—Ç–∞ —Å—ä—Å —Å–µ–±–µ —Å–∏\n",
    "                context = words[j]\n",
    "                matrix[word_to_idx[center], word_to_idx[context]] += 1\n",
    "    \n",
    "    return matrix, vocab\n",
    "\n",
    "cooc_matrix, vocab = build_cooccurrence_matrix(words, window_size=2)\n",
    "print(f\"‚úì Co-occurrence –º–∞—Ç—Ä–∏—Ü–∞: {cooc_matrix.shape}\")\n",
    "print(f\"   –ù–µ–Ω—É–ª–µ–≤–∏ –µ–ª–µ–º–µ–Ω—Ç–∏: {np.count_nonzero(cooc_matrix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooccurrence-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞ –∏–∑–±—Ä–∞–Ω–∏ –¥—É–º–∏\n",
    "focus_words = ['king', 'queen', 'man', 'woman', 'dog', 'cat', 'car']\n",
    "focus_idx = [vocab.index(w) for w in focus_words if w in vocab]\n",
    "focus_words = [vocab[i] for i in focus_idx]\n",
    "\n",
    "submatrix = cooc_matrix[np.ix_(focus_idx, focus_idx)]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(submatrix, xticklabels=focus_words, yticklabels=focus_words,\n",
    "            annot=True, fmt='.0f', cmap='YlOrRd')\n",
    "plt.title('Co-occurrence –º–∞—Ç—Ä–∏—Ü–∞ (–ø—Ä–æ–∑–æ—Ä–µ—Ü = 2)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –î—É–º–∏, –∫–æ–∏—Ç–æ —Å–µ —Å—Ä–µ—â–∞—Ç –∑–∞–µ–¥–Ω–æ (king-queen, dog-cat), –∏–º–∞—Ç –≤–∏—Å–æ–∫–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosine-similarity",
   "metadata": {},
   "source": [
    "### –ö–æ—Å–∏–Ω—É—Å–æ–≤–æ —Å—Ö–æ–¥—Å—Ç–≤–æ\n",
    "\n",
    "**–ò–¥–µ—è:** –°—Ä–∞–≤–Ω—è–≤–∞–º–µ –¥—É–º–∏ –ø–æ —ä–≥—ä–ª–∞ –º–µ–∂–¥—É —Ç–µ—Ö–Ω–∏—Ç–µ –≤–µ–∫—Ç–æ—Ä–∏.\n",
    "\n",
    "$$\\text{cos}(\\vec{u}, \\vec{v}) = \\frac{\\vec{u} \\cdot \\vec{v}}{||\\vec{u}|| \\cdot ||\\vec{v}||}$$\n",
    "\n",
    "- **1.0** = –∏–¥–µ–Ω—Ç–∏—á–Ω–∏ –ø–æ—Å–æ–∫–∏ (–º–Ω–æ–≥–æ —Å—Ö–æ–¥–Ω–∏)\n",
    "- **0.0** = –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–Ω–∏ (–Ω–µ—Å–≤—ä—Ä–∑–∞–Ω–∏)\n",
    "- **-1.0** = –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–∏ (–∞–Ω—Ç–æ–Ω–∏–º–∏?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥—É–º–∏\n",
    "def word_similarity(w1, w2, matrix, vocab):\n",
    "    \"\"\"Cosine similarity between two words.\"\"\"\n",
    "    i1, i2 = vocab.index(w1), vocab.index(w2)\n",
    "    v1, v2 = matrix[i1], matrix[i2]\n",
    "    \n",
    "    dot = np.dot(v1, v2)\n",
    "    norm = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    \n",
    "    if norm == 0:\n",
    "        return 0\n",
    "    return dot / norm\n",
    "\n",
    "print(\"üìè –ö–æ—Å–∏–Ω—É—Å–æ–≤–æ —Å—Ö–æ–¥—Å—Ç–≤–æ (co-occurrence):\\n\")\n",
    "\n",
    "pairs = [('king', 'queen'), ('king', 'man'), ('dog', 'cat'), ('dog', 'car')]\n",
    "for w1, w2 in pairs:\n",
    "    if w1 in vocab and w2 in vocab:\n",
    "        sim = word_similarity(w1, w2, cooc_matrix, vocab)\n",
    "        bar = '‚ñà' * int(sim * 15) if sim > 0 else ''\n",
    "        print(f\"   {w1:6s} ‚Üî {w2:6s}: {sim:.3f} {bar}\")\n",
    "\n",
    "print(\"\\nüí° Co-occurrence —É–ª–∞–≤—è –Ω—è–∫–æ–∏ –≤—Ä—ä–∑–∫–∏, –Ω–æ –µ —à—É–º–µ–Ω –º–µ—Ç–æ–¥.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sparse-problems",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–±–ª–µ–º–∏ —Å co-occurrence –≤–µ–∫—Ç–æ—Ä–∏\n",
    "\n",
    "- **–í–∏—Å–æ–∫–æ–º–µ—Ä–Ω–∏:** —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç = —Ä–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞ (10K - 1M)\n",
    "- **–†–∞–∑—Ä–µ–¥–µ–Ω–∏:** –ø–æ–≤–µ—á–µ—Ç–æ –µ–ª–µ–º–µ–Ω—Ç–∏ —Å–∞ 0\n",
    "- **–ù–µ–∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–∏:** –∫–∞–∫–≤–æ –æ–∑–Ω–∞—á–∞–≤–∞—Ç –æ—Ç–¥–µ–ª–Ω–∏—Ç–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Word Embeddings: Word2Vec\n",
    "\n",
    "### –û—Ç —Ä–∞–∑—Ä–µ–¥–µ–Ω–∏ –∫—ä–º –ø–ª—ä—Ç–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è\n",
    "\n",
    "| –¢–∏–ø | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç | –ï–ª–µ–º–µ–Ω—Ç–∏ | –ü—Ä–∏–º–µ—Ä |\n",
    "|-----|------------|----------|--------|\n",
    "| **One-hot** | V (—Ä–µ—á–Ω–∏–∫) | 0/1 | [0,0,1,0,...,0] |\n",
    "| **Co-occurrence** | V | —Ü–µ–ª–∏ —á–∏—Å–ª–∞ | [3,0,7,1,...,0] |\n",
    "| **Word2Vec** | 50-300 | —Ä–µ–∞–ª–Ω–∏ —á–∏—Å–ª–∞ | [0.2,-0.5,0.8,...] |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "word2vec-idea",
   "metadata": {},
   "source": [
    "### –ò–¥–µ—è –Ω–∞ Word2Vec (Mikolov et al., 2013)\n",
    "\n",
    "**–û–±—É—á–∞–≤–∞–º–µ –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞** –¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–≤–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç –¥—É–º–∞ (–∏–ª–∏ –æ–±—Ä–∞—Ç–Ω–æ—Ç–æ).\n",
    "\n",
    "**–î–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∏:**\n",
    "\n",
    "| –ú–æ–¥–µ–ª | –í—Ö–æ–¥ | –ò–∑—Ö–æ–¥ | –î–æ–±—ä—Ä –∑–∞ |\n",
    "|-------|------|-------|----------|\n",
    "| **Skip-gram** | –¶–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –¥—É–º–∞ | –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏ –¥—É–º–∏ | –†–µ–¥–∫–∏ –¥—É–º–∏ |\n",
    "| **CBOW** | –ö–æ–Ω—Ç–µ–∫—Å—Ç | –¶–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –¥—É–º–∞ | –ß–µ—Å—Ç–∏ –¥—É–º–∏ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skipgram-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–ª—é—Å—Ç—Ä–∞—Ü–∏—è: Skip-gram training pairs\n",
    "sentence = \"the king rules the kingdom\".split()\n",
    "window = 2\n",
    "\n",
    "print(\"üìö Skip-gram: –ø—Ä–µ–¥—Å–∫–∞–∑–≤–∞–º–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –¥—É–º–∞\\n\")\n",
    "print(f\"–ò–∑—Ä–µ—á–µ–Ω–∏–µ: {' '.join(sentence)}\")\n",
    "print(f\"–ü—Ä–æ–∑–æ—Ä–µ—Ü: ¬±{window}\\n\")\n",
    "\n",
    "print(\"Training pairs (center ‚Üí context):\")\n",
    "for i, center in enumerate(sentence):\n",
    "    contexts = []\n",
    "    for j in range(max(0, i-window), min(len(sentence), i+window+1)):\n",
    "        if j != i:\n",
    "            contexts.append(sentence[j])\n",
    "    \n",
    "    print(f\"   {center:8s} ‚Üí {contexts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-pretrained",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –ø—Ä–µ–¥—Ç—Ä–µ–Ω–∏—Ä–∞–Ω Word2Vec –º–æ–¥–µ–ª\n",
    "print(\"üì• –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ Word2Vec –º–æ–¥–µ–ª (–º–æ–∂–µ –¥–∞ –æ—Ç–Ω–µ–º–µ 1-2 –º–∏–Ω—É—Ç–∏)...\\n\")\n",
    "\n",
    "# –ò–∑–ø–æ–ª–∑–≤–∞–º–µ –ø–æ-–º–∞–ª—ä–∫ –º–æ–¥–µ–ª –∑–∞ –±—ä—Ä–∑–∏–Ω–∞\n",
    "word2vec = api.load('glove-wiki-gigaword-50')  # 50-–º–µ—Ä–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏\n",
    "\n",
    "print(f\"‚úì –ú–æ–¥–µ–ª –∑–∞—Ä–µ–¥–µ–Ω!\")\n",
    "print(f\"   –†–µ—á–Ω–∏–∫: {len(word2vec)} –¥—É–º–∏\")\n",
    "print(f\"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç: {word2vec.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "word-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–∞–∫ –∏–∑–≥–ª–µ–∂–¥–∞ –µ–¥–∏–Ω word vector?\n",
    "word = 'king'\n",
    "vector = word2vec[word]\n",
    "\n",
    "print(f\"üìä –í–µ–∫—Ç–æ—Ä –∑–∞ –¥—É–º–∞—Ç–∞ '{word}':\\n\")\n",
    "print(f\"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç: {len(vector)}\")\n",
    "print(f\"   –ü—ä—Ä–≤–∏—Ç–µ 10 —Å—Ç–æ–π–Ω–æ—Å—Ç–∏: {vector[:10].round(3)}\")\n",
    "print(f\"   Min: {vector.min():.3f}, Max: {vector.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "most-similar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞–º–∏—Ä–∞–Ω–µ –Ω–∞ —Å—Ö–æ–¥–Ω–∏ –¥—É–º–∏\n",
    "print(\"üîç –ù–∞–π-—Å—Ö–æ–¥–Ω–∏ –¥—É–º–∏:\\n\")\n",
    "\n",
    "test_words = ['king', 'computer', 'happy', 'dog']\n",
    "for word in test_words:\n",
    "    similar = word2vec.most_similar(word, topn=5)\n",
    "    neighbors = ', '.join([f\"{w} ({s:.2f})\" for w, s in similar])\n",
    "    print(f\"   {word}: {neighbors}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "word2vec-properties",
   "metadata": {},
   "source": [
    "### –í—ä–ª—à–µ–±—Å—Ç–≤–æ—Ç–æ –Ω–∞ Word2Vec: –ê–Ω–∞–ª–æ–≥–∏–∏\n",
    "\n",
    "**–°–µ–º–∞–Ω—Ç–∏—á–Ω–∞ –∞—Ä–∏—Ç–º–µ—Ç–∏–∫–∞:**\n",
    "$$\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}$$\n",
    "\n",
    "**–ö–∞–∫–≤–æ –æ–∑–Ω–∞—á–∞–≤–∞ —Ç–æ–≤–∞?**\n",
    "- –ü–æ—Å–æ–∫–∞—Ç–∞ –æ—Ç \"man\" –∫—ä–º \"king\" = –∫–æ–Ω—Ü–µ–ø—Ü–∏—è—Ç–∞ \"—Ä–æ—è–ª—Ç–∏\"\n",
    "- –ü—Ä–∏–ª–æ–∂–µ–Ω–∞ –≤—ä—Ä—Ö—É \"woman\" ‚Üí \"queen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analogies",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –∞–Ω–∞–ª–æ–≥–∏–∏\n",
    "print(\"üëë Word2Vec –∞–Ω–∞–ª–æ–≥–∏–∏:\\n\")\n",
    "\n",
    "analogies = [\n",
    "    ('king', 'man', 'woman'),      # king - man + woman = ?\n",
    "    ('paris', 'france', 'italy'),  # paris - france + italy = ?\n",
    "    ('bigger', 'big', 'small'),    # bigger - big + small = ?\n",
    "    ('walked', 'walk', 'swim'),    # walked - walk + swim = ?\n",
    "]\n",
    "\n",
    "for a, b, c in analogies:\n",
    "    try:\n",
    "        result = word2vec.most_similar(positive=[a, c], negative=[b], topn=1)\n",
    "        answer = result[0][0]\n",
    "        score = result[0][1]\n",
    "        print(f\"   {a} - {b} + {c} = {answer} (score: {score:.3f})\")\n",
    "    except KeyError as e:\n",
    "        print(f\"   –î—É–º–∞ –Ω–µ –µ –≤ —Ä–µ—á–Ω–∏–∫–∞: {e}\")\n",
    "\n",
    "print(\"\\nüí° Word2Vec —É–ª–∞–≤—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏ –∏ —Å–∏–Ω—Ç–∞–∫—Ç–∏—á–Ω–∏ —Ä–µ–ª–∞—Ü–∏–∏!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analogy-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –∞–Ω–∞–ª–æ–≥–∏—è\n",
    "words = ['king', 'queen', 'man', 'woman', 'prince', 'princess']\n",
    "vectors = np.array([word2vec[w] for w in words])\n",
    "\n",
    "# –†–µ–¥—É—Ü–∏—Ä–∞–º–µ –¥–æ 2D —Å PCA\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# –¢–æ—á–∫–∏\n",
    "plt.scatter(coords[:, 0], coords[:, 1], s=100, c='steelblue', zorder=5)\n",
    "\n",
    "# –ï—Ç–∏–∫–µ—Ç–∏\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, coords[i], fontsize=14, ha='center', va='bottom',\n",
    "                 xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "# –°—Ç—Ä–µ–ª–∫–∏ –∑–∞ –∞–Ω–∞–ª–æ–≥–∏—è: man‚Üíking –ø–∞—Ä–∞–ª–µ–ª–Ω–æ –Ω–∞ woman‚Üíqueen\n",
    "# man=2, king=0, woman=3, queen=1\n",
    "plt.arrow(coords[2,0], coords[2,1], \n",
    "          coords[0,0]-coords[2,0], coords[0,1]-coords[2,1],\n",
    "          head_width=0.1, head_length=0.05, fc='red', ec='red', alpha=0.7)\n",
    "plt.arrow(coords[3,0], coords[3,1],\n",
    "          coords[1,0]-coords[3,0], coords[1,1]-coords[3,1],\n",
    "          head_width=0.1, head_length=0.05, fc='red', ec='red', alpha=0.7)\n",
    "\n",
    "plt.title('Word2Vec: king - man + woman ‚âà queen', fontsize=14)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ü–∞—Ä–∞–ª–µ–ª–Ω–∏—Ç–µ —Å—Ç—Ä–µ–ª–∫–∏ –ø–æ–∫–∞–∑–≤–∞—Ç, —á–µ 'gender' –µ –æ—Ç–¥–µ–ª–Ω–∞ –ø–æ—Å–æ–∫–∞ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ—Ç–æ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding-clusters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏ –∫–ª—ä—Å—Ç–µ—Ä–∏\n",
    "categories = {\n",
    "    'Royalty': ['king', 'queen', 'prince', 'princess', 'throne', 'crown'],\n",
    "    'Animals': ['dog', 'cat', 'horse', 'bird', 'fish', 'lion'],\n",
    "    'Countries': ['france', 'germany', 'italy', 'spain', 'japan', 'china'],\n",
    "    'Colors': ['red', 'blue', 'green', 'yellow', 'black', 'white']\n",
    "}\n",
    "\n",
    "# –°—ä–±–∏—Ä–∞–º–µ –≤—Å–∏—á–∫–∏ –¥—É–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∏\n",
    "all_words = []\n",
    "all_vectors = []\n",
    "all_categories = []\n",
    "\n",
    "for cat, words in categories.items():\n",
    "    for w in words:\n",
    "        if w in word2vec:\n",
    "            all_words.append(w)\n",
    "            all_vectors.append(word2vec[w])\n",
    "            all_categories.append(cat)\n",
    "\n",
    "vectors_array = np.array(all_vectors)\n",
    "coords = PCA(n_components=2).fit_transform(vectors_array)\n",
    "\n",
    "# –¶–≤–µ—Ç–æ–≤–µ –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "colors = {'Royalty': 'gold', 'Animals': 'forestgreen', \n",
    "          'Countries': 'royalblue', 'Colors': 'purple'}\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for cat in categories.keys():\n",
    "    mask = [c == cat for c in all_categories]\n",
    "    cat_coords = coords[mask]\n",
    "    plt.scatter(cat_coords[:, 0], cat_coords[:, 1], \n",
    "                c=colors[cat], label=cat, s=100, alpha=0.7)\n",
    "    \n",
    "    # –ï—Ç–∏–∫–µ—Ç–∏\n",
    "    for i, (x, y) in enumerate(cat_coords):\n",
    "        word = [w for w, c in zip(all_words, all_categories) if c == cat][i]\n",
    "        plt.annotate(word, (x, y), fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Word2Vec: –°–µ–º–∞–Ω—Ç–∏—á–Ω–∏ –∫–ª—ä—Å—Ç–µ—Ä–∏', fontsize=14)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –î—É–º–∏ –æ—Ç –µ–¥–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Å–µ –≥—Ä—É–ø–∏—Ä–∞—Ç –∑–∞–µ–¥–Ω–æ –≤ embedding –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ—Ç–æ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "word2vec-limitations",
   "metadata": {},
   "source": [
    "### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ Word2Vec\n",
    "\n",
    "**1. –ï–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä –∑–∞ –¥—É–º–∞ (–±–µ–∑ –ø–æ–ª–∏—Å–µ–º–∏—è):**\n",
    "- \"bank\" (—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞) = \"bank\" (—Ä–µ—á–Ω–∞)\n",
    "- –†–µ—à–µ–Ω–∏–µ: –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–Ω–∏ embeddings (BERT, –õ–µ–∫—Ü–∏—è 5)\n",
    "\n",
    "**2. –£–ª–∞–≤—è –±–∏–∞—Å–æ–≤–µ –æ—Ç –¥–∞–Ω–Ω–∏—Ç–µ:**\n",
    "- \"doctor\" –µ –ø–æ-–±–ª–∏–∑–æ –¥–æ \"man\", \"nurse\" –¥–æ \"woman\"\n",
    "\n",
    "**3. –°—Ç–∞—Ç–∏—á–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏:**\n",
    "- –ù–µ —Å–µ –ø—Ä–æ–º–µ–Ω—è—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bias-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ bias –≤ embeddings\n",
    "print(\"‚ö†Ô∏è Bias –≤ Word Embeddings:\\n\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –∞—Å–æ—Ü–∏–∞—Ü–∏–∏ —Å –ø—Ä–æ—Ñ–µ—Å–∏–∏\n",
    "professions = ['doctor', 'nurse', 'engineer', 'teacher', 'programmer', 'secretary']\n",
    "\n",
    "print(\"–°—Ö–æ–¥—Å—Ç–≤–æ —Å 'man' vs 'woman':\\n\")\n",
    "print(f\"{'–ü—Ä–æ—Ñ–µ—Å–∏—è':<12} {'sim(man)':<12} {'sim(woman)':<12} {'–†–∞–∑–ª–∏–∫–∞':<10}\")\n",
    "print(\"-\" * 46)\n",
    "\n",
    "for prof in professions:\n",
    "    if prof in word2vec:\n",
    "        sim_man = word2vec.similarity(prof, 'man')\n",
    "        sim_woman = word2vec.similarity(prof, 'woman')\n",
    "        diff = sim_man - sim_woman\n",
    "        indicator = '‚Üí M' if diff > 0.05 else ('‚Üí W' if diff < -0.05 else '‚âà')\n",
    "        print(f\"{prof:<12} {sim_man:<12.3f} {sim_woman:<12.3f} {diff:+.3f} {indicator}\")\n",
    "\n",
    "print(\"\\nüí° Embeddings —É–ª–∞–≤—è—Ç —Å–æ—Ü–∏–∞–ª–Ω–∏ –±–∏–∞—Å–æ–≤–µ –æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Å—ä–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "\n",
    "### –ö–æ–≥–∞ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞–º–µ —Ä–∞–∑–ª–∏—á–Ω–∏—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è?\n",
    "\n",
    "| –ü—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ | –ö–æ–≥–∞ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞–º–µ | –ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞ |\n",
    "|-------------|-------------------|------------|\n",
    "| **Bag-of-Words** | Baseline, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç | –ü—Ä–æ—Å—Ç–æ, –±—ä—Ä–∑–æ |\n",
    "| **TF-IDF** | Information retrieval, search | –ü–æ-–¥–æ–±—Ä–æ –ø—Ä–µ—Ç–µ–≥–ª—è–Ω–µ |\n",
    "| **Word2Vec** | –°–µ–º–∞–Ω—Ç–∏—á–Ω–æ —Å—Ö–æ–¥—Å—Ç–≤–æ, –∞–Ω–∞–ª–æ–≥–∏–∏ | –ü–ª—ä—Ç–Ω–æ, —É–ª–∞–≤—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ |\n",
    "| **–ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–Ω–∏** | –ü–æ–ª–∏—Å–µ–º–∏—è, —Å—ä–≤—Ä–µ–º–µ–Ω–Ω–∏ –∑–∞–¥–∞—á–∏ | State-of-the-art |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–∞–≤–∞–Ω–µ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω Word2Vec –º–æ–¥–µ–ª\n",
    "custom_corpus = [\n",
    "    ['machine', 'learning', 'is', 'a', 'subset', 'of', 'artificial', 'intelligence'],\n",
    "    ['deep', 'learning', 'uses', 'neural', 'networks'],\n",
    "    ['neural', 'networks', 'have', 'many', 'layers'],\n",
    "    ['artificial', 'intelligence', 'includes', 'machine', 'learning'],\n",
    "    ['nlp', 'uses', 'machine', 'learning', 'techniques'],\n",
    "    ['transformers', 'revolutionized', 'nlp'],\n",
    "    ['bert', 'is', 'a', 'transformer', 'model'],\n",
    "    ['gpt', 'uses', 'transformer', 'architecture'],\n",
    "]\n",
    "\n",
    "# –û–±—É—á–∞–≤–∞–º–µ –º–∞–ª—ä–∫ –º–æ–¥–µ–ª\n",
    "custom_model = Word2Vec(\n",
    "    sentences=custom_corpus,\n",
    "    vector_size=20,     # –ú–∞–ª–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç –∑–∞ –¥–µ–º–æ\n",
    "    window=2,           # –ö–æ–Ω—Ç–µ–∫—Å—Ç–µ–Ω –ø—Ä–æ–∑–æ—Ä–µ—Ü\n",
    "    min_count=1,        # –ú–∏–Ω–∏–º–∞–ª–Ω–∞ —á–µ—Å—Ç–æ—Ç–∞\n",
    "    epochs=100          # –ë—Ä–æ–π –µ–ø–æ—Ö–∏\n",
    ")\n",
    "\n",
    "print(\"‚úì –û–±—É—á–µ–Ω custom Word2Vec –º–æ–¥–µ–ª!\")\n",
    "print(f\"   –†–µ—á–Ω–∏–∫: {len(custom_model.wv)} –¥—É–º–∏\")\n",
    "print(f\"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç: {custom_model.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-similar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –∫–∞–∫–≤–æ –µ –Ω–∞—É—á–∏–ª\n",
    "print(\"üîç –ö–∞–∫–≤–æ –Ω–∞—É—á–∏ custom –º–æ–¥–µ–ª–∞?\\n\")\n",
    "\n",
    "test_words = ['learning', 'neural', 'transformer']\n",
    "for word in test_words:\n",
    "    if word in custom_model.wv:\n",
    "        similar = custom_model.wv.most_similar(word, topn=3)\n",
    "        neighbors = ', '.join([f\"{w} ({s:.2f})\" for w, s in similar])\n",
    "        print(f\"   {word}: {neighbors}\")\n",
    "\n",
    "print(\"\\nüí° –ù–∞ –º–∞–ª—ä–∫ –∫–æ—Ä–ø—É—Å —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ —Å–∞ —à—É–º–Ω–∏, –Ω–æ —É–ª–∞–≤—è—Ç –Ω—è–∫–æ–∏ –≤—Ä—ä–∑–∫–∏.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyperparameters",
   "metadata": {},
   "source": [
    "### –í–∞–∂–Ω–∏ —Ö–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ Word2Vec\n",
    "\n",
    "| –ü–∞—Ä–∞–º–µ—Ç—ä—Ä | –¢–∏–ø–∏—á–Ω–∞ —Å—Ç–æ–π–Ω–æ—Å—Ç | –ï—Ñ–µ–∫—Ç |\n",
    "|-----------|------------------|-------|\n",
    "| **vector_size** | 100-300 | –ü–æ-–≥–æ–ª–µ–º–∏ = –ø–æ–≤–µ—á–µ –¥–µ—Ç–∞–π–ª–∏ |\n",
    "| **window** | 5-10 | –ü–æ-–≥–æ–ª–µ–º–∏ = –ø–æ-—à–∏—Ä–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç |\n",
    "| **min_count** | 5-10 | –§–∏–ª—Ç—Ä–∏—Ä–∞ —Ä–µ–¥–∫–∏ –¥—É–º–∏ |\n",
    "| **epochs** | 5-20 | –ü–æ–≤–µ—á–µ = –ø–æ-–¥–æ–±—Ä–æ –æ–±—É—á–µ–Ω–∏–µ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. –û–±–æ–±—â–µ–Ω–∏–µ –∏ –º–æ—Å—Ç –∫—ä–º –õ–µ–∫—Ü–∏—è 3\n",
    "\n",
    "### –ö–ª—é—á–æ–≤–∏ –∏–∑–≤–æ–¥–∏ –æ—Ç –¥–Ω–µ—Å\n",
    "\n",
    "**1. –ï–∑–∏–∫–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏—Å–≤–æ—è–≤–∞—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ–∫—Å—Ç**\n",
    "- N-–≥—Ä–∞–º –º–æ–¥–µ–ª–∏: –ø—Ä–æ—Å—Ç–æ, –Ω–æ sparsity –ø—Ä–æ–±–ª–µ–º\n",
    "- –ü–µ—Ä–ø–ª–µ–∫—Å–∏—è: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ –∑–∞ –æ—Ü–µ–Ω–∫–∞\n",
    "\n",
    "**2. –î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–∞ —Ö–∏–ø–æ—Ç–µ–∑–∞**\n",
    "- \"–î—É–º–∏—Ç–µ —Å–µ –ø–æ–∑–Ω–∞–≤–∞—Ç –ø–æ –∫–æ–º–ø–∞–Ω–∏—è—Ç–∞, –∫–æ—è—Ç–æ –≤–æ–¥—è—Ç\"\n",
    "- Co-occurrence –º–∞—Ç—Ä–∏—Ü–∏ —É–ª–∞–≤—è—Ç –Ω—è–∫–æ–∏ —Ä–µ–ª–∞—Ü–∏–∏\n",
    "\n",
    "**3. Word2Vec: –ø–ª—ä—Ç–Ω–∏ word embeddings**\n",
    "- –£–ª–∞–≤—è—Ç —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ —Å—Ö–æ–¥—Å—Ç–≤–æ –∏ –∞–Ω–∞–ª–æ–≥–∏–∏\n",
    "- –ï–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä –Ω–∞ –¥—É–º–∞ ‚Üí –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "- –°—ä–¥—ä—Ä–∂–∞—Ç –±–∏–∞—Å–æ–≤–µ –æ—Ç –¥–∞–Ω–Ω–∏—Ç–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-lecture",
   "metadata": {},
   "source": [
    "### –°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "**–í—ä–ø—Ä–æ—Å–∏, –Ω–∞ –∫–æ–∏—Ç–æ —â–µ –æ—Ç–≥–æ–≤–æ—Ä–∏–º:**\n",
    "\n",
    "- –ö–∞–∫–≤–æ –µ \"–¥—É–º–∞\"? –ö–∞–∫ –¥–µ–ª–∏–º —Ç–µ–∫—Å—Ç–∞?\n",
    "- –ö–∞–∫ –¥–∞ —Å–µ —Å–ø—Ä–∞–≤–∏–º —Å –Ω–æ–≤–∏ –¥—É–º–∏ (OOV)?\n",
    "- –ö–∞–∫–≤–æ –µ BPE, WordPiece, SentencePiece?\n",
    "- –ö–∞–∫ tokenizer-—ä—Ç –≤–ª–∏—è–µ –Ω–∞ –º–æ–¥–µ–ª–∞?\n",
    "\n",
    "**–ó–∞—â–æ –µ –≤–∞–∂–Ω–æ:**\n",
    "- GPT –∏–∑–ø–æ–ª–∑–≤–∞ BPE —Å ~50K —Ç–æ–∫–µ–Ω–∏\n",
    "- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è—Ç–∞ –æ–ø—Ä–µ–¥–µ–ª—è –∫–∞–∫–≤–æ \"–≤–∏–∂–¥–∞\" –º–æ–¥–µ–ª—ä—Ç\n",
    "- –ö—Ä–∏—Ç–∏—á–Ω–æ –∑–∞ –º–Ω–æ–≥–æ–µ–∑–∏—á–Ω–∏ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "## –†–µ—Å—É—Ä—Å–∏\n",
    "\n",
    "### –ü—Ä–µ–ø–æ—Ä—ä—á–∏—Ç–µ–ª–Ω–æ —á–µ—Ç–µ–Ω–µ\n",
    "\n",
    "**–°—Ç–∞—Ç–∏–∏:**\n",
    "1. Mikolov et al. (2013) - \"Efficient Estimation of Word Representations in Vector Space\"\n",
    "2. Mikolov et al. (2013) - \"Distributed Representations of Words and Phrases\"\n",
    "3. Pennington et al. (2014) - \"GloVe: Global Vectors for Word Representation\"\n",
    "\n",
    "**–£—á–µ–±–Ω–∏—Ü–∏:**\n",
    "1. \"Speech and Language Processing\" - Jurafsky & Martin, Ch. 3 (N-grams), Ch. 6 (Embeddings)\n",
    "2. \"Introduction to Information Retrieval\" - Manning et al., Ch. 6\n",
    "\n",
    "**Online:**\n",
    "1. Stanford CS224N Lecture 2 - Word Vectors\n",
    "2. Gensim Word2Vec Tutorial\n",
    "3. TensorFlow Embedding Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∑–∞ –≤–∫—ä—â–∏\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 1: N-–≥—Ä–∞–º –º–æ–¥–µ–ª**\n",
    "- –ò–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–π—Ç–µ trigram –º–æ–¥–µ–ª\n",
    "- –°—Ä–∞–≤–Ω–µ—Ç–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è—Ç–∞ —Å bigram\n",
    "- –ì–µ–Ω–µ—Ä–∏—Ä–∞–π—Ç–µ —Ç–µ–∫—Å—Ç –∏ —Å—Ä–∞–≤–Ω–µ—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ—Ç–æ\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 2: Word2Vec –∞–Ω–∞–ª–æ–≥–∏–∏**\n",
    "- –ù–∞–º–µ—Ä–µ—Ç–µ 10 —Ä–∞–±–æ—Ç–µ—â–∏ –∞–Ω–∞–ª–æ–≥–∏–∏ (state:capital, verb:past_tense, ...)\n",
    "- –ù–∞–º–µ—Ä–µ—Ç–µ 5 –ø—Ä–∏–º–µ—Ä–∞, –∫—ä–¥–µ—Ç–æ –∞–Ω–∞–ª–æ–≥–∏–∏—Ç–µ —Å–µ –ø—Ä–æ–≤–∞–ª—è—Ç\n",
    "- –ó–∞—â–æ –Ω—è–∫–æ–∏ —Ä–∞–±–æ—Ç—è—Ç, –∞ –¥—Ä—É–≥–∏ –Ω–µ?\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 3: Bias –∞–Ω–∞–ª–∏–∑**\n",
    "- –ò–∑—Å–ª–µ–¥–≤–∞–π—Ç–µ bias –∑–∞ –Ω–∞—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç–∏, –≤—ä–∑—Ä–∞—Å—Ç, —Ä–µ–ª–∏–≥–∏—è\n",
    "- –ü—Ä–µ–¥–ª–æ–∂–µ—Ç–µ –∫–∞–∫ –º–æ–∂–µ –¥–∞ —Å–µ –Ω–∞–º–∞–ª–∏ bias-—ä—Ç\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 4: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è**\n",
    "- –ò–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–π—Ç–µ text classification —Å BoW, TF-IDF, Word2Vec (—Å—Ä–µ–¥–Ω–æ)\n",
    "- –°—Ä–∞–≤–Ω–µ—Ç–µ accuracy –Ω–∞ 20 Newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "end",
   "metadata": {},
   "source": [
    "---\n",
    "## –ö—Ä–∞–π –Ω–∞ –õ–µ–∫—Ü–∏—è 2\n",
    "\n",
    "### –ë–ª–∞–≥–æ–¥–∞—Ä—è –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ—Ç–æ!\n",
    "\n",
    "**–í—ä–ø—Ä–æ—Å–∏?**\n",
    "\n",
    "---\n",
    "\n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (BPE, WordPiece, Subword –∞–ª–≥–æ—Ä–∏—Ç–º–∏)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
