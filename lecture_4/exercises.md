# Кратки упражнения: Лекция 4

Следните упражнения са за самостоятелна работа по време на лекцията или веднага след нея. Очаквано време: 2-3 минути за упражнение.

---

## Упражнение 1: Query-Key-Value интуиция

Разгледайте изречението: "The cat sat on the mat because **it** was tired."

**Въпроси:**
1. Когато обработваме думата "it", към коя дума трябва да има силен attention?
2. Защо това е пример за нещо, което pooling не може да улови?
3. Как Q-K-V механизмът помага: коя дума генерира Query, коя Key?

---

## Упражнение 2: Attention scores на ръка

Дадени са Query и Key вектори за 3 токена:
```
Q = [[1, 0],    # токен 1
     [0, 1],    # токен 2
     [1, 1]]    # токен 3

K = [[1, 0],    # токен 1
     [0, 1],    # токен 2
     [1, 1]]    # токен 3
```

Изчислете attention scores матрицата $QK^T$ (без scaling):

```
Scores[i,j] = Q[i] · K[j]
```

**Hint:** $[1,0] \cdot [1,1] = 1$, $[1,1] \cdot [1,1] = 2$

---

## Упражнение 3: Scaling factor

Имате $d_k = 64$ (размерност на Key вектори).

**Въпроси:**
1. Каква е стойността на $\sqrt{d_k}$?
2. Ако dot product между Q и K е 16.0, каква е scaled стойността?
3. Защо scaling е важен за softmax? Какво става без него при големи $d_k$?

---

## Упражнение 4: Softmax в attention

След scaling, attention scores за един Query са: $[2.0, 1.0, 0.0]$

Изчислете attention weights (softmax):
$$\text{weight}_i = \frac{e^{s_i}}{\sum_j e^{s_j}}$$

**Hints:** $e^2 \approx 7.39$, $e^1 \approx 2.72$, $e^0 = 1$

**Въпрос:** Сумата на weights трябва да е?

---

## Упражнение 5: Attention output

Attention weights за токен: $[0.7, 0.2, 0.1]$

Value вектори:
```
V = [[1, 0, 0],   # токен 1
     [0, 1, 0],   # токен 2
     [0, 0, 1]]   # токен 3
```

Изчислете output вектора:
$$\text{output} = 0.7 \cdot V_1 + 0.2 \cdot V_2 + 0.1 \cdot V_3 = ?$$

---

## Упражнение 6: Positional encoding

Без positional encoding, attention е permutation equivariant.

**Въпроси:**
1. Какво означава "permutation equivariant"?
2. Защо "dog bites man" и "man bites dog" биха имали идентични attention patterns без PE?
3. Как sinusoidal PE решава проблема?

---

## Упражнение 7: Multi-Head размерности

Модел има:
- $d_{model} = 512$
- $n_{heads} = 8$

**Изчислете:**
1. $d_k = d_v = d_{model} / n_{heads} = ?$
2. Колко параметъра има $W_Q$ за **един** head? (shape: $d_{model} \times d_k$)
3. Колко параметъра има $W_Q$ за **всички** heads общо?

---

## Упражнение 8: Causal mask

За последователност от 4 токена, causal attention позволява:
- Токен 1 вижда: ?
- Токен 2 вижда: ?
- Токен 3 вижда: ?
- Токен 4 вижда: ?

**Въпрос:** Как изглежда causal mask матрицата? (1 = може да вижда, 0 = не може)

```
     t1  t2  t3  t4
t1 [  ?   ?   ?   ?  ]
t2 [  ?   ?   ?   ?  ]
t3 [  ?   ?   ?   ?  ]
t4 [  ?   ?   ?   ?  ]
```

---

## Упражнение 9: Computational complexity

**Въпроси:**
1. Каква е времевата сложност на self-attention спрямо дължина на последователността $n$?
2. Колко пъти повече памет е нужна за $n=1024$ спрямо $n=512$?
3. Защо това е проблем за дълги контексти (напр. 100K токена)?

---

## Упражнение 10: RNN vs Attention

Сравнете RNN и Self-Attention:

| Свойство | RNN | Self-Attention |
|----------|-----|----------------|
| Максимална дължина на пътя между позиции | ? | ? |
| Паралелизация | ? | ? |
| Времева сложност | ? | ? |

**Въпрос:** Защо Transformer замени RNN като доминираща архитектура за NLP?

---

## Упражнение 11: Attention като lookup

Attention може да се разбира като "soft dictionary lookup".

**Въпроси:**
1. Ако attention weight към една позиция е 1.0 и към всички други е 0.0, какво прави attention?
2. Ако attention weights са равни ($1/n$ за всички), какво прави attention?
3. Защо наричаме attention "soft" lookup?

---

## Упражнение 12: Брой параметри

Single-head attention слой има:
- $d_{model} = 256$
- Проекции: $W_Q$, $W_K$, $W_V$, $W_O$ (всяка $256 \times 256$)

**Изчислете:**
1. Параметри за $W_Q$: ?
2. Общо параметри за attention слоя (без bias): ?
3. Ако добавим 8 heads (но $d_k = 32$), променя ли се броят параметри?

---

# Решения

## Решение 1
1. "it" трябва да има силен attention към "cat" (coreference resolution)
2. Pooling осреднява всички думи - губи информацията коя дума се отнася към коя
3. "it" генерира Query ("какво търся?"), "cat" генерира Key ("аз съм нещо, към което можеш да се обърнеш")

## Решение 2
```
QK^T = [[1·1+0·0, 1·0+0·1, 1·1+0·1],
        [0·1+1·0, 0·0+1·1, 0·1+1·1],
        [1·1+1·0, 1·0+1·1, 1·1+1·1]]
     = [[1, 0, 1],
        [0, 1, 1],
        [1, 1, 2]]
```

## Решение 3
1. $\sqrt{64} = 8$
2. $16.0 / 8 = 2.0$
3. Без scaling, при големи $d_k$ dot products стават много големи, softmax сатурира (почти 0 или 1), градиентите изчезват

## Решение 4
- $e^2 + e^1 + e^0 = 7.39 + 2.72 + 1 = 11.11$
- $w_1 = 7.39 / 11.11 \approx 0.665$
- $w_2 = 2.72 / 11.11 \approx 0.245$
- $w_3 = 1 / 11.11 \approx 0.090$

Сума: $0.665 + 0.245 + 0.090 = 1.0$

## Решение 5
$$\text{output} = 0.7 \cdot [1,0,0] + 0.2 \cdot [0,1,0] + 0.1 \cdot [0,0,1]$$
$$= [0.7, 0, 0] + [0, 0.2, 0] + [0, 0, 0.1]$$
$$= [0.7, 0.2, 0.1]$$

## Решение 6
1. Ако размесим входа, изходът се размесва по същия начин
2. Без PE, attention вижда само съдържанието, не позицията - двете изречения имат същите думи
3. PE добавя уникален "код" за всяка позиция, така че attention може да различава позиции

## Решение 7
1. $d_k = 512 / 8 = 64$
2. $512 \times 64 = 32,768$ параметъра
3. $512 \times 512 = 262,144$ (или $8 \times 32,768 = 262,144$)

## Решение 8
```
     t1  t2  t3  t4
t1 [  1   0   0   0  ]
t2 [  1   1   0   0  ]
t3 [  1   1   1   0  ]
t4 [  1   1   1   1  ]
```
Lower triangular матрица

## Решение 9
1. $O(n^2)$ - квадратична
2. $(1024)^2 / (512)^2 = 4$ пъти повече памет
3. За 100K токена: $(100000)^2 = 10^{10}$ операции/памет - непрактично без оптимизации

## Решение 10
| Свойство | RNN | Self-Attention |
|----------|-----|----------------|
| Максимална дължина на пътя | $O(n)$ | $O(1)$ |
| Паралелизация | Не | Да |
| Времева сложност | $O(n)$ | $O(n^2)$ |

Transformer замени RNN заради: паралелизация (GPU ускорение), директни връзки (по-добри градиенти), по-добро моделиране на дълги зависимости

## Решение 11
1. Копира value от тази позиция (hard lookup)
2. Осреднява всички values (mean pooling)
3. "Soft" защото комбинира информация от множество позиции с различни тегла, не избира само една

## Решение 12
1. $256 \times 256 = 65,536$
2. $4 \times 65,536 = 262,144$
3. Не! Multi-head restructurira параметрите, но общият брой остава същият:
   - 8 heads × ($256 \times 32$) × 4 проекции = $8 \times 8192 \times 4 = 262,144$
