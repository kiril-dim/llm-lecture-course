{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 5: Transformer архитектура и дълъг контекст\n",
    "\n",
    "**Продължителност:** 2-2.5 часа  \n",
    "**Предпоставки:** Лекция 4 (Attention механизми)  \n",
    "**Следваща лекция:** Foundation Models и Pretraining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Цели на лекцията\n",
    "\n",
    "След тази лекция ще можете:\n",
    "\n",
    "- Разбирате пълната структура на transformer блок (FFN, residuals, layer norm)\n",
    "- Обяснявате разликите между encoder и decoder архитектури\n",
    "- Имплементирате GQA и разбирате ефективността на KV cache\n",
    "- Обяснявате RoPE и защо е важен за дълъг контекст\n",
    "- Разбирате как моделите обработват 100K-1M+ токена"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пътна карта\n",
    "\n",
    "```\n",
    "1. Recap → 2. Transformer Block → 3. Stacking Layers → 4. Encoder vs Decoder\n",
    "                                                              ↓\n",
    "    8. Обобщение ← 7. Цялостна архитектура ← 6. Long Context ← 5. Modern Attention\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основни библиотеки\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Настройки\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Възпроизводимост\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"Библиотеките са заредени успешно.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Recap и мотивация\n",
    "\n",
    "### Какво научихме в Лекция 4\n",
    "\n",
    "| Концепция | Описание |\n",
    "|-----------|----------|\n",
    "| **Self-Attention** | Всяка позиция \"вижда\" всяка друга |\n",
    "| **Q, K, V** | Query търси, Key идентифицира, Value съдържа |\n",
    "| **Scaled Dot-Product** | $\\text{Attention} = \\text{softmax}(QK^T / \\sqrt{d_k})V$ |\n",
    "| **Multi-Head** | Множество паралелни attention patterns |\n",
    "| **Positional Encodings** | Добавят информация за реда |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какво липсва?\n",
    "\n",
    "Multi-head attention е **ядрото**, но не е достатъчно:\n",
    "\n",
    "- Как се стекват слоеве?\n",
    "- Какво още има в transformer блок?\n",
    "- Как от 512 токена стигаме до 1M?\n",
    "\n",
    "**Тази лекция:** Пълна transformer архитектура + модерни оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Пълният Transformer блок\n",
    "\n",
    "### Архитектура на блока\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "Layer Norm\n",
    "  ↓\n",
    "Multi-Head Self-Attention\n",
    "  ↓ (+) Residual Connection\n",
    "Layer Norm\n",
    "  ↓\n",
    "Feed-Forward Network\n",
    "  ↓ (+) Residual Connection\n",
    "Output\n",
    "```\n",
    "\n",
    "Нека разгледаме всеки компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Network (FFN)\n",
    "\n",
    "Две линейни трансформации с нелинейност между тях:\n",
    "\n",
    "$$\\text{FFN}(x) = W_2 \\cdot \\text{GELU}(W_1 x + b_1) + b_2$$\n",
    "\n",
    "**Размерности:**\n",
    "- Вход: $d_{model}$\n",
    "- Скрит слой: $4 \\times d_{model}$ (типично)\n",
    "- Изход: $d_{model}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример: GPT-2 размерности\n",
    "d_model = 768\n",
    "d_ff = 4 * d_model  # 3072\n",
    "\n",
    "print(f\"GPT-2 FFN: {d_model} → {d_ff} → {d_model}\")\n",
    "print(f\"Параметри: {d_model * d_ff + d_ff + d_ff * d_model + d_model:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Защо FFN?\n",
    "\n",
    "| Attention | FFN |\n",
    "|-----------|-----|\n",
    "| Смесва информация **между** позиции | Обработва всяка позиция **независимо** |\n",
    "| Линейна операция (без activation) | Добавя нелинейност |\n",
    "| \"Комуникация\" | \"Изчисление\" |\n",
    "\n",
    "**Интересно откритие:** FFN слоевете съхраняват фактически знания!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network.\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        x = self.linear1(x)         # → [batch, seq_len, d_ff]\n",
    "        x = F.gelu(x)               # Нелинейност\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)         # → [batch, seq_len, d_model]\n",
    "        return x\n",
    "\n",
    "# Тест\n",
    "ffn = FeedForward(d_model=64, d_ff=256)\n",
    "x = torch.randn(2, 10, 64)\n",
    "print(f\"Input: {x.shape} → Output: {ffn(x).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation функции\n",
    "\n",
    "| Функция | Формула | Използва се от |\n",
    "|---------|---------|----------------|\n",
    "| ReLU | $\\max(0, x)$ | Original Transformer |\n",
    "| GELU | $x \\cdot \\Phi(x)$ | GPT, BERT |\n",
    "| SwiGLU | $\\text{Swish}(xW_1) \\odot xV$ | LLaMA, Mistral |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение на activation функции\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(x.numpy(), F.relu(x).numpy(), label='ReLU', linewidth=2)\n",
    "ax.plot(x.numpy(), F.gelu(x).numpy(), label='GELU', linewidth=2)\n",
    "ax.plot(x.numpy(), F.silu(x).numpy(), label='SiLU/Swish', linewidth=2)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('x'); ax.set_ylabel('f(x)')\n",
    "ax.set_title('Сравнение на Activation функции')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"GELU и SiLU са 'гладки' версии на ReLU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SwiGLU: Модерна алтернатива\n",
    "\n",
    "Използва се от LLaMA, Mistral, повечето модерни модели:\n",
    "\n",
    "$$\\text{SwiGLU}(x) = \\text{Swish}(xW_1) \\odot (xV)$$\n",
    "\n",
    "**Gated Linear Unit:** Част от входа контролира \"врата\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"SwiGLU activation (LLaMA-style FFN).\"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, d_ff, bias=False)  # Gate\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "# Тест\n",
    "swiglu = SwiGLU(d_model=64, d_ff=256)\n",
    "x = torch.randn(2, 10, 64)\n",
    "print(f\"SwiGLU: {x.shape} → {swiglu(x).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connections (Skip Connections)\n",
    "\n",
    "$$\\text{output} = x + \\text{Sublayer}(x)$$\n",
    "\n",
    "**Защо са критични?**\n",
    "\n",
    "- Позволяват обучение на дълбоки мрежи (96+ слоя)\n",
    "- Директен път за градиента от изход към вход\n",
    "- Слоевете научават \"подобрения\", не цялата трансформация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация: Gradient flow с и без residuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Без residuals\n",
    "ax = axes[0]\n",
    "layers = ['Layer 1', 'Layer 2', 'Layer 3', 'Layer 4', 'Output']\n",
    "for i, layer in enumerate(layers):\n",
    "    ax.add_patch(plt.Rectangle((0.3, i*0.8), 0.4, 0.5, \n",
    "                               fill=True, color='steelblue', alpha=0.7))\n",
    "    ax.text(0.5, i*0.8 + 0.25, layer, ha='center', va='center', \n",
    "            fontsize=10, color='white')\n",
    "    if i < len(layers) - 1:\n",
    "        ax.annotate('', xy=(0.5, (i+1)*0.8), xytext=(0.5, i*0.8 + 0.5),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "ax.set_xlim(0, 1); ax.set_ylim(-0.3, 4)\n",
    "ax.axis('off')\n",
    "ax.set_title('Без Residuals\\n(градиент минава през всеки слой)', fontsize=12)\n",
    "\n",
    "# С residuals\n",
    "ax = axes[1]\n",
    "for i, layer in enumerate(layers):\n",
    "    ax.add_patch(plt.Rectangle((0.3, i*0.8), 0.4, 0.5, \n",
    "                               fill=True, color='steelblue', alpha=0.7))\n",
    "    ax.text(0.5, i*0.8 + 0.25, layer, ha='center', va='center', \n",
    "            fontsize=10, color='white')\n",
    "    if i < len(layers) - 1:\n",
    "        ax.annotate('', xy=(0.5, (i+1)*0.8), xytext=(0.5, i*0.8 + 0.5),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "# Skip connections\n",
    "ax.annotate('', xy=(0.85, 3.2), xytext=(0.85, 0.0),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=3, \n",
    "                           connectionstyle='arc3,rad=0.3'))\n",
    "ax.text(0.95, 1.6, 'Skip\\npath', fontsize=10, color='green', ha='left')\n",
    "\n",
    "ax.set_xlim(0, 1.2); ax.set_ylim(-0.3, 4)\n",
    "ax.axis('off')\n",
    "ax.set_title('С Residuals\\n(директен път за градиента)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
    "\n",
    "- $\\mu, \\sigma$ се изчисляват по **feature** измерението (за всяка позиция)\n",
    "- $\\gamma, \\beta$ са научени параметри"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Защо LayerNorm, а не BatchNorm?\n",
    "\n",
    "| BatchNorm | LayerNorm |\n",
    "|-----------|----------|\n",
    "| Нормализира по batch измерение | Нормализира по feature измерение |\n",
    "| Зависи от batch статистики | Независим за всеки пример |\n",
    "| Проблем с различни дължини | Работи с всякаква дължина |\n",
    "| Различно поведение train/inference | Еднакво поведение |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNorm от scratch\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "# Сравнение с PyTorch\n",
    "ln_custom = LayerNorm(64)\n",
    "ln_pytorch = nn.LayerNorm(64)\n",
    "\n",
    "x = torch.randn(2, 10, 64)\n",
    "out_custom = ln_custom(x)\n",
    "out_pytorch = ln_pytorch(x)\n",
    "\n",
    "print(f\"Custom LayerNorm: mean={out_custom.mean().item():.4f}, std={out_custom.std().item():.4f}\")\n",
    "print(f\"PyTorch LayerNorm: mean={out_pytorch.mean().item():.4f}, std={out_pytorch.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Norm vs Post-Norm\n",
    "\n",
    "**Post-Norm (Original Transformer):**\n",
    "$$x' = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
    "\n",
    "**Pre-Norm (Modern):**\n",
    "$$x' = x + \\text{Sublayer}(\\text{LayerNorm}(x))$$\n",
    "\n",
    "**Pre-norm е по-стабилен** за обучение на дълбоки модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация на Pre-Norm vs Post-Norm\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Post-Norm\n",
    "ax = axes[0]\n",
    "blocks = [\n",
    "    ('Input', 0, 'lightblue'),\n",
    "    ('Attention', 1, 'coral'),\n",
    "    ('Add', 2, 'lightgreen'),\n",
    "    ('LayerNorm', 3, 'gold'),\n",
    "    ('FFN', 4, 'coral'),\n",
    "    ('Add', 5, 'lightgreen'),\n",
    "    ('LayerNorm', 6, 'gold'),\n",
    "]\n",
    "for name, i, color in blocks:\n",
    "    ax.add_patch(plt.Rectangle((0.3, i*0.6), 0.4, 0.4, fill=True, color=color))\n",
    "    ax.text(0.5, i*0.6 + 0.2, name, ha='center', va='center', fontsize=9)\n",
    "ax.set_xlim(0, 1); ax.set_ylim(-0.2, 4.5)\n",
    "ax.axis('off')\n",
    "ax.set_title('Post-Norm (Original)', fontsize=12)\n",
    "\n",
    "# Pre-Norm\n",
    "ax = axes[1]\n",
    "blocks = [\n",
    "    ('Input', 0, 'lightblue'),\n",
    "    ('LayerNorm', 1, 'gold'),\n",
    "    ('Attention', 2, 'coral'),\n",
    "    ('Add', 3, 'lightgreen'),\n",
    "    ('LayerNorm', 4, 'gold'),\n",
    "    ('FFN', 5, 'coral'),\n",
    "    ('Add', 6, 'lightgreen'),\n",
    "]\n",
    "for name, i, color in blocks:\n",
    "    ax.add_patch(plt.Rectangle((0.3, i*0.6), 0.4, 0.4, fill=True, color=color))\n",
    "    ax.text(0.5, i*0.6 + 0.2, name, ha='center', va='center', fontsize=9)\n",
    "ax.set_xlim(0, 1); ax.set_ylim(-0.2, 4.5)\n",
    "ax.axis('off')\n",
    "ax.set_title('Pre-Norm (Modern)', fontsize=12)\n",
    "\n",
    "plt.suptitle('Pre-Norm: LayerNorm преди sublayer, по-стабилно обучение', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSNorm: Още по-просто\n",
    "\n",
    "Използва се от LLaMA, Mistral:\n",
    "\n",
    "$$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_i x_i^2 + \\epsilon}} \\cdot \\gamma$$\n",
    "\n",
    "**Предимство:** Без изваждане на средното → по-бързо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.weight\n",
    "\n",
    "# Тест\n",
    "rms_norm = RMSNorm(64)\n",
    "x = torch.randn(2, 10, 64)\n",
    "print(f\"RMSNorm: {x.shape} → {rms_norm(x).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пълен Transformer блок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention (от Лекция 4).\"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.W_o(context), attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Пълен Transformer блок (Pre-Norm).\"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention sublayer\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # FFN sublayer\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm: LayerNorm → Sublayer → Add\n",
    "        \n",
    "        # Attention sublayer\n",
    "        normed = self.norm1(x)\n",
    "        attn_out, attn_weights = self.attention(normed, mask)\n",
    "        x = x + self.dropout(attn_out)  # Residual\n",
    "        \n",
    "        # FFN sublayer\n",
    "        normed = self.norm2(x)\n",
    "        ffn_out = self.ffn(normed)\n",
    "        x = x + self.dropout(ffn_out)   # Residual\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "# Тест\n",
    "block = TransformerBlock(d_model=64, n_heads=8, d_ff=256)\n",
    "x = torch.randn(2, 10, 64)\n",
    "out, weights = block(x)\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {out.shape}\")\n",
    "print(f\"Attention weights: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Размерности през блока"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проследяване на размерностите\n",
    "batch, seq_len, d_model = 2, 10, 64\n",
    "n_heads, d_ff = 8, 256\n",
    "\n",
    "print(\"Размерности през Transformer блок:\")\n",
    "print(f\"\")\n",
    "print(f\"Input:           [{batch}, {seq_len}, {d_model}]\")\n",
    "print(f\"↓ LayerNorm:     [{batch}, {seq_len}, {d_model}]\")\n",
    "print(f\"↓ Q, K, V proj:  [{batch}, {seq_len}, {d_model}] → split heads\")\n",
    "print(f\"↓ Per head:      [{batch}, {n_heads}, {seq_len}, {d_model//n_heads}]\")\n",
    "print(f\"↓ Attention:     [{batch}, {n_heads}, {seq_len}, {seq_len}] (weights)\")\n",
    "print(f\"↓ Context:       [{batch}, {n_heads}, {seq_len}, {d_model//n_heads}]\")\n",
    "print(f\"↓ Concat heads:  [{batch}, {seq_len}, {d_model}]\")\n",
    "print(f\"↓ + Residual:    [{batch}, {seq_len}, {d_model}]\")\n",
    "print(f\"↓ LayerNorm:     [{batch}, {seq_len}, {d_model}]\")\n",
    "print(f\"↓ FFN up:        [{batch}, {seq_len}, {d_ff}]\")\n",
    "print(f\"↓ FFN down:      [{batch}, {seq_len}, {d_model}]\")\n",
    "print(f\"↓ + Residual:    [{batch}, {seq_len}, {d_model}]\")\n",
    "print(f\"Output:          [{batch}, {seq_len}, {d_model}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Стекване на слоеве\n",
    "\n",
    "### Дълбока Transformer архитектура\n",
    "\n",
    "Стекваме N идентични блока:\n",
    "\n",
    "| Модел | Слоеве | $d_{model}$ | Heads | Параметри |\n",
    "|-------|--------|-------------|-------|-----------|\n",
    "| BERT-base | 12 | 768 | 12 | 110M |\n",
    "| GPT-2 | 12-48 | 768-1600 | 12-25 | 124M-1.5B |\n",
    "| GPT-3 | 96 | 12288 | 96 | 175B |\n",
    "| LLaMA 70B | 80 | 8192 | 64 | 70B |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Пълен Transformer (Decoder-only).\"\"\"\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, \n",
    "                 max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, idx, mask=None):\n",
    "        batch, seq_len = idx.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.pos_embedding(torch.arange(seq_len, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x, _ = block(x, mask)\n",
    "        \n",
    "        # Output\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# GPT-2-like модел (малък)\n",
    "model = Transformer(\n",
    "    vocab_size=50257,\n",
    "    d_model=768,\n",
    "    n_layers=12,\n",
    "    n_heads=12,\n",
    "    d_ff=3072,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Брой параметри\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,} (~{total_params/1e6:.0f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какво научават различните слоеве?\n",
    "\n",
    "Изследвания показват специализация:\n",
    "\n",
    "| Слоеве | Научени features |\n",
    "|--------|------------------|\n",
    "| **Ранни** (1-4) | Синтаксис, локални зависимости, POS-like |\n",
    "| **Средни** (5-8) | Семантика на фразово ниво, coreference |\n",
    "| **Късни** (9-12) | Task-specific, високо абстрактни |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скалиране на параметри\n",
    "\n",
    "Всеки блок добавя:\n",
    "- **Attention:** $4 \\times d_{model}^2$ (Q, K, V, O проекции)\n",
    "- **FFN:** $8 \\times d_{model}^2$ (при 4x expansion)\n",
    "- **Общо на блок:** ~$12 \\times d_{model}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ на параметри по компонент\n",
    "d_model = 768\n",
    "n_layers = 12\n",
    "vocab_size = 50257\n",
    "\n",
    "# Per block\n",
    "attn_params = 4 * d_model * d_model  # W_q, W_k, W_v, W_o\n",
    "ffn_params = 2 * d_model * (4 * d_model)  # W1, W2\n",
    "ln_params = 2 * 2 * d_model  # 2 layer norms, gamma + beta\n",
    "\n",
    "block_params = attn_params + ffn_params + ln_params\n",
    "all_blocks = block_params * n_layers\n",
    "\n",
    "# Embeddings\n",
    "embed_params = vocab_size * d_model + 1024 * d_model  # token + position\n",
    "\n",
    "# Output head (tied with embeddings in some models)\n",
    "head_params = vocab_size * d_model\n",
    "\n",
    "print(\"GPT-2 (124M) параметри:\")\n",
    "print(f\"  Per attention: {attn_params:,}\")\n",
    "print(f\"  Per FFN: {ffn_params:,}\")\n",
    "print(f\"  Per block: {block_params:,}\")\n",
    "print(f\"  All blocks ({n_layers}): {all_blocks:,}\")\n",
    "print(f\"  Embeddings: {embed_params:,}\")\n",
    "print(f\"  Output head: {head_params:,}\")\n",
    "print(f\"  Total: {all_blocks + embed_params + head_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация на параметри по компонент\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "components = ['Attention\\n(12 layers)', 'FFN\\n(12 layers)', 'Token\\nEmbeddings', \n",
    "              'Position\\nEmbeddings', 'Output\\nHead']\n",
    "params = [attn_params * n_layers, ffn_params * n_layers, \n",
    "          vocab_size * d_model, 1024 * d_model, head_params]\n",
    "colors = ['coral', 'steelblue', 'green', 'gold', 'purple']\n",
    "\n",
    "bars = ax.bar(components, [p/1e6 for p in params], color=colors, alpha=0.8)\n",
    "ax.set_ylabel('Параметри (милиони)')\n",
    "ax.set_title('GPT-2 (124M): Разпределение на параметрите')\n",
    "\n",
    "for bar, p in zip(bars, params):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "            f'{p/1e6:.1f}M', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"FFN е най-големият компонент ({ffn_params * n_layers / (all_blocks + embed_params + head_params) * 100:.0f}%)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Encoder vs Decoder архитектури\n",
    "\n",
    "### Три основни варианта\n",
    "\n",
    "| Архитектура | Attention | Примери | Задачи |\n",
    "|-------------|-----------|---------|--------|\n",
    "| **Encoder-only** | Bidirectional | BERT, RoBERTa | Classification, NER |\n",
    "| **Decoder-only** | Causal | GPT, LLaMA | Generation |\n",
    "| **Encoder-Decoder** | Both + Cross | T5, BART | Translation, Summarization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Only (BERT)\n",
    "\n",
    "**Bidirectional attention:** Всяка позиция вижда всички други.\n",
    "\n",
    "```\n",
    "The cat [MASK] on mat\n",
    " ↓   ↓    ↓    ↓   ↓\n",
    "All ←→ positions ←→ connected\n",
    "```\n",
    "\n",
    "**Предимство:** Пълен контекст за всяка позиция.  \n",
    "**Недостатък:** Не може да генерира текст директно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder-Only (GPT, LLaMA)\n",
    "\n",
    "**Causal attention:** Позиция i вижда само позиции ≤ i.\n",
    "\n",
    "```\n",
    "The cat sat on mat\n",
    " ↓   ↓   ↓   ↓   ↓\n",
    " 1 → 2 → 3 → 4 → 5\n",
    "```\n",
    "\n",
    "**Доминира в модерните LLM!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация: Bidirectional vs Causal attention\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "tokens = ['The', 'cat', 'sat', 'on', 'mat']\n",
    "n = len(tokens)\n",
    "\n",
    "# Bidirectional (BERT)\n",
    "ax = axes[0]\n",
    "mask_bidir = np.ones((n, n))\n",
    "sns.heatmap(mask_bidir, annot=True, fmt='.0f', cmap='Greens',\n",
    "            xticklabels=tokens, yticklabels=tokens, ax=ax, cbar=False)\n",
    "ax.set_title('Encoder (BERT): Bidirectional\\n(всичко вижда всичко)', fontsize=11)\n",
    "ax.set_xlabel('Key'); ax.set_ylabel('Query')\n",
    "\n",
    "# Causal (GPT)\n",
    "ax = axes[1]\n",
    "mask_causal = np.tril(np.ones((n, n)))\n",
    "sns.heatmap(mask_causal, annot=True, fmt='.0f', cmap='Blues',\n",
    "            xticklabels=tokens, yticklabels=tokens, ax=ax, cbar=False)\n",
    "ax.set_title('Decoder (GPT): Causal\\n(само предишни позиции)', fontsize=11)\n",
    "ax.set_xlabel('Key'); ax.set_ylabel('Query')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Защо Decoder-only доминира?\n",
    "\n",
    "| Причина | Обяснение |\n",
    "|---------|-----------|\n",
    "| **По-проста архитектура** | Един тип attention |\n",
    "| **Natural за генерация** | Next-token prediction |\n",
    "| **Unified approach** | Всички задачи като text generation |\n",
    "| **По-добро scaling** | Емпирично доказано |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder (T5, BART)\n",
    "\n",
    "```\n",
    "Encoder                    Decoder\n",
    "  ↓                          ↓\n",
    "Bidirectional    →    Causal + Cross-attention\n",
    "  ↓                          ↓\n",
    "Input representation    Output generation\n",
    "```\n",
    "\n",
    "**Cross-attention:** Decoder \"пита\" encoder outputs.\n",
    "\n",
    "**По-малко популярен днес:** Decoder-only може същите задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Модерни Attention варианти\n",
    "\n",
    "### Проблемът с ефективността\n",
    "\n",
    "Standard Multi-Head Attention:\n",
    "- **Памет за weights:** O(n²) за всеки head\n",
    "- **Compute:** O(n²d)\n",
    "\n",
    "**При дълъг контекст:** Памет и compute експлодират!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O(n²) scaling\n",
    "seq_lengths = np.array([512, 1024, 2048, 4096, 8192, 16384, 32768])\n",
    "n_heads = 32\n",
    "d_k = 64\n",
    "\n",
    "# Памет за attention weights: n² * n_heads * 4 bytes (float32)\n",
    "memory_weights_mb = (seq_lengths ** 2 * n_heads * 4) / (1024 ** 2)\n",
    "\n",
    "# KV cache памет: 2 * n_heads * seq_len * d_k * n_layers * 4 bytes\n",
    "n_layers = 32\n",
    "kv_cache_mb = (2 * n_heads * seq_lengths * d_k * n_layers * 4) / (1024 ** 2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(seq_lengths / 1000, memory_weights_mb / 1000, 'o-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Sequence Length (K tokens)')\n",
    "ax.set_ylabel('Memory for Attention Weights (GB)')\n",
    "ax.set_title('O(n²) Memory Scaling за Attention Weights')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(seq_lengths / 1000, kv_cache_mb / 1000, 'o-', linewidth=2, markersize=8, color='coral')\n",
    "ax.set_xlabel('Sequence Length (K tokens)')\n",
    "ax.set_ylabel('KV Cache Size (GB)')\n",
    "ax.set_title('O(n) KV Cache Scaling (32 layers, 32 heads)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"При 32K tokens: {memory_weights_mb[-1]/1000:.1f} GB само за attention weights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Query Attention (MQA)\n",
    "\n",
    "**Идея:** Споделяме K, V между всички heads, само Q е per-head.\n",
    "\n",
    "| Компонент | Standard MHA | MQA |\n",
    "|-----------|--------------|-----|\n",
    "| Q projections | h | h |\n",
    "| K projections | h | **1** |\n",
    "| V projections | h | **1** |\n",
    "| KV cache | $2 \\times h \\times n \\times d_k$ | $2 \\times n \\times d_k$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"Multi-Query Attention: Shared K, V across heads.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Separate Q for each head\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # d_model = n_heads * d_k\n",
    "        \n",
    "        # SHARED K, V (single head dimension)\n",
    "        self.W_k = nn.Linear(d_model, self.d_k)  # Only d_k, not d_model!\n",
    "        self.W_v = nn.Linear(d_model, self.d_k)\n",
    "        \n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Q: [batch, seq, n_heads, d_k]\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        Q = Q.transpose(1, 2)  # [batch, n_heads, seq, d_k]\n",
    "        \n",
    "        # K, V: [batch, seq, d_k] - SHARED!\n",
    "        K = self.W_k(x)  # [batch, seq, d_k]\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Expand K, V to match heads: [batch, 1, seq, d_k] → broadcast\n",
    "        K = K.unsqueeze(1)  # [batch, 1, seq, d_k]\n",
    "        V = V.unsqueeze(1)\n",
    "        \n",
    "        # Attention (K, V broadcast across heads)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.W_o(context), attn_weights\n",
    "\n",
    "# Сравнение на параметри\n",
    "mha = MultiHeadAttention(d_model=512, n_heads=8)\n",
    "mqa = MultiQueryAttention(d_model=512, n_heads=8)\n",
    "\n",
    "mha_params = sum(p.numel() for p in mha.parameters())\n",
    "mqa_params = sum(p.numel() for p in mqa.parameters())\n",
    "\n",
    "print(f\"MHA parameters: {mha_params:,}\")\n",
    "print(f\"MQA parameters: {mqa_params:,}\")\n",
    "print(f\"MQA reduction: {(1 - mqa_params/mha_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped-Query Attention (GQA)\n",
    "\n",
    "**Компромис:** Групи от heads споделят K, V.\n",
    "\n",
    "**Пример:** 32 query heads, 8 KV heads → всеки 4 Q heads споделят K, V.\n",
    "\n",
    "| Вариант | Query heads | KV heads | KV cache размер |\n",
    "|---------|-------------|----------|----------------|\n",
    "| MHA | 32 | 32 | 32x |\n",
    "| GQA | 32 | 8 | 8x |\n",
    "| MQA | 32 | 1 | 1x |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"Grouped-Query Attention: Groups of Q heads share K, V.\"\"\"\n",
    "    def __init__(self, d_model, n_heads, n_kv_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        assert n_heads % n_kv_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_heads // n_kv_heads  # How many Q heads per KV head\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Q: full heads\n",
    "        self.W_q = nn.Linear(d_model, n_heads * self.d_k)\n",
    "        \n",
    "        # K, V: fewer heads\n",
    "        self.W_k = nn.Linear(d_model, n_kv_heads * self.d_k)\n",
    "        self.W_v = nn.Linear(d_model, n_kv_heads * self.d_k)\n",
    "        \n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Q: [batch, seq, n_heads, d_k]\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        Q = Q.transpose(1, 2)  # [batch, n_heads, seq, d_k]\n",
    "        \n",
    "        # K, V: [batch, seq, n_kv_heads, d_k]\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
    "        K = K.transpose(1, 2)  # [batch, n_kv_heads, seq, d_k]\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        # Repeat K, V for each group\n",
    "        # [batch, n_kv_heads, seq, d_k] → [batch, n_heads, seq, d_k]\n",
    "        K = K.repeat_interleave(self.n_rep, dim=1)\n",
    "        V = V.repeat_interleave(self.n_rep, dim=1)\n",
    "        \n",
    "        # Standard attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.W_o(context), attn_weights\n",
    "\n",
    "# LLaMA 2 style: 32 query heads, 8 KV heads\n",
    "gqa = GroupedQueryAttention(d_model=512, n_heads=32, n_kv_heads=8)\n",
    "\n",
    "x = torch.randn(2, 10, 512)\n",
    "out, weights = gqa(x)\n",
    "\n",
    "print(f\"GQA: {x.shape} → {out.shape}\")\n",
    "print(f\"Attention weights: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация: MHA vs GQA vs MQA\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "def draw_heads(ax, n_q, n_kv, title):\n",
    "    # Draw Q heads\n",
    "    for i in range(n_q):\n",
    "        ax.add_patch(plt.Rectangle((i*0.3, 2), 0.25, 0.4, \n",
    "                                   fill=True, color='steelblue', alpha=0.8))\n",
    "        ax.text(i*0.3 + 0.125, 2.2, f'Q{i}', ha='center', va='center', \n",
    "                fontsize=8, color='white')\n",
    "    \n",
    "    # Draw KV heads\n",
    "    kv_width = n_q * 0.3 / n_kv\n",
    "    for i in range(n_kv):\n",
    "        ax.add_patch(plt.Rectangle((i*kv_width, 0.5), kv_width - 0.05, 0.4, \n",
    "                                   fill=True, color='coral', alpha=0.8))\n",
    "        ax.text(i*kv_width + kv_width/2 - 0.025, 0.7, f'KV{i}', \n",
    "                ha='center', va='center', fontsize=8, color='white')\n",
    "    \n",
    "    # Draw connections\n",
    "    for qi in range(n_q):\n",
    "        kvi = qi * n_kv // n_q\n",
    "        ax.plot([qi*0.3 + 0.125, kvi*kv_width + kv_width/2 - 0.025], \n",
    "                [2, 0.9], 'k-', alpha=0.3)\n",
    "    \n",
    "    ax.set_xlim(-0.2, n_q * 0.3 + 0.1)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=11)\n",
    "\n",
    "draw_heads(axes[0], 8, 8, 'MHA\\n(8 Q heads, 8 KV heads)')\n",
    "draw_heads(axes[1], 8, 2, 'GQA\\n(8 Q heads, 2 KV heads)')\n",
    "draw_heads(axes[2], 8, 1, 'MQA\\n(8 Q heads, 1 KV head)')\n",
    "\n",
    "plt.suptitle('Query-Key-Value Head Sharing', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flash Attention\n",
    "\n",
    "**Проблемът:** Standard attention изисква O(n²) памет.\n",
    "\n",
    "**Flash Attention идея:**\n",
    "- Не материализираме пълната attention матрица\n",
    "- Изчисляваме на блокове (tiles) в бърза SRAM памет\n",
    "- **O(n²) compute, O(n) memory!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Memory Hierarchy\n",
    "\n",
    "```\n",
    "SRAM (on-chip):    ~20 MB, ~19 TB/s\n",
    "HBM (GPU memory):  ~40 GB, ~1.5 TB/s\n",
    "```\n",
    "\n",
    "**Memory bandwidth е bottleneck**, не compute!\n",
    "\n",
    "Flash Attention:\n",
    "- Load Q, K, V на блокове в SRAM\n",
    "- Compute attention локално\n",
    "- Accumulate outputs\n",
    "- Never store full n×n matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code за Flash Attention (концептуално)\n",
    "def flash_attention_pseudocode(Q, K, V, block_size=64):\n",
    "    \"\"\"\n",
    "    Flash Attention алгоритъм (опростен).\n",
    "    \n",
    "    Реалната имплементация е в CUDA и много по-сложна.\n",
    "    \"\"\"\n",
    "    seq_len, d = Q.shape\n",
    "    output = np.zeros_like(Q)\n",
    "    \n",
    "    # Process in blocks\n",
    "    for i in range(0, seq_len, block_size):\n",
    "        q_block = Q[i:i+block_size]  # Load Q block to SRAM\n",
    "        \n",
    "        # Running statistics for softmax\n",
    "        max_score = -np.inf\n",
    "        sum_exp = 0\n",
    "        acc = np.zeros((block_size, d))\n",
    "        \n",
    "        for j in range(0, seq_len, block_size):\n",
    "            k_block = K[j:j+block_size]  # Load K block\n",
    "            v_block = V[j:j+block_size]  # Load V block\n",
    "            \n",
    "            # Compute attention scores (in SRAM)\n",
    "            scores = q_block @ k_block.T / np.sqrt(d)\n",
    "            \n",
    "            # Online softmax update\n",
    "            # ... complex math to maintain running max/sum ...\n",
    "            \n",
    "            # Accumulate output\n",
    "            # acc += softmax(scores) @ v_block\n",
    "        \n",
    "        output[i:i+block_size] = acc  # Write back to HBM\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(\"Flash Attention:\")\n",
    "print(\"  - 2-4x speedup in practice\")\n",
    "print(\"  - O(n²) compute, O(n) memory\")\n",
    "print(\"  - Standard за всички модерни LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение на memory: Standard vs Flash\n",
    "seq_lengths = np.array([1024, 2048, 4096, 8192, 16384, 32768, 65536])\n",
    "d_model = 4096\n",
    "n_heads = 32\n",
    "\n",
    "# Standard: store full n×n attention matrix\n",
    "standard_memory_gb = (seq_lengths ** 2 * n_heads * 4) / (1024 ** 3)\n",
    "\n",
    "# Flash: only store O(n) intermediate results\n",
    "flash_memory_gb = (seq_lengths * d_model * 4 * 4) / (1024 ** 3)  # ~4x for intermediates\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.semilogy(seq_lengths / 1000, standard_memory_gb, 'o-', \n",
    "            label='Standard Attention', linewidth=2, markersize=8)\n",
    "ax.semilogy(seq_lengths / 1000, flash_memory_gb, 's-', \n",
    "            label='Flash Attention', linewidth=2, markersize=8)\n",
    "ax.axhline(y=40, color='red', linestyle='--', label='A100 40GB limit')\n",
    "\n",
    "ax.set_xlabel('Sequence Length (K tokens)')\n",
    "ax.set_ylabel('Memory (GB, log scale)')\n",
    "ax.set_title('Memory Scaling: Standard vs Flash Attention')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"При 64K tokens:\")\n",
    "print(f\"  Standard: {standard_memory_gb[-1]:.0f} GB (невъзможно!)\")\n",
    "print(f\"  Flash: {flash_memory_gb[-1]:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Long Context: От 512 до 1M+ токена\n",
    "\n",
    "### Еволюция на context length\n",
    "\n",
    "| Модел | Година | Context |\n",
    "|-------|--------|--------|\n",
    "| Original Transformer | 2017 | 512 |\n",
    "| GPT-2 | 2019 | 1,024 |\n",
    "| GPT-3 | 2020 | 2,048 |\n",
    "| GPT-4 | 2023 | 8K-128K |\n",
    "| Claude 3 | 2024 | 200K |\n",
    "| Gemini 1.5 | 2024 | 1M+ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Защо дълъг контекст е важен?\n",
    "\n",
    "- **Дълги документи:** Книги, правни текстове, код\n",
    "- **Повече примери:** In-context learning с много shots\n",
    "- **Сложно reasoning:** Multi-step задачи\n",
    "- **Code repositories:** Цели проекти в контекста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемът с позициите\n",
    "\n",
    "**Learned positional embeddings:**\n",
    "- Фиксиран максимум $L_{max}$\n",
    "- Не генерализира отвъд training length\n",
    "\n",
    "**Sinusoidal:**\n",
    "- Теоретично unlimited\n",
    "- На практика лоша екстраполация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary Position Embeddings (RoPE)\n",
    "\n",
    "**Идея:** Кодираме позицията чрез ротация на Q, K векторите.\n",
    "\n",
    "$$\\text{Attention}(R_{pos_q} \\cdot q, R_{pos_k} \\cdot k, v)$$\n",
    "\n",
    "Където $R_{pos}$ е ротационна матрица.\n",
    "\n",
    "**Ключово свойство:** $q^T R_{pos_q}^T R_{pos_k} k$ зависи от $pos_q - pos_k$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE: Интуиция\n",
    "\n",
    "- Всяка двойка измерения се завърта на различен ъгъл\n",
    "- Ъгълът зависи от позицията\n",
    "- Dot product \"вижда\" относителната позиция\n",
    "\n",
    "**Предимства:**\n",
    "- По-добра екстраполация от learned/sinusoidal\n",
    "- Ефективно изчисление\n",
    "- **Използва се от:** LLaMA, Mistral, повечето модерни модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_freqs(dim, max_seq_len, base=10000):\n",
    "    \"\"\"Precompute RoPE frequencies.\"\"\"\n",
    "    # Frequencies for each dimension pair\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    \n",
    "    # Position indices\n",
    "    t = torch.arange(max_seq_len)\n",
    "    \n",
    "    # Outer product: [seq_len, dim//2]\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    \n",
    "    # Complex exponential: e^(i*theta) = cos(theta) + i*sin(theta)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    \n",
    "    return freqs_cis\n",
    "\n",
    "def apply_rope(x, freqs_cis):\n",
    "    \"\"\"Apply RoPE to input tensor.\"\"\"\n",
    "    # x: [batch, seq_len, n_heads, d_k]\n",
    "    # Reshape to complex: [batch, seq_len, n_heads, d_k//2, 2] → complex\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    \n",
    "    # Apply rotation\n",
    "    freqs_cis = freqs_cis[:x.shape[1], :].unsqueeze(0).unsqueeze(2)  # [1, seq, 1, d_k//2]\n",
    "    x_rotated = x_complex * freqs_cis\n",
    "    \n",
    "    # Back to real\n",
    "    return torch.view_as_real(x_rotated).flatten(-2).type_as(x)\n",
    "\n",
    "# Demo\n",
    "d_k = 64\n",
    "freqs = precompute_rope_freqs(d_k, max_seq_len=1000)\n",
    "print(f\"RoPE frequencies shape: {freqs.shape}\")\n",
    "\n",
    "x = torch.randn(2, 10, 8, d_k)  # [batch, seq, heads, d_k]\n",
    "x_rotated = apply_rope(x, freqs)\n",
    "print(f\"Input: {x.shape} → Rotated: {x_rotated.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация на RoPE ротации\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Rotation angles за различни позиции и измерения\n",
    "d_k = 32\n",
    "positions = torch.arange(100)\n",
    "base = 10000\n",
    "freqs = 1.0 / (base ** (torch.arange(0, d_k, 2).float() / d_k))\n",
    "angles = torch.outer(positions, freqs).numpy()\n",
    "\n",
    "ax = axes[0]\n",
    "im = ax.imshow(angles[:50, :], aspect='auto', cmap='twilight')\n",
    "ax.set_xlabel('Dimension pair')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_title('RoPE Rotation Angles (θ)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 2D rotation visualization\n",
    "ax = axes[1]\n",
    "# Show how a 2D vector rotates with position\n",
    "for pos in [0, 5, 10, 20, 50]:\n",
    "    theta = pos * freqs[0].item()  # First frequency\n",
    "    x = np.cos(theta)\n",
    "    y = np.sin(theta)\n",
    "    ax.arrow(0, 0, x*0.8, y*0.8, head_width=0.05, head_length=0.05, \n",
    "             fc=plt.cm.viridis(pos/50), ec=plt.cm.viridis(pos/50), linewidth=2)\n",
    "    ax.text(x*0.9, y*0.9, f'pos={pos}', fontsize=9)\n",
    "\n",
    "ax.set_xlim(-1.2, 1.2)\n",
    "ax.set_ylim(-1.2, 1.2)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(0, color='gray', linewidth=0.5)\n",
    "ax.axvline(0, color='gray', linewidth=0.5)\n",
    "ax.set_title('RoPE: Ротация на вектор за различни позиции\\n(първа dimension pair)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Interpolation\n",
    "\n",
    "**Проблем:** Модел обучен на 4K, искаме 32K.\n",
    "\n",
    "**Решение:** Скалираме позициите:\n",
    "\n",
    "$$pos' = pos \\times \\frac{L_{train}}{L_{target}}$$\n",
    "\n",
    "32K позиции се \"свиват\" в 4K.\n",
    "\n",
    "**Изисква:** Кратък fine-tuning на дълги примери."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position interpolation demo\n",
    "L_train = 4096\n",
    "L_target = 32768\n",
    "scale = L_train / L_target\n",
    "\n",
    "# Original positions\n",
    "positions_original = np.arange(L_target)\n",
    "\n",
    "# Interpolated positions\n",
    "positions_interpolated = positions_original * scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(positions_original[:1000], positions_interpolated[:1000], linewidth=2)\n",
    "ax.axhline(y=L_train, color='red', linestyle='--', label=f'Training max ({L_train})')\n",
    "ax.set_xlabel('Actual Position')\n",
    "ax.set_ylabel('Interpolated Position')\n",
    "ax.set_title(f'Position Interpolation: {L_train} → {L_target}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Scale factor: {scale:.4f}\")\n",
    "print(f\"Position 32000 → {32000 * scale:.0f} (within training range)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YaRN и NTK-aware Scaling\n",
    "\n",
    "**YaRN (Yet another RoPE extensioN):**\n",
    "- Различно scaling за различни честоти\n",
    "- По-добра дългосрочна performance\n",
    "\n",
    "**NTK-aware:**\n",
    "- Модифицира base frequency в RoPE\n",
    "- Работи zero-shot (без fine-tuning)\n",
    "- Известна загуба на качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window Attention\n",
    "\n",
    "**Идея:** Всеки токен attend-ва само към локален прозорец.\n",
    "\n",
    "```\n",
    "Token i attends to: [i-w, i-w+1, ..., i-1, i]\n",
    "```\n",
    "\n",
    "- **Сложност:** O(n × w) вместо O(n²)\n",
    "- **Дълги зависимости:** Информация се пренася през слоевете\n",
    "\n",
    "**Използва се от:** Mistral, Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding window attention mask\n",
    "def create_sliding_window_mask(seq_len, window_size):\n",
    "    \"\"\"Create sliding window attention mask.\"\"\"\n",
    "    mask = torch.zeros(seq_len, seq_len)\n",
    "    for i in range(seq_len):\n",
    "        start = max(0, i - window_size + 1)\n",
    "        mask[i, start:i+1] = 1\n",
    "    return mask\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, mask) in zip(axes, [\n",
    "    ('Full Causal', torch.tril(torch.ones(20, 20))),\n",
    "    ('Sliding Window (w=5)', create_sliding_window_mask(20, 5)),\n",
    "    ('Sliding Window (w=10)', create_sliding_window_mask(20, 10)),\n",
    "]):\n",
    "    sns.heatmap(mask.numpy(), cmap='Blues', cbar=False, ax=ax,\n",
    "                xticklabels=False, yticklabels=False)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Key position')\n",
    "    ax.set_ylabel('Query position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sliding window: O(n·w) вместо O(n²)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KV Cache: Ефективна генерация\n",
    "\n",
    "**При генерация:** Всеки нов токен изисква attention към всички предишни.\n",
    "\n",
    "**Без cache:** Преизчисляваме K, V за всички позиции на всяка стъпка.\n",
    "\n",
    "**С KV cache:** Съхраняваме K, V за предишни позиции, изчисляваме само за новия токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedAttention(nn.Module):\n",
    "    \"\"\"Attention с KV Cache за ефективна генерация.\"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len, d_model]\n",
    "        cache: tuple (K_cache, V_cache) from previous steps\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Compute Q, K, V for new tokens\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K_new = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V_new = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Append to cache\n",
    "        if cache is not None:\n",
    "            K_cache, V_cache = cache\n",
    "            K = torch.cat([K_cache, K_new], dim=2)  # Concat along seq dimension\n",
    "            V = torch.cat([V_cache, V_new], dim=2)\n",
    "        else:\n",
    "            K = K_new\n",
    "            V = V_new\n",
    "        \n",
    "        # Standard attention with full K, V\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        # Return output and updated cache\n",
    "        return output, (K, V)\n",
    "\n",
    "# Demo: генерация токен по токен\n",
    "attn = CachedAttention(d_model=64, n_heads=4)\n",
    "\n",
    "# First token\n",
    "x1 = torch.randn(1, 1, 64)\n",
    "out1, cache = attn(x1, cache=None)\n",
    "print(f\"Step 1: input {x1.shape}, cache K shape: {cache[0].shape}\")\n",
    "\n",
    "# Second token\n",
    "x2 = torch.randn(1, 1, 64)\n",
    "out2, cache = attn(x2, cache=cache)\n",
    "print(f\"Step 2: input {x2.shape}, cache K shape: {cache[0].shape}\")\n",
    "\n",
    "# Third token\n",
    "x3 = torch.randn(1, 1, 64)\n",
    "out3, cache = attn(x3, cache=cache)\n",
    "print(f\"Step 3: input {x3.shape}, cache K shape: {cache[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KV Cache размер: MHA vs GQA\n",
    "seq_len = 8192\n",
    "n_layers = 32\n",
    "d_k = 128\n",
    "n_heads = 32\n",
    "n_kv_heads_gqa = 8\n",
    "\n",
    "# KV cache: 2 (K and V) * layers * heads * seq * d_k * 4 bytes\n",
    "mha_cache_gb = (2 * n_layers * n_heads * seq_len * d_k * 4) / (1024 ** 3)\n",
    "gqa_cache_gb = (2 * n_layers * n_kv_heads_gqa * seq_len * d_k * 4) / (1024 ** 3)\n",
    "mqa_cache_gb = (2 * n_layers * 1 * seq_len * d_k * 4) / (1024 ** 3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "variants = ['MHA\\n(32 KV heads)', 'GQA\\n(8 KV heads)', 'MQA\\n(1 KV head)']\n",
    "sizes = [mha_cache_gb, gqa_cache_gb, mqa_cache_gb]\n",
    "colors = ['coral', 'steelblue', 'green']\n",
    "\n",
    "bars = ax.bar(variants, sizes, color=colors, alpha=0.8)\n",
    "ax.set_ylabel('KV Cache Size (GB)')\n",
    "ax.set_title(f'KV Cache Memory (seq_len={seq_len}, {n_layers} layers)')\n",
    "\n",
    "for bar, size in zip(bars, sizes):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "            f'{size:.2f} GB', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"GQA спестява {(1 - gqa_cache_gb/mha_cache_gb)*100:.0f}% KV cache памет!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ring Attention\n",
    "\n",
    "**За много дълги последователности (1M+):**\n",
    "\n",
    "- Разпределяме sequence между множество GPU\n",
    "- Всеки GPU обработва chunk\n",
    "- KV cache се предава \"по ринг\" между GPU\n",
    "\n",
    "**Ограничение:** Общата памет на всички GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Цялостна архитектура\n",
    "\n",
    "### Модерен LLM (2024 style)\n",
    "\n",
    "**Типична конфигурация (LLaMA 3 70B):**\n",
    "\n",
    "| Компонент | Стойност |\n",
    "|-----------|----------|\n",
    "| Слоеве | 80 |\n",
    "| $d_{model}$ | 8192 |\n",
    "| Query heads | 64 |\n",
    "| KV heads | 8 (GQA) |\n",
    "| FFN | SwiGLU, ~22K hidden |\n",
    "| Normalization | RMSNorm (pre-norm) |\n",
    "| Positions | RoPE |\n",
    "| Context | 8K-128K |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение на архитектури\n",
    "models = {\n",
    "    'GPT-2 (124M)': {'layers': 12, 'd_model': 768, 'heads': 12, 'kv_heads': 12, 'context': 1024},\n",
    "    'LLaMA 7B': {'layers': 32, 'd_model': 4096, 'heads': 32, 'kv_heads': 32, 'context': 4096},\n",
    "    'LLaMA 2 70B': {'layers': 80, 'd_model': 8192, 'heads': 64, 'kv_heads': 8, 'context': 4096},\n",
    "    'Mistral 7B': {'layers': 32, 'd_model': 4096, 'heads': 32, 'kv_heads': 8, 'context': 32768},\n",
    "}\n",
    "\n",
    "print(\"Сравнение на модерни LLM архитектури:\\n\")\n",
    "print(f\"{'Model':<15} {'Layers':>7} {'d_model':>8} {'Q Heads':>8} {'KV Heads':>9} {'Context':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for name, config in models.items():\n",
    "    print(f\"{name:<15} {config['layers']:>7} {config['d_model']:>8} {config['heads']:>8} \"\n",
    "          f\"{config['kv_heads']:>9} {config['context']:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пълна \"production-ready\" архитектура\n",
    "class ModernTransformerBlock(nn.Module):\n",
    "    \"\"\"Modern Transformer block (LLaMA-style).\"\"\"\n",
    "    def __init__(self, d_model, n_heads, n_kv_heads, d_ff):\n",
    "        super().__init__()\n",
    "        \n",
    "        # RMSNorm instead of LayerNorm\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        \n",
    "        # GQA instead of MHA\n",
    "        self.attention = GroupedQueryAttention(d_model, n_heads, n_kv_heads)\n",
    "        \n",
    "        # SwiGLU instead of standard FFN\n",
    "        self.ffn = SwiGLU(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm with residual\n",
    "        h = x + self.attention(self.norm1(x), mask)[0]\n",
    "        out = h + self.ffn(self.norm2(h))\n",
    "        return out\n",
    "\n",
    "# LLaMA 2-style block\n",
    "block = ModernTransformerBlock(\n",
    "    d_model=4096, \n",
    "    n_heads=32, \n",
    "    n_kv_heads=8,  # GQA!\n",
    "    d_ff=11008     # SwiGLU dimension\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 10, 4096)\n",
    "out = block(x)\n",
    "print(f\"Modern block: {x.shape} → {out.shape}\")\n",
    "\n",
    "params = sum(p.numel() for p in block.parameters())\n",
    "print(f\"Parameters per block: {params:,} (~{params/1e6:.0f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Обобщение и мост към следващата лекция\n",
    "\n",
    "### Ключови изводи\n",
    "\n",
    "1. **Transformer блок** = Attention + FFN + Residuals + LayerNorm\n",
    "2. **Decoder-only** доминира в модерните LLM\n",
    "3. **GQA/MQA** намаляват KV cache за ефективен inference\n",
    "4. **Flash Attention** позволява по-дълги контексти\n",
    "5. **RoPE + interpolation** разширяват контекста\n",
    "6. **Модерни модели:** 100K-1M+ токена контекст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Следваща лекция: Foundation Models и Pretraining Data\n",
    "\n",
    "Архитектурата е готова. **Лекция 6** обхваща:\n",
    "\n",
    "- Pretraining objectives: MLM vs autoregressive\n",
    "- Data sources: Common Crawl, books, code\n",
    "- Data quality: filtering, deduplication\n",
    "- Scaling laws: оптимално съотношение model/data\n",
    "- Contamination и evaluation integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ресурси\n",
    "\n",
    "### Основни статии\n",
    "\n",
    "1. **\"Attention Is All You Need\"** — Vaswani et al. (2017) — Original Transformer\n",
    "2. **\"Language Models are Few-Shot Learners\"** — Brown et al. (2020) — GPT-3\n",
    "3. **\"LLaMA: Open and Efficient Foundation Language Models\"** — Touvron et al. (2023)\n",
    "\n",
    "### Efficiency Innovations\n",
    "\n",
    "1. **\"Fast Transformer Decoding: One Write-Head is All You Need\"** — Shazeer (2019) — MQA\n",
    "2. **\"GQA: Training Generalized Multi-Query Transformer Models\"** — Ainslie et al. (2023)\n",
    "3. **\"FlashAttention: Fast and Memory-Efficient Exact Attention\"** — Dao et al. (2022)\n",
    "\n",
    "### Long Context\n",
    "\n",
    "1. **\"RoFormer: Enhanced Transformer with Rotary Position Embedding\"** — Su et al. (2021)\n",
    "2. **\"Extending Context Window via Positional Interpolation\"** — Chen et al. (2023)\n",
    "3. **\"YaRN: Efficient Context Window Extension\"** — Peng et al. (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Упражнения\n",
    "\n",
    "### Упражнение 1: Пълен Transformer\n",
    "- Имплементирайте пълен decoder-only transformer\n",
    "- Обучете на малък dataset (Shakespeare, tiny stories)\n",
    "- Генерирайте текст\n",
    "\n",
    "### Упражнение 2: GQA имплементация\n",
    "- Имплементирайте GQA от scratch\n",
    "- Сравнете memory usage с MHA\n",
    "- Benchmark inference speed\n",
    "\n",
    "### Упражнение 3: RoPE exploration\n",
    "- Имплементирайте RoPE\n",
    "- Визуализирайте rotation patterns\n",
    "- Тествайте extrapolation отвъд training length\n",
    "\n",
    "### Упражнение 4: KV Cache\n",
    "- Добавете KV caching към transformer\n",
    "- Измерете speedup за генерация\n",
    "- Profile memory usage\n",
    "\n",
    "### Упражнение 5: Context Extension\n",
    "- Вземете pre-trained модел (GPT-2)\n",
    "- Приложете position interpolation\n",
    "- Оценете perplexity на дълги последователности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Край на Лекция 5\n",
    "\n",
    "**Въпроси?**\n",
    "\n",
    "---\n",
    "\n",
    "**Следваща лекция:** Foundation Models и Pretraining Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
