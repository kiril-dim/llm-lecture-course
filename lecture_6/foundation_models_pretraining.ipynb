{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 6: Фундаментални модели и данни за предварително обучение\n",
    "\n",
    "**Продължителност:** 2-2.5 часа  \n",
    "**Предпоставки:** Лекция 5 (Transformer архитектура)  \n",
    "**Следваща лекция:** Емерджентни способности при мащаб"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Цели на лекцията\n",
    "\n",
    "След тази лекция ще можете:\n",
    "\n",
    "- Обяснявате разликата между MLM и autoregressive езикови модели\n",
    "- Разбирате какви данни се използват за обучение на LLM\n",
    "- Имплементирате качествени филтри и дедупликация (MinHash)\n",
    "- Прилагате scaling laws за предвиждане на производителност\n",
    "- Идентифицирате контаминация в данни и бенчмаркове"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пътна карта\n",
    "\n",
    "```\n",
    "1. Мотивация → 2. Pretraining Objectives → 3. Източници на данни\n",
    "       ↓\n",
    "4. Качество и филтриране → 5. Контаминация → 6. Scaling Laws\n",
    "       ↓\n",
    "7. Обучение в мащаб → 8. От base към useful модел → 9. Обобщение\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основни библиотеки\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Настройки\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Възпроизводимост\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"Библиотеките са заредени успешно.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Мотивация: Парадигмата на фундаменталните модели\n",
    "\n",
    "### От специализирани към универсални модели\n",
    "\n",
    "| Период | Подход | Пример |\n",
    "|--------|--------|--------|\n",
    "| Преди 2018 | Task-specific модели | Отделен модел за всяка задача |\n",
    "| 2018-2020 | Pretrain + finetune | BERT за различни NLP задачи |\n",
    "| 2020+ | Foundation models | Един GPT за всичко |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трите съставки на Foundation Models\n",
    "\n",
    "```\n",
    "Foundation Model = Архитектура + Pretraining + Мащаб\n",
    "                   (Transformer)   (Next token)   (Трилиони токени)\n",
    "```\n",
    "\n",
    "**Тази лекция:** Фокус върху **Pretraining** и **Данните**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Supervised Learning: Безплатни етикети\n",
    "\n",
    "**Традиционно:** Нужни са човешки етикети (скъпо, бавно)\n",
    "\n",
    "**Self-supervised:** Самият текст съдържа \"етикети\"\n",
    "\n",
    "- **MLM:** Скрий дума → предвиди я\n",
    "- **Autoregressive:** Дадени първи N думи → предвиди N+1\n",
    "\n",
    "Така можем да използваме **целия интернет** за обучение!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Демонстрация: Self-supervised \"етикети\" са вградени в текста\n",
    "text = \"Котката седи на дивана.\"\n",
    "\n",
    "# Autoregressive: всяка позиция е пример за обучение\n",
    "tokens = text.split()\n",
    "print(\"Autoregressive training examples:\")\n",
    "print(\"=\"*50)\n",
    "for i in range(1, len(tokens)):\n",
    "    context = \" \".join(tokens[:i])\n",
    "    target = tokens[i]\n",
    "    print(f\"Input: '{context}' → Target: '{target}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"От 1 изречение получаваме {len(tokens)-1} training примера!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Един модел, много приложения\n",
    "\n",
    "Foundation model може да:\n",
    "\n",
    "- Отговаря на въпроси\n",
    "- Превежда текст\n",
    "- Пише код\n",
    "- Обобщава документи\n",
    "- Анализира sentiment\n",
    "\n",
    "**Без специално обучение за всяка задача!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Pretraining Objectives: Как учим от текст\n",
    "\n",
    "### Два основни подхода\n",
    "\n",
    "| Подход | Модели | Идея |\n",
    "|--------|--------|------|\n",
    "| **Masked LM** | BERT, RoBERTa | Скрий токени, предвиди ги |\n",
    "| **Autoregressive LM** | GPT, LLaMA | Предвиди следващия токен |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Masked Language Modeling (MLM)\n",
    "\n",
    "**BERT-style подход:**\n",
    "\n",
    "1. Вземи изречение\n",
    "2. Замаскирай 15% от токените с [MASK]\n",
    "3. Предвиди маскираните токени\n",
    "\n",
    "$$\\mathcal{L}_{MLM} = -\\sum_{i \\in M} \\log P(x_i | x_{\\backslash M})$$\n",
    "\n",
    "Където $M$ е множеството от маскирани позиции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM демонстрация\n",
    "def demonstrate_mlm(sentence, mask_prob=0.15):\n",
    "    \"\"\"Показва как MLM маскира и предвижда токени.\"\"\"\n",
    "    tokens = sentence.split()\n",
    "    masked_tokens = tokens.copy()\n",
    "    masked_positions = []\n",
    "    \n",
    "    # Маскираме ~15% от токените\n",
    "    np.random.seed(42)\n",
    "    for i in range(len(tokens)):\n",
    "        if np.random.random() < mask_prob:\n",
    "            masked_positions.append(i)\n",
    "            masked_tokens[i] = \"[MASK]\"\n",
    "    \n",
    "    print(f\"Original:  {sentence}\")\n",
    "    print(f\"Masked:    {' '.join(masked_tokens)}\")\n",
    "    print(f\"\\nМоделът трябва да предвиди:\")\n",
    "    for pos in masked_positions:\n",
    "        print(f\"  Position {pos}: '{tokens[pos]}'\")\n",
    "\n",
    "sentence = \"Големият черен котарак спеше спокойно на топлия диван в хола\"\n",
    "demonstrate_mlm(sentence, mask_prob=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLM: Предимства и недостатъци\n",
    "\n",
    "| Предимства | Недостатъци |\n",
    "|------------|-------------|\n",
    "| Bidirectional context | Не може да генерира естествено |\n",
    "| Добър за \"разбиране\" | [MASK] не съществува при inference |\n",
    "| Ефективно за classification | Ограничен до understanding tasks |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация: MLM вижда в двете посоки\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "tokens_viz = ['The', 'cat', '[MASK]', 'on', 'mat']\n",
    "n = len(tokens_viz)\n",
    "\n",
    "# MLM - bidirectional\n",
    "ax = axes[0]\n",
    "for i, tok in enumerate(tokens_viz):\n",
    "    color = 'red' if tok == '[MASK]' else 'steelblue'\n",
    "    ax.add_patch(plt.Rectangle((i*1.5, 0), 1, 0.8, fill=True, color=color, alpha=0.7))\n",
    "    ax.text(i*1.5 + 0.5, 0.4, tok, ha='center', va='center', fontsize=10, \n",
    "            color='white', fontweight='bold')\n",
    "\n",
    "# Стрелки от всички към [MASK]\n",
    "mask_idx = 2\n",
    "for i in range(n):\n",
    "    if i != mask_idx:\n",
    "        ax.annotate('', xy=(mask_idx*1.5 + 0.5, 0.9), xytext=(i*1.5 + 0.5, 0.9),\n",
    "                    arrowprops=dict(arrowstyle='->', color='green', alpha=0.6,\n",
    "                                   connectionstyle='arc3,rad=0.3'))\n",
    "\n",
    "ax.set_xlim(-0.5, n*1.5)\n",
    "ax.set_ylim(-0.5, 2)\n",
    "ax.axis('off')\n",
    "ax.set_title('MLM: Bidirectional\\n[MASK] вижда минало И бъдеще', fontsize=12)\n",
    "\n",
    "# Autoregressive - causal\n",
    "tokens_ar = ['The', 'cat', 'sat', 'on', '?']\n",
    "ax = axes[1]\n",
    "for i, tok in enumerate(tokens_ar):\n",
    "    color = 'orange' if tok == '?' else 'steelblue'\n",
    "    ax.add_patch(plt.Rectangle((i*1.5, 0), 1, 0.8, fill=True, color=color, alpha=0.7))\n",
    "    ax.text(i*1.5 + 0.5, 0.4, tok, ha='center', va='center', fontsize=10,\n",
    "            color='white', fontweight='bold')\n",
    "\n",
    "# Стрелки само от миналото\n",
    "target_idx = 4\n",
    "for i in range(target_idx):\n",
    "    ax.annotate('', xy=(target_idx*1.5 + 0.5, 0.9), xytext=(i*1.5 + 0.5, 0.9),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', alpha=0.6,\n",
    "                               connectionstyle='arc3,rad=0.3'))\n",
    "\n",
    "ax.set_xlim(-0.5, n*1.5)\n",
    "ax.set_ylim(-0.5, 2)\n",
    "ax.axis('off')\n",
    "ax.set_title('Autoregressive: Causal\\n? вижда само миналото', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Autoregressive Language Modeling\n",
    "\n",
    "**GPT-style подход:**\n",
    "\n",
    "Предвиждай следващия токен, даден контекст:\n",
    "\n",
    "$$\\mathcal{L}_{AR} = -\\sum_{i=1}^{n} \\log P(x_i | x_1, x_2, ..., x_{i-1})$$\n",
    "\n",
    "**Същата формула като n-gram модели от Лекция 1!**\n",
    "\n",
    "Разликата: невронна мрежа вместо counting statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive training - всяка позиция е loss\n",
    "def compute_ar_loss_example(tokens, vocab_probs):\n",
    "    \"\"\"\n",
    "    Демонстрира как се изчислява AR loss.\n",
    "    vocab_probs: симулирани вероятности за всеки токен.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    print(\"Autoregressive Loss изчисление:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(1, len(tokens)):\n",
    "        context = tokens[:i]\n",
    "        target = tokens[i]\n",
    "        prob = vocab_probs.get(target, 0.01)  # Симулирана вероятност\n",
    "        log_prob = np.log(prob)\n",
    "        total_loss -= log_prob\n",
    "        \n",
    "        print(f\"P('{target}' | '{' '.join(context)}') = {prob:.3f}\")\n",
    "        print(f\"  -log(P) = {-log_prob:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    avg_loss = total_loss / (len(tokens) - 1)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Loss: {total_loss:.3f}\")\n",
    "    print(f\"Average Loss: {avg_loss:.3f}\")\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "    return total_loss, perplexity\n",
    "\n",
    "# Пример\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "# Симулирани вероятности (в реалност идват от модела)\n",
    "vocab_probs = {\"cat\": 0.15, \"sat\": 0.08, \"on\": 0.20, \"the\": 0.25, \"mat\": 0.05}\n",
    "\n",
    "loss, ppl = compute_ar_loss_example(tokens, vocab_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Защо Autoregressive победи?\n",
    "\n",
    "| Фактор | Обяснение |\n",
    "|--------|----------|\n",
    "| **Естествена генерация** | Токен по токен, като писане |\n",
    "| **Без mismatch** | Същият процес при training и inference |\n",
    "| **Унификация** | Всички задачи като text generation |\n",
    "| **Scaling** | Емпирично по-добре при мащаб |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просто autoregressive генериране\n",
    "def simple_ar_generate(start_tokens, vocab, model_fn, max_tokens=10):\n",
    "    \"\"\"\n",
    "    Демонстрира autoregressive генериране.\n",
    "    model_fn: функция, която дава вероятности за следващ токен.\n",
    "    \"\"\"\n",
    "    tokens = list(start_tokens)\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        # Получаваме вероятности за следващ токен\n",
    "        probs = model_fn(tokens, vocab)\n",
    "        \n",
    "        # Сампълваме следващ токен\n",
    "        next_token = np.random.choice(vocab, p=probs)\n",
    "        tokens.append(next_token)\n",
    "        \n",
    "        if next_token == '.':\n",
    "            break\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Прост \"модел\" - uniform с bias към някои думи\n",
    "def toy_model(context, vocab):\n",
    "    \"\"\"Toy model: дава по-висока вероятност на 'cat', 'sat', '.'\"\"\"\n",
    "    probs = np.ones(len(vocab)) * 0.05\n",
    "    \n",
    "    # Bias към смислени продължения\n",
    "    if 'The' in context and 'cat' not in context:\n",
    "        probs[vocab.index('cat')] = 0.4\n",
    "    if 'cat' in context and 'sat' not in context:\n",
    "        probs[vocab.index('sat')] = 0.4\n",
    "    if 'sat' in context:\n",
    "        probs[vocab.index('.')] = 0.3\n",
    "    \n",
    "    return probs / probs.sum()\n",
    "\n",
    "vocab = ['The', 'cat', 'sat', 'on', 'mat', 'dog', 'ran', '.']\n",
    "\n",
    "print(\"Autoregressive генериране (toy model):\")\n",
    "for i in range(3):\n",
    "    np.random.seed(42 + i)\n",
    "    result = simple_ar_generate(['The'], vocab, toy_model, max_tokens=6)\n",
    "    print(f\"  {i+1}. {' '.join(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какво научава моделът при pretraining?\n",
    "\n",
    "Предвиждането на следващ токен изисква:\n",
    "\n",
    "| Знание | Пример |\n",
    "|--------|--------|\n",
    "| **Граматика** | \"The cats **are**\" не \"The cats **is**\" |\n",
    "| **Факти** | \"Paris is the capital of **France**\" |\n",
    "| **Логика** | \"If A then B. A is true. Therefore **B**\" |\n",
    "| **Код** | \"def factorial(n): **return**\" |\n",
    "| **Стил** | Formal vs informal writing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Прост autoregressive training loop (концептуален)\nclass TinyLanguageModel(nn.Module):\n    \"\"\"Минимален езиков модел за демонстрация.\"\"\"\n    def __init__(self, vocab_size, d_model=64, n_heads=4, n_layers=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_embedding = nn.Embedding(512, d_model)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n        self.output = nn.Linear(d_model, vocab_size)\n    \n    def forward(self, x):\n        seq_len = x.shape[1]\n        positions = torch.arange(seq_len, device=x.device)\n        \n        # Embeddings + positions\n        h = self.embedding(x) + self.pos_embedding(positions)\n        \n        # Causal mask: -inf за позиции, които не трябва да виждаме\n        mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=x.device)\n        \n        # Transformer\n        h = self.transformer(h, mask=mask, is_causal=True)\n        \n        # Output logits\n        return self.output(h)\n\n# Демонстрация\nvocab_size = 1000\nmodel = TinyLanguageModel(vocab_size)\nx = torch.randint(0, vocab_size, (2, 10))  # batch=2, seq_len=10\n\nlogits = model(x)\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output logits shape: {logits.shape}\")\nprint(f\"  (batch_size, seq_len, vocab_size)\")\nprint(f\"\\nПараметри: {sum(p.numel() for p in model.parameters()):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training step демонстрация\ndef training_step(model, batch, optimizer):\n    \"\"\"\n    Един training step за autoregressive LM.\n    batch: [batch_size, seq_len] - токенизиран текст\n    \"\"\"\n    model.train()\n    optimizer.zero_grad()\n    \n    # Input: всички токени освен последния\n    # Target: всички токени освен първия\n    input_ids = batch[:, :-1]\n    target_ids = batch[:, 1:]\n    \n    # Forward pass\n    logits = model(input_ids)  # [batch, seq_len-1, vocab]\n    \n    # Loss: cross-entropy за всяка позиция\n    loss = F.cross_entropy(\n        logits.reshape(-1, logits.size(-1)),  # [batch*seq, vocab]\n        target_ids.reshape(-1)                 # [batch*seq]\n    )\n    \n    # Backward pass\n    loss.backward()\n    optimizer.step()\n    \n    return loss.item()\n\n# Демонстрация на един step\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nbatch = torch.randint(0, vocab_size, (4, 32))\n\nloss = training_step(model, batch, optimizer)\nperplexity = np.exp(loss)\n\nprint(f\"Loss: {loss:.4f}\")\nprint(f\"Perplexity: {perplexity:.2f}\")\nprint(f\"\\n(Random init → perplexity ≈ vocab_size = {vocab_size})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity: Основната метрика\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N} \\log P(x_i | x_{<i})\\right)$$\n",
    "\n",
    "**Интуиция:** Средно колко \"объркан\" е моделът.\n",
    "\n",
    "| Perplexity | Значение |\n",
    "|------------|----------|\n",
    "| 1 | Перфектно предвиждане |\n",
    "| vocab_size | Random guessing |\n",
    "| ~20-30 | Добър LM на английски |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Източници и състав на данните\n",
    "\n",
    "### Мащаб на данните\n",
    "\n",
    "| Модел | Токени | Година |\n",
    "|-------|--------|--------|\n",
    "| GPT-2 | 40B | 2019 |\n",
    "| GPT-3 | 300B | 2020 |\n",
    "| LLaMA | 1.4T | 2023 |\n",
    "| LLaMA 2 | 2T | 2023 |\n",
    "| GPT-4 | ~13T (estimate) | 2023 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация на ръста на training данни\n",
    "models = ['GPT-2\\n(2019)', 'GPT-3\\n(2020)', 'Chinchilla\\n(2022)', \n",
    "          'LLaMA\\n(2023)', 'LLaMA 2\\n(2023)']\n",
    "tokens_billions = [40, 300, 1400, 1400, 2000]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(models)))\n",
    "\n",
    "bars = ax.bar(models, tokens_billions, color=colors, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Добавяме стойности върху барове\n",
    "for bar, tokens in zip(bars, tokens_billions):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 30,\n",
    "            f'{tokens}B', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Токени (милиарди)', fontsize=12)\n",
    "ax.set_title('Експоненциален ръст на training данни', fontsize=14)\n",
    "ax.set_ylim(0, 2500)\n",
    "\n",
    "# Добавяме линия на \"целия интернет\"\n",
    "ax.axhline(y=2000, color='red', linestyle='--', alpha=0.5)\n",
    "ax.text(4.5, 2050, '~Лимит на качествен\\nтекст в интернета', \n",
    "        ha='right', fontsize=10, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"50x ръст за 4 години!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основни източници на данни\n",
    "\n",
    "#### Web Crawls (60-80% от данните)\n",
    "\n",
    "| Dataset | Описание | Размер |\n",
    "|---------|----------|--------|\n",
    "| **Common Crawl** | Най-голям публичен web crawl | Петабайти HTML |\n",
    "| **C4** | Filtered Common Crawl | 800GB текст |\n",
    "| **RefinedWeb** | High-quality filtered | 5T токени |\n",
    "| **FineWeb** | Още по-качествен | 15T токени |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Книги (4-5%)\n",
    "\n",
    "- **Project Gutenberg:** Public domain класика\n",
    "- **Books3:** Controversial dataset (legal issues)\n",
    "- **Качество:** Високо, но ограничен обем\n",
    "\n",
    "#### Код (4-5%)\n",
    "\n",
    "- **GitHub:** Огромно repository\n",
    "- **The Stack:** Curated с лицензи\n",
    "- **Важно за:** Reasoning, структурирано мислене"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Научен текст (2-5%)\n",
    "\n",
    "- **arXiv:** Preprints в science/math\n",
    "- **PubMed:** Biomedical literature\n",
    "- **Semantic Scholar:** Academic papers\n",
    "\n",
    "#### Curated източници (5-10%)\n",
    "\n",
    "- **Wikipedia:** Висококачествен, factual\n",
    "- **StackExchange:** Q&A формат\n",
    "- **Reddit:** Conversational, varied quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Типичен dataset mix (LLaMA-style)\n",
    "sources = ['Web Crawl', 'Code', 'Wikipedia', 'Books', 'arXiv', 'StackExchange']\n",
    "percentages = [67, 4.5, 4.5, 4.5, 2.5, 2]\n",
    "tokens_t = [0.95, 0.065, 0.065, 0.065, 0.035, 0.03]  # В трилиони\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(sources)))\n",
    "explode = [0.05 if p < 5 else 0 for p in percentages]\n",
    "\n",
    "axes[0].pie(percentages, labels=sources, autopct='%1.1f%%', \n",
    "            colors=colors, explode=explode, startangle=90)\n",
    "axes[0].set_title('Разпределение по източник', fontsize=12)\n",
    "\n",
    "# Bar chart с токени\n",
    "bars = axes[1].barh(sources, tokens_t, color=colors, edgecolor='black')\n",
    "axes[1].set_xlabel('Токени (трилиони)', fontsize=11)\n",
    "axes[1].set_title('Абсолютен брой токени', fontsize=12)\n",
    "\n",
    "for bar, t in zip(bars, tokens_t):\n",
    "    axes[1].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{t}T', va='center', fontsize=10)\n",
    "\n",
    "axes[1].set_xlim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Защо съставът е важен?\n",
    "\n",
    "| Проблем | Причина |\n",
    "|---------|--------|\n",
    "| Твърде много web | Ниско качество, повторения |\n",
    "| Твърде много код | Странни генерации |\n",
    "| Само английски | Лоша multilingual способност |\n",
    "| Без код | Слаба логика и reasoning |\n",
    "\n",
    "**Баланс е критичен!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data (нов тренд)\n",
    "\n",
    "**Идея:** Генерирай данни с друг LLM.\n",
    "\n",
    "**Примери:**\n",
    "- Math задачи и решения\n",
    "- Code + тестове\n",
    "- Instruction-following pairs\n",
    "\n",
    "**Спорно:** Може ли модел да се учи от себе си?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Качество на данните и филтриране\n",
    "\n",
    "### Quality vs Quantity\n",
    "\n",
    "**Ранно вярване:** Повече данни = по-добър модел\n",
    "\n",
    "**Модерен insight (Chinchilla, Phi):** \n",
    "- Качеството е еднакво важно\n",
    "- Малък модел + качествени данни може да победи голям модел + шумни данни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примери за лошо качество на web data\n",
    "bad_examples = [\n",
    "    # Boilerplate\n",
    "    \"Cookie Policy. We use cookies to improve your experience. Accept All | Reject All | Manage Preferences\",\n",
    "    \n",
    "    # Repetition\n",
    "    \"Buy now! Buy now! Buy now! Best prices! Buy now!\",\n",
    "    \n",
    "    # Gibberish\n",
    "    \"asdfasdf lorem ipsum dolor sit amet asdfasdf\",\n",
    "    \n",
    "    # Too short\n",
    "    \"OK\",\n",
    "    \n",
    "    # Machine generated\n",
    "    \"Product SKU: 12345-ABC | Weight: 2.5kg | Dimensions: 10x20x30cm\",\n",
    "]\n",
    "\n",
    "good_examples = [\n",
    "    \"The transformer architecture revolutionized natural language processing by enabling parallel computation and capturing long-range dependencies.\",\n",
    "    \"Machine learning models learn patterns from data. The key is to have representative training examples.\",\n",
    "]\n",
    "\n",
    "print(\"Примери за ЛОШО качество (трябва да се филтрира):\")\n",
    "print(\"=\" * 60)\n",
    "for i, ex in enumerate(bad_examples, 1):\n",
    "    print(f\"{i}. {ex[:70]}...\" if len(ex) > 70 else f\"{i}. {ex}\")\n",
    "    \n",
    "print(\"\\nПримери за ДОБРО качество:\")\n",
    "print(\"=\" * 60)\n",
    "for i, ex in enumerate(good_examples, 1):\n",
    "    print(f\"{i}. {ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Filters\n",
    "\n",
    "Прости правила за филтриране:\n",
    "\n",
    "| Филтър | Описание | Threshold |\n",
    "|--------|----------|----------|\n",
    "| Дължина | Премахни много кратки документи | < 50 думи |\n",
    "| Повторения | Много еднакви думи/n-grams | > 20% repeats |\n",
    "| Специални символи | Твърде много punct/digits | > 30% |\n",
    "| Дължина на думи | Средна дължина | < 3 или > 10 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Имплементация на heuristic filters\n",
    "def quality_filters(text):\n",
    "    \"\"\"Връща score от 0 до 1 и причини за low score.\"\"\"\n",
    "    issues = []\n",
    "    words = text.split()\n",
    "    \n",
    "    # 1. Length filter\n",
    "    if len(words) < 10:\n",
    "        issues.append(f\"Too short ({len(words)} words)\")\n",
    "    \n",
    "    # 2. Repetition filter\n",
    "    if len(words) > 0:\n",
    "        word_counts = Counter(words)\n",
    "        most_common_count = word_counts.most_common(1)[0][1]\n",
    "        repetition_ratio = most_common_count / len(words)\n",
    "        if repetition_ratio > 0.2:\n",
    "            issues.append(f\"High repetition ({repetition_ratio:.1%})\")\n",
    "    \n",
    "    # 3. Special character ratio\n",
    "    if len(text) > 0:\n",
    "        special_chars = sum(1 for c in text if not c.isalnum() and c != ' ')\n",
    "        special_ratio = special_chars / len(text)\n",
    "        if special_ratio > 0.3:\n",
    "            issues.append(f\"Too many special chars ({special_ratio:.1%})\")\n",
    "    \n",
    "    # 4. Average word length\n",
    "    if len(words) > 0:\n",
    "        avg_word_len = np.mean([len(w) for w in words])\n",
    "        if avg_word_len < 2 or avg_word_len > 15:\n",
    "            issues.append(f\"Unusual word length ({avg_word_len:.1f})\")\n",
    "    \n",
    "    # Score: 1 if no issues, lower otherwise\n",
    "    score = max(0, 1 - len(issues) * 0.25)\n",
    "    \n",
    "    return score, issues\n",
    "\n",
    "# Тестваме\n",
    "test_texts = bad_examples + good_examples\n",
    "\n",
    "print(\"Quality Filter Results:\")\n",
    "print(\"=\" * 70)\n",
    "for text in test_texts:\n",
    "    score, issues = quality_filters(text)\n",
    "    status = \"PASS\" if score >= 0.75 else \"FAIL\"\n",
    "    print(f\"[{status}] Score: {score:.2f}\")\n",
    "    print(f\"    Text: {text[:50]}...\")\n",
    "    if issues:\n",
    "        print(f\"    Issues: {', '.join(issues)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier-Based Filtering\n",
    "\n",
    "**Идея:** Обучи класификатор на \"високо качество\" vs \"ниско качество\"\n",
    "\n",
    "**Положителни примери:** Wikipedia, учебници, научни статии\n",
    "\n",
    "**Отрицателни примери:** Random web pages\n",
    "\n",
    "**Използва се от:** RefinedWeb, FineWeb, GPT-4 data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity Filtering\n",
    "\n",
    "**Идея:** Използвай reference модел да оцени текст.\n",
    "\n",
    "- **Много ниска perplexity:** Прекалено просто/повтарящо се\n",
    "- **Много висока perplexity:** Gibberish или грешен език\n",
    "- **Средна perplexity:** Оптимално!\n",
    "\n",
    "```\n",
    "Keep if: threshold_low < PPL < threshold_high\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Deduplication: Защо е критично\n",
    "\n",
    "**Проблемите с дупликати:**\n",
    "\n",
    "| Проблем | Описание |\n",
    "|---------|----------|\n",
    "| Пропиляна compute | Учим едно и също много пъти |\n",
    "| Меморизация | Моделът запомня verbatim |\n",
    "| Benchmark leak | Тестовете може да са в training |\n",
    "| Biased representations | Overrepresented content |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мащаб на проблема с дупликати\n",
    "datasets = ['Raw\\nCommon Crawl', 'C4', 'RefinedWeb']\n",
    "duplicate_rates = [45, 25, 8]  # Процент дупликати\n",
    "\n",
    "colors = ['#ff6b6b', '#ffa502', '#2ed573']\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "bars = ax.bar(datasets, duplicate_rates, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for bar, rate in zip(bars, duplicate_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{rate}%', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Процент near-duplicates', fontsize=11)\n",
    "ax.set_title('Дупликати в различни datasets\\n(преди/след филтриране)', fontsize=12)\n",
    "ax.set_ylim(0, 55)\n",
    "\n",
    "# Анотация\n",
    "ax.annotate('Почти половината!', xy=(0, 45), xytext=(0.5, 50),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            fontsize=10, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Deduplication\n",
    "\n",
    "**Просто:** Hash документ → премахни еднакви hash-ове.\n",
    "\n",
    "```python\n",
    "doc_hash = hashlib.md5(doc.encode()).hexdigest()\n",
    "if doc_hash in seen_hashes:\n",
    "    skip(doc)\n",
    "```\n",
    "\n",
    "**Недостатък:** Не хваща \"почти еднакви\" документи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact deduplication demo\n",
    "def exact_dedup(documents):\n",
    "    \"\"\"Премахва точни дупликати чрез hashing.\"\"\"\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    duplicates = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_hash = hashlib.md5(doc.encode()).hexdigest()\n",
    "        if doc_hash not in seen:\n",
    "            seen.add(doc_hash)\n",
    "            unique.append(doc)\n",
    "        else:\n",
    "            duplicates += 1\n",
    "    \n",
    "    return unique, duplicates\n",
    "\n",
    "# Тест\n",
    "docs = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog ran in the park.\",\n",
    "    \"The cat sat on the mat.\",  # Exact duplicate\n",
    "    \"The cat sat on the mat!\",  # Near-duplicate (different punctuation)\n",
    "    \"The dog ran in the park.\",  # Exact duplicate\n",
    "]\n",
    "\n",
    "unique, dups = exact_dedup(docs)\n",
    "print(f\"Original: {len(docs)} documents\")\n",
    "print(f\"After exact dedup: {len(unique)} unique, {dups} duplicates removed\")\n",
    "print(f\"\\nUnique documents:\")\n",
    "for doc in unique:\n",
    "    print(f\"  - {doc}\")\n",
    "\n",
    "print(\"\\n⚠️ Near-duplicate 'mat!' не е хванат!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash: Near-Duplicate Detection\n",
    "\n",
    "**Идея:** Апроксимира Jaccard similarity между документи.\n",
    "\n",
    "$$\\text{Jaccard}(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "**Алгоритъм:**\n",
    "1. Раздели документ на n-grams (shingles)\n",
    "2. Приложи k различни hash функции\n",
    "3. За всяка функция запази минималния hash\n",
    "4. Signature = [min_hash_1, min_hash_2, ..., min_hash_k]\n",
    "\n",
    "**Ключово:** $P(\\text{min\\_hash равни}) = \\text{Jaccard}(A, B)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinHash имплементация от scratch\n",
    "class MinHash:\n",
    "    def __init__(self, num_hashes=100):\n",
    "        self.num_hashes = num_hashes\n",
    "        # Генерираме random hash параметри\n",
    "        self.a = np.random.randint(1, 2**31, num_hashes)\n",
    "        self.b = np.random.randint(0, 2**31, num_hashes)\n",
    "        self.prime = 2**31 - 1  # Mersenne prime\n",
    "    \n",
    "    def _hash(self, x, i):\n",
    "        \"\"\"Hash функция i за стойност x.\"\"\"\n",
    "        return (self.a[i] * x + self.b[i]) % self.prime\n",
    "    \n",
    "    def get_shingles(self, text, k=3):\n",
    "        \"\"\"Генерира k-grams (shingles) от текст.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        shingles = set()\n",
    "        for i in range(len(words) - k + 1):\n",
    "            shingle = \" \".join(words[i:i+k])\n",
    "            shingles.add(hash(shingle))\n",
    "        return shingles\n",
    "    \n",
    "    def signature(self, text):\n",
    "        \"\"\"Изчислява MinHash signature за текст.\"\"\"\n",
    "        shingles = self.get_shingles(text)\n",
    "        sig = np.full(self.num_hashes, np.inf)\n",
    "        \n",
    "        for shingle in shingles:\n",
    "            for i in range(self.num_hashes):\n",
    "                hash_val = self._hash(shingle, i)\n",
    "                sig[i] = min(sig[i], hash_val)\n",
    "        \n",
    "        return sig\n",
    "    \n",
    "    def similarity(self, sig1, sig2):\n",
    "        \"\"\"Jaccard approximation от signatures.\"\"\"\n",
    "        return np.mean(sig1 == sig2)\n",
    "\n",
    "# Демонстрация\n",
    "minhash = MinHash(num_hashes=128)\n",
    "\n",
    "doc1 = \"The cat sat on the mat in the living room\"\n",
    "doc2 = \"The cat sat on the mat in the kitchen\"  # Near-duplicate\n",
    "doc3 = \"Dogs are wonderful pets that bring joy\"  # Different\n",
    "\n",
    "sig1 = minhash.signature(doc1)\n",
    "sig2 = minhash.signature(doc2)\n",
    "sig3 = minhash.signature(doc3)\n",
    "\n",
    "print(\"MinHash Similarity (Jaccard approximation):\")\n",
    "print(f\"  doc1 vs doc2: {minhash.similarity(sig1, sig2):.3f} (near-duplicates)\")\n",
    "print(f\"  doc1 vs doc3: {minhash.similarity(sig1, sig3):.3f} (different)\")\n",
    "print(f\"  doc2 vs doc3: {minhash.similarity(sig2, sig3):.3f} (different)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Jaccard за сравнение\n",
    "def true_jaccard(text1, text2, k=3):\n",
    "    \"\"\"Изчислява истинската Jaccard similarity.\"\"\"\n",
    "    words1 = text1.lower().split()\n",
    "    words2 = text2.lower().split()\n",
    "    \n",
    "    shingles1 = set(\" \".join(words1[i:i+k]) for i in range(len(words1)-k+1))\n",
    "    shingles2 = set(\" \".join(words2[i:i+k]) for i in range(len(words2)-k+1))\n",
    "    \n",
    "    intersection = len(shingles1 & shingles2)\n",
    "    union = len(shingles1 | shingles2)\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "print(\"Сравнение: True Jaccard vs MinHash approximation\")\n",
    "print(\"=\"*50)\n",
    "pairs = [(doc1, doc2, \"near-dup\"), (doc1, doc3, \"different\"), (doc2, doc3, \"different\")]\n",
    "\n",
    "for d1, d2, label in pairs:\n",
    "    true_j = true_jaccard(d1, d2)\n",
    "    mh_j = minhash.similarity(minhash.signature(d1), minhash.signature(d2))\n",
    "    print(f\"{label:12} True: {true_j:.3f}  MinHash: {mh_j:.3f}  Error: {abs(true_j-mh_j):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация: MinHash accuracy vs брой hash функции\n",
    "num_hashes_range = [8, 16, 32, 64, 128, 256, 512]\n",
    "errors = []\n",
    "\n",
    "true_sim = true_jaccard(doc1, doc2)\n",
    "\n",
    "for n_hash in num_hashes_range:\n",
    "    # Average over multiple trials\n",
    "    trial_errors = []\n",
    "    for _ in range(20):\n",
    "        mh = MinHash(num_hashes=n_hash)\n",
    "        est_sim = mh.similarity(mh.signature(doc1), mh.signature(doc2))\n",
    "        trial_errors.append(abs(true_sim - est_sim))\n",
    "    errors.append(np.mean(trial_errors))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(num_hashes_range, errors, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xscale('log', base=2)\n",
    "plt.xlabel('Брой hash функции', fontsize=11)\n",
    "plt.ylabel('Средна грешка в Jaccard', fontsize=11)\n",
    "plt.title('MinHash: Повече hashes → По-точна оценка', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(num_hashes_range, [str(n) for n in num_hashes_range])\n",
    "\n",
    "# Annotation\n",
    "plt.annotate('128 hashes е\\nдобър баланс', xy=(128, errors[4]), \n",
    "             xytext=(200, errors[4]+0.03),\n",
    "             arrowprops=dict(arrowstyle='->', color='green'),\n",
    "             fontsize=10, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH Banding за ефективно търсене\n",
    "\n",
    "**Проблем:** O(n²) сравнения между всички документи е твърде бавно.\n",
    "\n",
    "**Решение:** Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "1. Раздели signature на bands\n",
    "2. Hash всеки band в bucket\n",
    "3. Само документи в същия bucket се сравняват\n",
    "\n",
    "**Резултат:** От O(n²) към почти O(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Контаминация и evaluation integrity\n",
    "\n",
    "### Проблемът с контаминацията\n",
    "\n",
    "**Контаминация:** Test данни попадат в training set.\n",
    "\n",
    "**Резултат:** \n",
    "- Inflated benchmark scores\n",
    "- Моделът \"знае\" отговорите\n",
    "- Невалидна evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример: Контаминация\n",
    "benchmark_example = {\n",
    "    \"question\": \"What is the capital of France?\",\n",
    "    \"answer\": \"Paris\"\n",
    "}\n",
    "\n",
    "training_documents = [\n",
    "    \"France is a country in Western Europe. Its capital is Paris.\",  # OK - factual\n",
    "    \"The capital of France is Paris, which is also its largest city.\",  # OK - factual\n",
    "    \"Q: What is the capital of France? A: Paris\",  # CONTAMINATION!\n",
    "    \"Test question: What is the capital of France? Correct answer: Paris\",  # CONTAMINATION!\n",
    "]\n",
    "\n",
    "print(\"Benchmark:\")\n",
    "print(f\"  Q: {benchmark_example['question']}\")\n",
    "print(f\"  A: {benchmark_example['answer']}\")\n",
    "\n",
    "print(\"\\nTraining documents:\")\n",
    "for i, doc in enumerate(training_documents, 1):\n",
    "    is_contaminated = \"Q:\" in doc or \"Test question\" in doc\n",
    "    status = \"❌ CONTAMINATED\" if is_contaminated else \"✓ OK\"\n",
    "    print(f\"  {i}. [{status}] {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Типове контаминация\n",
    "\n",
    "| Тип | Описание | Severity |\n",
    "|-----|----------|----------|\n",
    "| **Direct** | Точен test example в training | Критичен |\n",
    "| **Indirect** | Парафраза на test example | Висок |\n",
    "| **Temporal** | Training data от след benchmark | Среден |\n",
    "| **Partial** | Само въпрос или само отговор | Нисък |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection methods\n",
    "\n",
    "#### N-gram Overlap\n",
    "\n",
    "Проверяваме дали n-grams от test set се срещат в training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_overlap(text1, text2, n=8):\n",
    "    \"\"\"\n",
    "    Изчислява n-gram overlap между два текста.\n",
    "    Висок overlap = potential contamination.\n",
    "    \"\"\"\n",
    "    def get_ngrams(text, n):\n",
    "        words = text.lower().split()\n",
    "        return set(\" \".join(words[i:i+n]) for i in range(len(words)-n+1))\n",
    "    \n",
    "    ngrams1 = get_ngrams(text1, n)\n",
    "    ngrams2 = get_ngrams(text2, n)\n",
    "    \n",
    "    if not ngrams1 or not ngrams2:\n",
    "        return 0\n",
    "    \n",
    "    overlap = len(ngrams1 & ngrams2)\n",
    "    return overlap / min(len(ngrams1), len(ngrams2))\n",
    "\n",
    "# Test\n",
    "test_question = \"What is the capital of France and what is its population?\"\n",
    "\n",
    "training_docs = [\n",
    "    \"France is known for its culture. Paris has many museums.\",  # Low overlap\n",
    "    \"The capital of France is Paris. Its population is about 2 million.\",  # Medium overlap\n",
    "    \"Question: What is the capital of France and what is its population?\",  # High overlap!\n",
    "]\n",
    "\n",
    "print(f\"Test: '{test_question}'\\n\")\n",
    "print(\"N-gram overlap (n=5) с training documents:\")\n",
    "for doc in training_docs:\n",
    "    overlap = ngram_overlap(test_question, doc, n=5)\n",
    "    status = \"⚠️ POTENTIAL CONTAMINATION\" if overlap > 0.3 else \"OK\"\n",
    "    print(f\"  {overlap:.2%} - {status}\")\n",
    "    print(f\"    Doc: {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canary Strings\n",
    "\n",
    "**Идея:** Вмъкни уникални \"canary\" strings в training data.\n",
    "\n",
    "```\n",
    "The secret canary code is: X7K9M2P4\n",
    "```\n",
    "\n",
    "Ако моделът може да възпроизведе canary → memorization риск."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitigation Strategies\n",
    "\n",
    "| Стратегия | Описание |\n",
    "|-----------|----------|\n",
    "| **Decontamination** | Премахни test-like примери от training |\n",
    "| **Temporal splits** | Training само от преди benchmark |\n",
    "| **New benchmarks** | Създавай нови тестове редовно |\n",
    "| **Private test sets** | Не публикувай test данни |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Простa decontamination функция\n",
    "def decontaminate(training_docs, test_examples, threshold=0.3, n=8):\n",
    "    \"\"\"\n",
    "    Премахва training документи с високо overlap с test.\n",
    "    \"\"\"\n",
    "    clean_docs = []\n",
    "    removed = 0\n",
    "    \n",
    "    for doc in training_docs:\n",
    "        is_contaminated = False\n",
    "        for test in test_examples:\n",
    "            if ngram_overlap(doc, test, n) > threshold:\n",
    "                is_contaminated = True\n",
    "                break\n",
    "        \n",
    "        if is_contaminated:\n",
    "            removed += 1\n",
    "        else:\n",
    "            clean_docs.append(doc)\n",
    "    \n",
    "    return clean_docs, removed\n",
    "\n",
    "# Demo\n",
    "test_set = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "]\n",
    "\n",
    "training_set = [\n",
    "    \"France is a European country.\",\n",
    "    \"Q: What is the capital of France? A: Paris\",  # Contaminated\n",
    "    \"Shakespeare was born in Stratford.\",\n",
    "    \"Test: Who wrote Romeo and Juliet? Answer: Shakespeare\",  # Contaminated\n",
    "    \"The weather in Paris is mild.\",\n",
    "]\n",
    "\n",
    "clean, removed = decontaminate(training_set, test_set, threshold=0.2, n=4)\n",
    "print(f\"Original training: {len(training_set)} docs\")\n",
    "print(f\"After decontamination: {len(clean)} docs ({removed} removed)\")\n",
    "print(f\"\\nClean documents:\")\n",
    "for doc in clean:\n",
    "    print(f\"  - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Scaling Laws\n",
    "\n",
    "### Емпиричното откритие\n",
    "\n",
    "**OpenAI (Kaplan et al., 2020):** Loss се предвижда от compute!\n",
    "\n",
    "- Забележително consistent\n",
    "- Работи през много порядъци на величина\n",
    "- Power law relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трите оси на scaling\n",
    "\n",
    "| Ос | Символ | Описание |\n",
    "|----|--------|----------|\n",
    "| **Model size** | N | Брой параметри |\n",
    "| **Data size** | D | Брой токени |\n",
    "| **Compute** | C | FLOPs за training |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Law формули\n",
    "\n",
    "$$L(N) = \\left(\\frac{N_c}{N}\\right)^{\\alpha_N}$$\n",
    "\n",
    "$$L(D) = \\left(\\frac{D_c}{D}\\right)^{\\alpha_D}$$\n",
    "\n",
    "$$L(C) = \\left(\\frac{C_c}{C}\\right)^{\\alpha_C}$$\n",
    "\n",
    "Типични експоненти: $\\alpha \\approx 0.05 - 0.1$\n",
    "\n",
    "**Интерпретация:** 10x повече compute → ~0.1-0.2 намаление в loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Симулация на scaling laws\n",
    "def scaling_law(x, x_c, alpha):\n",
    "    \"\"\"Power law: L = (x_c/x)^alpha\"\"\"\n",
    "    return (x_c / x) ** alpha\n",
    "\n",
    "# Параметри (приблизителни от литературата)\n",
    "alpha_N = 0.076  # Model size exponent\n",
    "alpha_D = 0.095  # Data size exponent\n",
    "N_c = 8.8e13     # Critical model size\n",
    "D_c = 5.4e13     # Critical data size\n",
    "\n",
    "# Ranges\n",
    "model_sizes = np.logspace(6, 12, 50)  # 1M to 1T params\n",
    "data_sizes = np.logspace(9, 13, 50)   # 1B to 10T tokens\n",
    "\n",
    "# Loss curves\n",
    "loss_N = scaling_law(model_sizes, N_c, alpha_N) + 1.5  # +baseline\n",
    "loss_D = scaling_law(data_sizes, D_c, alpha_D) + 1.5\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss vs Model Size\n",
    "ax = axes[0]\n",
    "ax.loglog(model_sizes, loss_N, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Model Parameters', fontsize=11)\n",
    "ax.set_ylabel('Test Loss', fontsize=11)\n",
    "ax.set_title('Scaling Law: Loss vs Model Size', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Маркери за известни модели\n",
    "known_models = {\n",
    "    'GPT-2': 1.5e9,\n",
    "    'GPT-3': 175e9,\n",
    "    'LLaMA 70B': 70e9,\n",
    "}\n",
    "for name, size in known_models.items():\n",
    "    loss = scaling_law(size, N_c, alpha_N) + 1.5\n",
    "    ax.scatter([size], [loss], s=100, zorder=5)\n",
    "    ax.annotate(name, (size, loss), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "# Loss vs Data Size\n",
    "ax = axes[1]\n",
    "ax.loglog(data_sizes, loss_D, 'r-', linewidth=2)\n",
    "ax.set_xlabel('Training Tokens', fontsize=11)\n",
    "ax.set_ylabel('Test Loss', fontsize=11)\n",
    "ax.set_title('Scaling Law: Loss vs Data Size', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Маркери за известни training runs\n",
    "known_data = {\n",
    "    'GPT-3': 300e9,\n",
    "    'LLaMA': 1.4e12,\n",
    "    'LLaMA 2': 2e12,\n",
    "}\n",
    "for name, tokens in known_data.items():\n",
    "    loss = scaling_law(tokens, D_c, alpha_D) + 1.5\n",
    "    ax.scatter([tokens], [loss], s=100, zorder=5)\n",
    "    ax.annotate(name, (tokens, loss), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinchilla: Compute-Optimal Training\n",
    "\n",
    "**DeepMind (Hoffmann et al., 2022):** Моделите са били undertrained!\n",
    "\n",
    "**Ключов insight:**\n",
    "\n",
    "$$\\text{Optimal ratio} \\approx 20 \\text{ tokens per parameter}$$\n",
    "\n",
    "| Модел | Parameters | Tokens | Ratio |\n",
    "|-------|------------|--------|-------|\n",
    "| GPT-3 | 175B | 300B | 1.7 (undertrained) |\n",
    "| Chinchilla | 70B | 1.4T | 20 (optimal) |\n",
    "| LLaMA | 7B | 1.4T | 200 (overtrained) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinchilla optimal frontier\n",
    "def chinchilla_optimal(compute_budget, a=0.5, b=0.5):\n",
    "    \"\"\"\n",
    "    Изчислява оптимален model size и data size за даден compute budget.\n",
    "    Опростена версия: N ~ C^a, D ~ C^b где a + b = 1\n",
    "    \"\"\"\n",
    "    N_opt = compute_budget ** a\n",
    "    D_opt = compute_budget ** b\n",
    "    return N_opt, D_opt\n",
    "\n",
    "# Различни compute budgets\n",
    "compute_budgets = np.logspace(18, 24, 20)  # FLOPs\n",
    "\n",
    "# За фиксиран compute: tradeoff между N и D\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "C_fixed = 1e21  # Fixed compute budget\n",
    "# Constraint: N * D ≈ C (опростено)\n",
    "N_range = np.logspace(9, 12, 100)\n",
    "D_range = C_fixed / N_range\n",
    "\n",
    "# Loss за различни combinations (опростен модел)\n",
    "def combined_loss(N, D, alpha_N=0.076, alpha_D=0.095):\n",
    "    return (N_c/N)**alpha_N + (D_c/D)**alpha_D + 1.5\n",
    "\n",
    "losses = [combined_loss(n, d) for n, d in zip(N_range, D_range)]\n",
    "\n",
    "ax.semilogx(N_range, losses, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Model Parameters (N)', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title(f'Loss vs Model Size при фиксиран compute (C={C_fixed:.0e} FLOPs)', fontsize=12)\n",
    "\n",
    "# Optimal point\n",
    "optimal_idx = np.argmin(losses)\n",
    "ax.scatter([N_range[optimal_idx]], [losses[optimal_idx]], s=150, c='red', zorder=5)\n",
    "ax.annotate(f'Optimal\\nN={N_range[optimal_idx]:.1e}', \n",
    "            (N_range[optimal_idx], losses[optimal_idx]),\n",
    "            xytext=(20, 20), textcoords='offset points',\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            fontsize=10, color='red')\n",
    "\n",
    "# Показваме GPT-3 style (голям модел, малко данни) vs Chinchilla style\n",
    "ax.axvline(x=175e9, color='gray', linestyle='--', alpha=0.5, label='GPT-3 size')\n",
    "ax.axvline(x=70e9, color='green', linestyle='--', alpha=0.5, label='Chinchilla size')\n",
    "ax.legend()\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"При compute budget {C_fixed:.0e} FLOPs:\")\n",
    "print(f\"  Optimal model size: {N_range[optimal_idx]:.2e} параметри\")\n",
    "print(f\"  Optimal data size: {D_range[optimal_idx]:.2e} токени\")\n",
    "print(f\"  Ratio: {D_range[optimal_idx]/N_range[optimal_idx]:.1f} tokens/param\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какво scale-ва добре и какво не\n",
    "\n",
    "| Scale-ва добре | Scale-ва непредвидимо |\n",
    "|----------------|----------------------|\n",
    "| Perplexity | Reasoning tasks |\n",
    "| Повечето NLP tasks | Specific capabilities |\n",
    "| Factual knowledge | Alignment/safety |\n",
    "| Code generation | Emergent abilities |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кога scaling се чупи\n",
    "\n",
    "| Лимит | Описание |\n",
    "|-------|----------|\n",
    "| **Data exhaustion** | Свършва качественият текст |\n",
    "| **Compute** | Твърде скъпо/бавно |\n",
    "| **Diminishing returns** | Подобренията намаляват |\n",
    "| **Capability gaps** | Някои неща не се учат със scale |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Обучение в мащаб\n",
    "\n",
    "### Computational Requirements\n",
    "\n",
    "| Модел | GPU Hours | Estimated Cost |\n",
    "|-------|-----------|----------------|\n",
    "| GPT-2 (1.5B) | ~5K A100 | ~$50K |\n",
    "| GPT-3 (175B) | ~3.6M A100 | ~$5-10M |\n",
    "| LLaMA 65B | ~1M A100 | ~$2-5M |\n",
    "| GPT-4 | Unknown | ~$50-100M (estimate) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute estimation\n",
    "def estimate_training_flops(params, tokens):\n",
    "    \"\"\"\n",
    "    Приблизителна оценка на FLOPs за training.\n",
    "    Rule of thumb: 6 * N * D FLOPs (forward + backward)\n",
    "    \"\"\"\n",
    "    return 6 * params * tokens\n",
    "\n",
    "def flops_to_gpu_hours(flops, gpu_tflops=312):\n",
    "    \"\"\"\n",
    "    Конвертира FLOPs към GPU hours.\n",
    "    A100: ~312 TFLOPS (FP16)\n",
    "    \"\"\"\n",
    "    gpu_flops_per_hour = gpu_tflops * 1e12 * 3600\n",
    "    # Assume 50% utilization\n",
    "    effective_flops = gpu_flops_per_hour * 0.5\n",
    "    return flops / effective_flops\n",
    "\n",
    "# Примери\n",
    "models = [\n",
    "    ('GPT-2', 1.5e9, 40e9),\n",
    "    ('LLaMA 7B', 7e9, 1.4e12),\n",
    "    ('LLaMA 65B', 65e9, 1.4e12),\n",
    "    ('GPT-3', 175e9, 300e9),\n",
    "]\n",
    "\n",
    "print(\"Training Compute Estimates:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<15} {'Params':<12} {'Tokens':<12} {'FLOPs':<15} {'GPU Hours':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, params, tokens in models:\n",
    "    flops = estimate_training_flops(params, tokens)\n",
    "    gpu_hours = flops_to_gpu_hours(flops)\n",
    "    \n",
    "    print(f\"{name:<15} {params:.1e}  {tokens:.1e}  {flops:.2e}  {gpu_hours:,.0f}\")\n",
    "\n",
    "print(\"\\n* При 50% GPU utilization на A100 (312 TFLOPS FP16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Training Strategies\n",
    "\n",
    "| Strategy | Описание | Кога се използва |\n",
    "|----------|----------|------------------|\n",
    "| **Data Parallel** | Копие на модела на всеки GPU | Малки модели |\n",
    "| **Tensor Parallel** | Разделяме layers между GPUs | Много големи layers |\n",
    "| **Pipeline Parallel** | Различни layers на различни GPUs | Много дълбоки модели |\n",
    "| **FSDP** | Sharded параметри и оптимизатор | Най-ефективно за LLMs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация на parallelism strategies\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Data Parallel\n",
    "ax = axes[0]\n",
    "for i in range(4):\n",
    "    ax.add_patch(plt.Rectangle((i*2.5, 0), 2, 3, fill=True, \n",
    "                                color='steelblue', alpha=0.7))\n",
    "    ax.text(i*2.5 + 1, 1.5, f'Model\\ncopy {i+1}', ha='center', va='center',\n",
    "            fontsize=9, color='white')\n",
    "    ax.text(i*2.5 + 1, -0.5, f'Batch {i+1}', ha='center', fontsize=9)\n",
    "ax.set_xlim(-0.5, 10.5)\n",
    "ax.set_ylim(-1, 4)\n",
    "ax.axis('off')\n",
    "ax.set_title('Data Parallel\\n(същият модел, различни данни)', fontsize=11)\n",
    "\n",
    "# Tensor Parallel\n",
    "ax = axes[1]\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, 4))\n",
    "for i in range(4):\n",
    "    ax.add_patch(plt.Rectangle((i*2.5, 0), 2, 3, fill=True,\n",
    "                                color=colors[i], alpha=0.8))\n",
    "    ax.text(i*2.5 + 1, 1.5, f'Layer\\npart {i+1}', ha='center', va='center',\n",
    "            fontsize=9)\n",
    "ax.set_xlim(-0.5, 10.5)\n",
    "ax.set_ylim(-1, 4)\n",
    "ax.axis('off')\n",
    "ax.set_title('Tensor Parallel\\n(един layer разделен)', fontsize=11)\n",
    "\n",
    "# Pipeline Parallel\n",
    "ax = axes[2]\n",
    "for i in range(4):\n",
    "    ax.add_patch(plt.Rectangle((2, i*0.8), 6, 0.7, fill=True,\n",
    "                                color=colors[i], alpha=0.8))\n",
    "    ax.text(5, i*0.8 + 0.35, f'Layers {i*8+1}-{(i+1)*8}', ha='center', va='center',\n",
    "            fontsize=9)\n",
    "    ax.text(0.5, i*0.8 + 0.35, f'GPU {i+1}', ha='center', va='center', fontsize=9)\n",
    "# Arrows\n",
    "for i in range(3):\n",
    "    ax.annotate('', xy=(5, (i+1)*0.8), xytext=(5, i*0.8+0.7),\n",
    "                arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax.set_xlim(-0.5, 10.5)\n",
    "ax.set_ylim(-0.5, 4)\n",
    "ax.axis('off')\n",
    "ax.set_title('Pipeline Parallel\\n(различни layers на различни GPUs)', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Typical Value | Notes |\n",
    "|-----------|---------------|-------|\n",
    "| Learning rate | 1e-4 to 6e-4 | Warmup + cosine decay |\n",
    "| Batch size | 2M-4M tokens | Gradient accumulation |\n",
    "| Weight decay | 0.1 | AdamW |\n",
    "| Warmup steps | 2000 | Linear warmup |\n",
    "| Gradient clipping | 1.0 | Stability |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule visualization\n",
    "def lr_schedule(step, warmup_steps=2000, max_steps=100000, \n",
    "                max_lr=3e-4, min_lr=3e-5):\n",
    "    \"\"\"Warmup + cosine decay schedule.\"\"\"\n",
    "    if step < warmup_steps:\n",
    "        # Linear warmup\n",
    "        return max_lr * step / warmup_steps\n",
    "    else:\n",
    "        # Cosine decay\n",
    "        progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "        return min_lr + 0.5 * (max_lr - min_lr) * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "steps = np.arange(0, 100000, 100)\n",
    "lrs = [lr_schedule(s) for s in steps]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Full schedule\n",
    "ax = axes[0]\n",
    "ax.plot(steps, lrs, 'b-', linewidth=2)\n",
    "ax.axvline(x=2000, color='red', linestyle='--', alpha=0.5, label='Warmup end')\n",
    "ax.set_xlabel('Training Step', fontsize=11)\n",
    "ax.set_ylabel('Learning Rate', fontsize=11)\n",
    "ax.set_title('Learning Rate Schedule: Warmup + Cosine Decay', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom on warmup\n",
    "ax = axes[1]\n",
    "warmup_steps = np.arange(0, 3000, 10)\n",
    "warmup_lrs = [lr_schedule(s) for s in warmup_steps]\n",
    "ax.plot(warmup_steps, warmup_lrs, 'b-', linewidth=2)\n",
    "ax.axvline(x=2000, color='red', linestyle='--', alpha=0.5, label='Warmup end')\n",
    "ax.set_xlabel('Training Step', fontsize=11)\n",
    "ax.set_ylabel('Learning Rate', fontsize=11)\n",
    "ax.set_title('Warmup Phase (zoom)', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Failures\n",
    "\n",
    "| Problem | Cause | Solution |\n",
    "|---------|-------|----------|\n",
    "| **Loss spikes** | Bad batch, LR too high | Lower LR, skip batch |\n",
    "| **NaN loss** | Numerical instability | Gradient clipping, FP32 |\n",
    "| **No convergence** | LR too low/high | Tune LR |\n",
    "| **Hardware failure** | GPU dies | Checkpoints |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Симулация на training loss curve с проблеми\n",
    "np.random.seed(42)\n",
    "steps = np.arange(0, 50000, 100)\n",
    "\n",
    "# Base loss curve (smooth decay)\n",
    "base_loss = 4.0 * np.exp(-steps/20000) + 1.5 + np.random.randn(len(steps)) * 0.02\n",
    "\n",
    "# Добавяме \"spikes\"\n",
    "spike_steps = [10000, 25000, 35000]\n",
    "for spike_step in spike_steps:\n",
    "    idx = np.abs(steps - spike_step).argmin()\n",
    "    base_loss[idx:idx+3] += np.array([0.5, 1.0, 0.3])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(steps, base_loss, 'b-', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "# Mark spikes\n",
    "for spike_step in spike_steps:\n",
    "    ax.axvline(x=spike_step, color='red', linestyle='--', alpha=0.3)\n",
    "    ax.text(spike_step, 5.5, 'spike', ha='center', fontsize=9, color='red')\n",
    "\n",
    "ax.set_xlabel('Training Step', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Typical Training Loss Curve\\n(с loss spikes)', fontsize=12)\n",
    "ax.set_ylim(1, 6)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotation\n",
    "ax.annotate('Loss spikes: нормално\\nпри large-scale training', \n",
    "            xy=(25000, 3), xytext=(35000, 4),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'),\n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. От base model към useful model\n",
    "\n",
    "### Base Model Limitations\n",
    "\n",
    "**Base model** (след pretraining) може да:\n",
    "- Предвижда следващ токен много добре\n",
    "- Генерира coherent text\n",
    "\n",
    "**НО не е \"полезен\":**\n",
    "- Не следва инструкции\n",
    "- Не е безопасен\n",
    "- Непредвидимо поведение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Демонстрация: Base model vs Assistant\n",
    "user_prompt = \"What is the capital of France?\"\n",
    "\n",
    "base_model_completions = [\n",
    "    \"What is the capital of France? What is the capital of Germany? What is the capital of Italy?\",\n",
    "    \"What is the capital of France? A. Paris B. London C. Berlin D. Madrid\",\n",
    "    \"What is the capital of France? I'm writing a geography quiz and need...\",\n",
    "]\n",
    "\n",
    "assistant_completion = \"The capital of France is Paris.\"\n",
    "\n",
    "print(\"User: What is the capital of France?\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASE MODEL (просто предвижда следващ токен):\")\n",
    "for i, comp in enumerate(base_model_completions, 1):\n",
    "    print(f\"  {i}. {comp}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ASSISTANT MODEL (instruction-tuned):\")\n",
    "print(f\"  {assistant_completion}\")\n",
    "\n",
    "print(\"\\n→ Base model предвижда какво би следвало в training data\")\n",
    "print(\"→ Assistant model е обучен да ОТГОВАРЯ на въпроси\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модерният Pipeline\n",
    "\n",
    "```\n",
    "Pretraining → SFT → RLHF → Deploy\n",
    "(тази лекция)   (Лекция 8)\n",
    "```\n",
    "\n",
    "| Етап | Цел | Данни |\n",
    "|------|-----|-------|\n",
    "| **Pretraining** | Научи езика | Трилиони токени от интернета |\n",
    "| **SFT** | Научи да следва инструкции | ~100K instruction pairs |\n",
    "| **RLHF** | Align с човешки preferences | ~50K comparisons |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview: Instruction Tuning (SFT)\n",
    "\n",
    "**Supervised Fine-Tuning:**\n",
    "\n",
    "Training data format:\n",
    "```\n",
    "User: <instruction>\n",
    "Assistant: <response>\n",
    "```\n",
    "\n",
    "**Ефект:** Моделът научава да отговаря вместо да продължава."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview: RLHF\n",
    "\n",
    "**Reinforcement Learning from Human Feedback:**\n",
    "\n",
    "1. Генерирай множество отговори\n",
    "2. Хора ги ранкират\n",
    "3. Обучи reward model\n",
    "4. Fine-tune с RL\n",
    "\n",
    "**Детайли в Лекция 8!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Обобщение\n",
    "\n",
    "### Ключови изводи\n",
    "\n",
    "1. **Autoregressive pretraining** победи: предвиждай следващия токен в мащаб\n",
    "\n",
    "2. **Качеството на данните** е критично: филтриране, дедупликация\n",
    "\n",
    "3. **Състав на данните** влияе на capabilities: web + code + books + wiki\n",
    "\n",
    "4. **Scaling laws** предвиждат performance: loss ~ (compute)^(-α)\n",
    "\n",
    "5. **Chinchilla insight:** balance между model size и data\n",
    "\n",
    "6. **Контаминация** е реален проблем: detection и decontamination\n",
    "\n",
    "7. **Base models** не са директно полезни: нужни SFT и RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Следваща лекция: Emergent Capabilities\n",
    "\n",
    "**Лекция 7** разглежда:\n",
    "\n",
    "- Какви способности се появяват при scale?\n",
    "- Zero-shot и few-shot learning\n",
    "- In-context learning механика\n",
    "- Reasoning emergence\n",
    "- Когато scale не е достатъчен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ресурси\n",
    "\n",
    "### Основни статии\n",
    "\n",
    "1. **\"Language Models are Few-Shot Learners\"** — Brown et al. (2020) — GPT-3 paper\n",
    "2. **\"Scaling Laws for Neural Language Models\"** — Kaplan et al. (2020) — Scaling laws\n",
    "3. **\"Training Compute-Optimal Large Language Models\"** — Hoffmann et al. (2022) — Chinchilla\n",
    "4. **\"LLaMA: Open and Efficient Foundation Language Models\"** — Touvron et al. (2023)\n",
    "\n",
    "### Данни и качество\n",
    "\n",
    "1. **\"The Pile: An 800GB Dataset of Diverse Text\"** — Gao et al. (2020)\n",
    "2. **\"Deduplicating Training Data Makes Language Models Better\"** — Lee et al. (2021)\n",
    "3. **\"The RefinedWeb Dataset for Falcon LLM\"** — Penedo et al. (2023)\n",
    "\n",
    "### Online ресурси\n",
    "\n",
    "1. **Hugging Face Datasets** — datasets catalog\n",
    "2. **Common Crawl** — commoncrawl.org\n",
    "3. **Stanford CS336** — Building LLMs course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Упражнения\n",
    "\n",
    "### Упражнение 1: Autoregressive Training\n",
    "- Обучете малък LM на детски разкази\n",
    "- Измерете perplexity\n",
    "- Генерирайте текст и оценете качеството\n",
    "\n",
    "### Упражнение 2: Data Quality Filtering\n",
    "- Вземете sample от Common Crawl\n",
    "- Имплементирайте heuristic филтри\n",
    "- Измерете какъв % преминава\n",
    "\n",
    "### Упражнение 3: MinHash Deduplication\n",
    "- Имплементирайте MinHash + LSH\n",
    "- Намерете near-duplicates в dataset\n",
    "- Анализирайте duplicate rate\n",
    "\n",
    "### Упражнение 4: Scaling Law Analysis\n",
    "- Обучете модели с различен размер\n",
    "- Фитнете power law\n",
    "- Предвидете performance за по-голям модел\n",
    "\n",
    "### Упражнение 5: Contamination Detection\n",
    "- Проверете dataset за benchmark overlap\n",
    "- Имплементирайте n-gram detection\n",
    "- Оценете severity на контаминацията"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Край на Лекция 6\n",
    "\n",
    "**Въпроси?**\n",
    "\n",
    "---\n",
    "\n",
    "**Следваща лекция:** Emergent Capabilities at Scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}