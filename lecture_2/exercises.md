# Кратки упражнения: Лекция 2

Следните упражнения са за самостоятелна работа по време на лекцията или веднага след нея. Очаквано време: 2-3 минути за упражнение.

---

## Упражнение 1: WordNet йерархия

Използвайте WordNet, за да намерите **hypernyms** (по-общи понятия) за думата `"cat"`.

```python
cat = wn.synset('cat.n.01')
# Вашият код тук: изведете hypernyms
```

**Въпрос:** Кой е най-общият hypernym в йерархията?

---

## Упражнение 2: Bigram вероятност на ръка

Даден е следният корпус (3 изречения):
- "I love NLP"
- "I love ML"
- "I study NLP"

Изчислете **на ръка** (без код):

$$P(\text{love} \mid \text{I}) = ?$$

**Hint:** Колко пъти "love" следва "I"? Колко пъти се среща "I" общо?

---

## Упражнение 3: Интерпретация на перплексия

Имате два езикови модела, оценени на един и същ тестов корпус:
- **Модел A:** Perplexity = 45
- **Модел B:** Perplexity = 120

**Въпроси:**
1. Кой модел е по-добър? Защо?
2. Какво означава перплексия 45 интуитивно?

---

## Упражнение 4: Co-occurrence прозорец

За изречението `"the cat sat on the mat"` с **window = 1**:

Напишете всички двойки (center, context) за централната дума **"sat"**.

**Формат:** (sat, ?) за всяка контекстна дума в прозореца.

---

## Упражнение 5: Word2Vec най-близки съседи

Изпълнете следния код и отговорете на въпроса:

```python
word2vec.most_similar('python', topn=5)
```

**Въпрос:** Резултатите свързани ли са с програмиране или със змии? Защо мислите, че е така?

---

## Упражнение 6: Word2Vec аналогия

Попълнете аналогията с код:

$$\text{berlin} - \text{germany} + \text{france} = ?$$

```python
word2vec.most_similar(positive=['berlin', 'france'], negative=['germany'], topn=1)
```

**Въпрос:** Полученият резултат логичен ли е? Какъв тип релация улавя тази аналогия?

---

## Упражнение 7: Bias в embeddings

Проверете коя дума е по-близо до `"programmer"` — `"man"` или `"woman"`:

```python
print(word2vec.similarity('programmer', 'man'))
print(word2vec.similarity('programmer', 'woman'))
```

**Въпроси:**
1. Какво наблюдавате?
2. Откъде идва този bias?
3. Защо това е проблем за реални приложения?
