# Изпит: Вариант B

**Инструкции:** Решете всички 8 задачи. Показвайте изчисленията си.

---

## Задача 1: Cross-Entropy и Perplexity (12 точки)

Езиков модел предсказва вероятности за следващия токен в изречение от 4 думи:

| Позиция | Истински токен | Предсказана вероятност |
|---------|----------------|------------------------|
| 1       | "The"          | 0.8                    |
| 2       | "model"        | 0.4                    |
| 3       | "works"        | 0.5                    |
| 4       | "well"         | 0.25                   |

**Задача:**
a) Изчислете cross-entropy loss за всяка позиция: $L_i = -\log(p_i)$
b) Изчислете average cross-entropy loss
c) Изчислете perplexity: $PPL = e^{L_{avg}}$
d) Какво означава тази perplexity интуитивно?

**Hints:** $\log(0.8) \approx -0.223$, $\log(0.4) \approx -0.916$, $\log(0.5) \approx -0.693$, $\log(0.25) \approx -1.386$

---

## Задача 2: Embedding и Vocabulary (12 точки)

Модел има:
- Vocabulary size: 32,000 токена
- Embedding dimension: 768
- 24 transformer layers с $d_{model} = 768$

**Задача:**
a) Колко параметъра има embedding матрицата?
b) Ако увеличим vocabulary до 50,000, с колко MB нараства моделът? (float32 = 4 bytes)
c) Ако средно документ се токенизира в 500 токена с vocab 32K и 650 токена с vocab 16K, кой вариант е по-ефективен за context window от 4096?
d) Защо по-голям vocabulary не винаги е по-добър?

---

## Задача 3: Multi-Head Attention (15 точки)

Модел с:
- $d_{model} = 512$
- $n_{heads} = 8$
- Sequence length: 1024 токена

**Задача:**
a) Каква е размерността на Q, K, V за един head ($d_k$)?
b) Каква е размерността на attention scores матрицата за един head?
c) Каква е общата памет за attention scores за всички heads? (float32 = 4 bytes)
d) Ако използваме GQA (Grouped Query Attention) с 2 KV heads вместо 8, колко памет спестяваме за KV cache?
e) Защо Multi-Head Attention е по-добър от Single-Head с по-голям $d_k$?

---

## Задача 4: Bias vs Variance (12 точки)

Три модела са обучени и тествани:

| Модел | Параметри | Train Loss | Val Loss | Test Accuracy |
|-------|-----------|------------|----------|---------------|
| A     | 100K      | 0.45       | 0.48     | 72%           |
| B     | 10M       | 0.08       | 0.52     | 68%           |
| C     | 1M        | 0.22       | 0.25     | 78%           |

**Задача:**
a) Кой модел показва underfitting (high bias)?
b) Кой модел показва overfitting (high variance)?
c) Кой модел има най-добър баланс?
d) Какви техники бихте приложили за подобряване на Модел B?
e) Ако имате повече training данни, кой модел би се подобрил най-много?

---

## Задача 5: Quantization и Local LLMs (12 точки)

Модел с 7B параметъра трябва да се зареди на GPU с 8GB VRAM.

| Precision | Bits | Размер | Perplexity |
|-----------|------|--------|------------|
| FP16      | 16   | 14 GB  | 5.8        |
| INT8      | 8    | 7 GB   | 5.9        |
| INT4      | 4    | 3.5 GB | 6.2        |
| INT3      | 3    | 2.6 GB | 7.1        |

GPU memory bandwidth: 256 GB/s

**Задача:**
a) Кои precision формати се побират в 8GB VRAM?
b) При INT4, колко tokens/second теоретично може да генерира системата?
c) Какъв е trade-off при избор на INT4 вместо INT8?
d) Защо INT3 обикновено не се препоръчва за production?

**Hint:** За генериране на 1 токен трябва да се прочетат всички weights веднъж.

---

## Задача 6: Chain of Thought и Reasoning (12 точки)

Резултати на математически benchmark:

| Модел | Параметри | Standard | + CoT | + Self-consistency (N=5) |
|-------|-----------|----------|-------|--------------------------|
| A     | 7B        | 15%      | 18%   | 22%                      |
| B     | 70B       | 45%      | 68%   | 78%                      |

**Задача:**
a) За кой модел CoT дава по-голямо относително подобрение?
b) Колко пъти повече inference cost е Self-consistency спрямо Standard?
c) Защо малките модели имат по-малка полза от CoT?
d) При какъв тип задачи CoT НЕ помага? Дайте 2 примера.

---

## Задача 7: Pretraining Data и Contamination (13 точки)

Модел е тестван на benchmark с 1000 въпроса:
- 80 въпроса имат exact match в training data
- 150 въпроса имат >70% n-gram overlap
- 770 въпроса са "clean"

Accuracy по категории:
- Exact match: 96%
- High overlap: 82%
- Clean: 58%

**Задача:**
a) Изчислете "наивната" обща accuracy (претеглена средна)
b) Каква е истинската способност на модела (clean accuracy)?
c) С колко процентни точки е надценена наивната accuracy?
d) Защо contamination е сериозен проблем за benchmark validity?
e) Как се открива contamination в практиката?

---

## Задача 8: Tool Use и Agent Design (12 точки)

Agent има достъп до следните tools:

| Tool | Описание | Latency | Cost |
|------|----------|---------|------|
| search | Web търсене | 500ms | $0.01 |
| calculator | Математика | 10ms | $0.001 |
| code_exec | Python код | 2000ms | $0.05 |
| database | SQL query | 100ms | $0.02 |

User query: "Намери топ 5 компании по пазарна капитализация и изчисли средната им стойност"

**Задача:**
a) Какви tools са необходими за тази задача?
b) Опишете ReAct trace (Thought → Action → Observation) за първите 2 стъпки
c) Ако agent прави грешка на стъпка 2 и трябва retry, колко се увеличава общият cost?
d) Защо е важно tool descriptions да са ясни и точни?

---

# Скала за оценяване

| Точки | Оценка |
|-------|--------|
| 90-100 | Отличен (6) |
| 75-89 | Много добър (5) |
| 60-74 | Добър (4) |
| 50-59 | Среден (3) |
| < 50 | Слаб (2) |
