Lecture 1: Introduction to AI and ML
Problem definition in ML. Definitions - labels, features, train, test and validation datasets. Supervised, unsupervised and reinforcement learning. Overfitting and underfitting. Metrics.

Lecture 2: Language models
Meaning of word, computer representation. WordNet with synonyms and hypernyms. N-gram models and limitations, Word2Vec.

Lecture 3: Tokenization
Dependency trees, BPE, unigram, char, word algorithms. Normalization, stemmingâ€¦

Lecture 4: Neural networks
Neurons. Layers. Forward pass. Activations. Back propagation. Gradient descent.

Lecture 5: Transformers
Brief on RNNs and their issues. Attention. Transformer architecture. BERT.

Lecture 6: Foundation Models
Pretraining objectives. Masked language models. Next token prediction. Datasets at scale. Scaling laws.

Lecture 7: Emergent Capabilities at Scale
Few-shot and zero-shot learning. In-context learning mechanics. Enhanced contextual understanding. What capabilities emerge and when. Reasoning capabilities that appear at scale.

Lecture 8: Alignment and RLHF
The alignment problem. Instruction tuning (SFT). RLHF deep dive - reward modeling, PPO. Constitutional AI. Safety and red teaming. Current debates and challenges.

Lecture 9: Local LLMs
Running Ollama, Llama.cpp. Quantization. Open source models. When and why to run locally.

Lecture 10: Advanced Prompting and Reasoning Models
Prompt engineering techniques. Few-shot prompting. Role playing. Chain of thought. Tree of thought. Reasoning models - o1-style, process supervision. When to use reasoning vs standard models.

Lecture 11: Hallucinations and RAG
Understanding hallucinations. Lack of context. In-context learning. Semantic search. Similarity search. Vector databases. ANN. Retrievers. Query reformulation. Retrieved document summarization.

Lecture 12: AI Agents and Tools
Agent architectures. Agent memory, tools, orchestration. ReAct, reflection, planning. Multi-agent systems. Key results and limitations.
