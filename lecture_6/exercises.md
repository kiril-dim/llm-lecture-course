# Кратки упражнения: Лекция 6

Следните упражнения са за самостоятелна работа по време на лекцията или веднага след нея. Очаквано време: 2-3 минути за упражнение.

---

## Упражнение 1: MLM vs Autoregressive

Разгледайте двата pretraining подхода:

**MLM (BERT):** "The [MASK] sat on the [MASK]"
**AR (GPT):** "The cat sat on the" → предсказва "mat"

**Въпроси:**
1. Кой подход вижда контекст от двете страни на предсказвания токен?
2. Кой подход има mismatch между training и inference?
3. Защо autoregressive доминира при модерните LLM?

---

## Упражнение 2: Cross-Entropy Loss за LM

Модел предсказва вероятности за следващия токен:

| Токен | Вероятност |
|-------|------------|
| cat   | 0.6        |
| dog   | 0.3        |
| bird  | 0.1        |

Истинският следващ токен е "cat".

**Задача:** Изчислете cross-entropy loss.

$$L = -\log P(\text{correct token})$$

**Hint:** $\log(0.6) \approx -0.511$

---

## Упражнение 3: Perplexity от Loss

Модел има average cross-entropy loss = 2.5 на тестов корпус.

**Задача:** Изчислете perplexity.

$$PPL = e^{L}$$

**Hint:** $e^{2.5} \approx 12.18$

**Въпрос:** Какво означава perplexity 12.18 интуитивно?

---

## Упражнение 4: Dataset Composition

LLaMA използва следното разпределение на данни:

| Източник | Процент |
|----------|---------|
| Web (CommonCrawl) | 67% |
| Code (GitHub) | 4.5% |
| Wikipedia | 4.5% |
| Books | 4.5% |
| arXiv | 2.5% |
| StackExchange | 2% |

Общо токени: 1.4 трилиона

**Задача:** Изчислете броя токени за:
1. Web данни
2. Code
3. Wikipedia

**Въпрос:** Защо Wikipedia е "oversampled" спрямо реалния ѝ размер в интернет?

---

## Упражнение 5: Jaccard Similarity

Два документа имат следните множества от думи (след preprocessing):

- Документ A: {machine, learning, model, training, data}
- Документ B: {machine, learning, neural, network, data}

**Задача:** Изчислете Jaccard similarity.

$$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$$

**Въпрос:** Ако threshold за дубликати е 0.5, това дубликати ли са?

---

## Упражнение 6: Chinchilla Optimal Training

Chinchilla препоръчва ~20 токена на параметър за compute-optimal training.

**Задача:** За модел с 7 милиарда параметъра:
1. Колко токена са optimal според Chinchilla?
2. GPT-3 (175B параметъра) е трениран на 300B токена. Колко е съотношението токени/параметри?
3. GPT-3 undertrained или overtrained е според Chinchilla?

---

## Упражнение 7: Scaling Law изчисление

Scaling law за loss спрямо брой параметри:

$$L(N) = \left(\frac{N_c}{N}\right)^{\alpha}$$

където $N_c = 8.8 \times 10^{13}$, $\alpha = 0.076$

**Задача:** Ако увеличим модела от 1B на 10B параметъра, с колко намалява loss?

**Hint:** $10^{0.076} \approx 1.19$

$$\frac{L(1B)}{L(10B)} = \left(\frac{10B}{1B}\right)^{0.076} = 10^{0.076}$$

---

## Упражнение 8: Training Compute

Приблизителна формула за FLOPs при training:

$$C \approx 6ND$$

където $N$ е брой параметъра, $D$ е брой токени.

**Задача:** За LLaMA 7B (7 милиарда параметъра, 1.4 трилиона токена):
1. Изчислете общите FLOPs
2. Ако GPU (A100) има throughput 312 TFLOPS, колко GPU-часа са нужни?

**Hints:**
- $6 \times 7 \times 10^9 \times 1.4 \times 10^{12} = 5.88 \times 10^{22}$ FLOPs
- 1 TFLOPS = $10^{12}$ FLOPS
- GPU-часове = FLOPs / (TFLOPS × 3600 × $10^{12}$)

---

## Упражнение 9: Quality Filtering

Имате текст от web crawl и прилагате следните филтри:

| Филтър | Оригинални документи | След филтър |
|--------|---------------------|-------------|
| Дължина (< 100 chars) | 1,000,000 | 850,000 |
| Повторения | 850,000 | 700,000 |
| Perplexity (outliers) | 700,000 | 600,000 |
| Език (не-English) | 600,000 | 500,000 |

**Задача:**
1. Какъв процент от данните остава след всички филтри?
2. Кой филтър премахва най-много документи (абсолютно)?
3. Кой филтър премахва най-много документи (относително спрямо входа си)?

---

## Упражнение 10: Contamination Detection

Тествате модел на benchmark с 1000 въпроса. Проверявате training data за n-gram overlap.

Резултати:
- 50 въпроса имат exact match в training data
- 100 въпроса имат >80% n-gram overlap
- 850 въпроса са "clean"

Модел accuracy:
- На exact match: 95%
- На >80% overlap: 80%
- На clean: 60%

**Задача:**
1. Каква е "наивната" обща accuracy (без корекция)?
2. Каква е accuracy на "чистите" данни?
3. Защо истинската способност на модела е по-близо до 60%?

---

## Упражнение 11: Data Deduplication Impact

Преди deduplication: 10 трилиона токена
След exact dedup: 8 трилиона токена
След near-duplicate dedup: 5 трилиона токена

**Въпроси:**
1. Какъв процент са exact duplicates?
2. Какъв процент са near-duplicates (от оригинала)?
3. Ако тренираме за 1 epoch, колко compute спестяваме с dedup?

---

## Упражнение 12: Model vs Data Scaling

Имате фиксиран compute budget C. Можете да изберете:

**Вариант A:** 10B параметъра, 100B токена
**Вариант B:** 5B параметъра, 200B токена

Двата варианта имат еднакъв compute ($C \approx 6ND$).

**Въпроси:**
1. Потвърдете, че compute е еднакъв
2. Според Chinchilla (20 токена/параметър), кой вариант е по-близо до optimal?
3. Какъв би бил optimal split за този compute?

---

# Решения

## Решение 1
1. MLM вижда контекст от двете страни (bidirectional)
2. MLM има mismatch - при inference няма [MASK] токени
3. AR е natural за генерация, няма train/inference mismatch, всички задачи могат да се формулират като генерация

## Решение 2
$$L = -\log(0.6) = -(-0.511) = 0.511$$

## Решение 3
$$PPL = e^{2.5} \approx 12.18$$

Интуитивно: моделът е "изненадан" толкова, колкото ако избира между ~12 равновероятни токена на всяка стъпка.

## Решение 4
1. Web: $1.4T \times 0.67 = 938B$ токена
2. Code: $1.4T \times 0.045 = 63B$ токена
3. Wikipedia: $1.4T \times 0.045 = 63B$ токена

Wikipedia е oversampled защото е много качествена, добре структурирана и фактологична. Реалният размер е < 1% от интернет данните.

## Решение 5
- $A \cap B$ = {machine, learning, data} → 3 елемента
- $A \cup B$ = {machine, learning, model, training, data, neural, network} → 7 елемента
- $J(A, B) = 3/7 \approx 0.43$

При threshold 0.5, това НЕ са дубликати (0.43 < 0.5).

## Решение 6
1. Optimal: $7B \times 20 = 140B$ токена
2. GPT-3: $300B / 175B \approx 1.7$ токена/параметър
3. GPT-3 е силно **undertrained** (1.7 << 20)

## Решение 7
$$\frac{L(1B)}{L(10B)} = 10^{0.076} \approx 1.19$$

Loss намалява с ~19% при 10x увеличение на параметрите. Това показва, че scaling дава diminishing returns - 10x compute за 19% подобрение.

## Решение 8
1. $C = 6 \times 7 \times 10^9 \times 1.4 \times 10^{12} = 5.88 \times 10^{22}$ FLOPs

2. GPU-часове:
$$\frac{5.88 \times 10^{22}}{312 \times 10^{12} \times 3600} = \frac{5.88 \times 10^{22}}{1.12 \times 10^{18}} \approx 52,000 \text{ GPU-часа}$$

(Реалните числа са ~80-100K поради inefficiencies)

## Решение 9
1. Остават: $500,000 / 1,000,000 = 50\%$
2. Абсолютно най-много: Дължина (150,000 документа)
3. Относително най-много: Повторения ($150,000 / 850,000 = 17.6\%$) или Език ($100,000 / 600,000 = 16.7\%$)

## Решение 10
1. Наивна accuracy: $(50 \times 0.95 + 100 \times 0.80 + 850 \times 0.60) / 1000$
   $= (47.5 + 80 + 510) / 1000 = 63.75\%$

2. Clean accuracy: 60%

3. Истинската способност е ~60%, защото:
   - На contaminated данни моделът е "запомнил" отговорите
   - Clean accuracy отразява истинската генерализация
   - 95% на exact match ≠ разбиране, а memorization

## Решение 11
1. Exact duplicates: $(10T - 8T) / 10T = 20\%$
2. Near-duplicates: $(8T - 5T) / 10T = 30\%$
3. Спестяваме: $(10T - 5T) / 10T = 50\%$ от compute

## Решение 12
1. Вариант A: $6 \times 10B \times 100B = 6 \times 10^{21}$
   Вариант B: $6 \times 5B \times 200B = 6 \times 10^{21}$ ✓ Еднакви

2. Вариант A: $100B / 10B = 10$ токена/параметър
   Вариант B: $200B / 5B = 40$ токена/параметър

   Optimal е 20, така че **Вариант B е по-близо** (40 vs 10)

3. За compute $C = 6 \times 10^{21}$:
   - $C = 6ND$ и $D = 20N$ (Chinchilla)
   - $C = 6N \times 20N = 120N^2$
   - $N = \sqrt{C/120} = \sqrt{6 \times 10^{21}/120} = \sqrt{5 \times 10^{19}} \approx 7B$
   - $D = 20 \times 7B = 140B$ токена

   Optimal: ~7B параметъра, ~140B токена
