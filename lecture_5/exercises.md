# Кратки упражнения: Лекция 5

Следните упражнения са за самостоятелна работа по време на лекцията или веднага след нея. Очаквано време: 2-3 минути за упражнение.

---

## Упражнение 1: FFN размерности

Feed-Forward мрежа в transformer има expansion factor 4x.

За модел с $d_{model} = 512$:
1. Каква е размерността на скрития слой в FFN?
2. Колко параметъра има $W_1$ (без bias)?
3. Колко параметъра има $W_2$ (без bias)?
4. Общо параметри за FFN (само weights)?

---

## Упражнение 2: Residual Connections

Дадена е операция с residual connection:
$$\text{output} = x + \text{Sublayer}(x)$$

**Въпроси:**
1. Ако $x = [1, 2, 3]$ и $\text{Sublayer}(x) = [0.1, -0.2, 0.3]$, какъв е output?
2. Какво се случва с градиента при backpropagation през residual? (подсказка: chain rule)
3. Защо това помага при обучение на 96+ слоя?

---

## Упражнение 3: Layer Normalization на ръка

Даден е вектор $x = [2, 4, 6]$.

Изчислете LayerNorm (без learnable $\gamma$, $\beta$, т.е. $\gamma=1$, $\beta=0$):

1. Средно: $\mu = ?$
2. Variance: $\sigma^2 = \frac{1}{n}\sum(x_i - \mu)^2 = ?$
3. Стандартно отклонение: $\sigma = ?$
4. Нормализиран изход: $\hat{x} = \frac{x - \mu}{\sigma} = ?$

---

## Упражнение 4: Pre-Norm vs Post-Norm

Разгледайте двете схеми:

**Post-Norm:** $\text{LayerNorm}(x + \text{Attention}(x))$

**Pre-Norm:** $x + \text{Attention}(\text{LayerNorm}(x))$

**Въпроси:**
1. В коя схема residual path е "по-чист" (без transformation)?
2. Коя схема се използва в модерните LLM (LLaMA, GPT-3)?
3. Защо Pre-Norm е по-стабилен за обучение?

---

## Упражнение 5: Causal vs Bidirectional Attention

За последователност от 4 токена, начертайте attention mask матрица (1 = може да attend, 0 = не може):

1. **Bidirectional (BERT):**
```
     t0  t1  t2  t3
t0 [  ?   ?   ?   ?  ]
t1 [  ?   ?   ?   ?  ]
t2 [  ?   ?   ?   ?  ]
t3 [  ?   ?   ?   ?  ]
```

2. **Causal (GPT):**
```
     t0  t1  t2  t3
t0 [  ?   ?   ?   ?  ]
t1 [  ?   ?   ?   ?  ]
t2 [  ?   ?   ?   ?  ]
t3 [  ?   ?   ?   ?  ]
```

**Въпрос:** Защо decoder-only (causal) доминира в модерните LLM?

---

## Упражнение 6: GQA KV Cache спестявания

Модел има 32 query heads, $d_k = 128$, sequence length 8192.

Изчислете KV cache размер (в MB, float16 = 2 bytes) за:

1. **MHA (32 KV heads):**
   - $\text{Size} = 2 \times \text{kv\_heads} \times \text{seq\_len} \times d_k \times 2 \text{ bytes}$

2. **GQA (8 KV heads):**

3. **MQA (1 KV head):**

**Въпрос:** Колко процента памет спестява GQA спрямо MHA?

---

## Упражнение 7: Параметри на Transformer блок

Един Transformer блок съдържа:
- Multi-Head Attention: Q, K, V, O проекции
- FFN: две линейни трансформации с 4x expansion
- Два LayerNorm слоя

За $d_{model} = 768$, $n_{heads} = 12$ (MHA):

1. Attention параметри (4 матрици $d \times d$): ?
2. FFN параметри ($d \times 4d$ + $4d \times d$): ?
3. LayerNorm параметри ($2 \times 2 \times d$, gamma + beta за два слоя): ?
4. **Общо за блок:** ?

---

## Упражнение 8: RoPE интуиция

RoPE кодира позицията чрез ротация на Q, K векторите.

**Въпроси:**
1. Ако $q$ и $k$ са на позиции 5 и 3, от какво зависи техният dot product след RoPE?
   - a) Абсолютните позиции 5 и 3
   - b) Относителната позиция (5 - 3 = 2)
   - c) Само стойностите на $q$ и $k$

2. Защо относителната позиция е по-полезна от абсолютната?

3. Какво предимство има RoPE при context extension?

---

## Упражнение 9: Position Interpolation

Модел е обучен с max context 4096 токена. Искаме да го разширим до 16384.

**Задача:**
1. Изчислете scale factor за position interpolation
2. На каква "интерполирана" позиция съответства реална позиция 8000?
3. Защо тази интерполирана позиция е в training range?

**Формула:** $pos' = pos \times \frac{L_{train}}{L_{target}}$

---

## Упражнение 10: Sliding Window Attention

Модел използва sliding window attention с window size $w = 4096$.

За последователност от 32,000 токена:

1. Стандартен attention: колко двойки (q, k) трябва да изчислим? ($n^2$)
2. Sliding window: колко двойки? ($n \times w$)
3. Колко пъти по-малко изчисления?

**Въпрос:** Как информация от token 0 достига до token 32,000 при sliding window?

---

## Упражнение 11: KV Cache при генерация

При autoregressive генерация, на всяка стъпка:
- Без cache: изчисляваме K, V за всички предишни токени
- С cache: изчисляваме K, V само за новия токен

**Въпрос:** Ако генерираме 100 токена:
1. Без cache: колко K, V изчисления общо? (hint: 1 + 2 + 3 + ... + 100)
2. С cache: колко K, V изчисления общо?
3. Колко пъти е по-ефективен cache?

**Формула:** $1 + 2 + ... + n = \frac{n(n+1)}{2}$

---

## Упражнение 12: Flash Attention концепция

Standard attention изисква съхранение на пълната $n \times n$ attention матрица.

Flash Attention изчислява на блокове без материализиране на пълната матрица.

**Въпроси:**
1. За $n = 8192$ токена, колко елемента има attention матрицата?
2. При float32 (4 bytes), колко MB е това?
3. Ако GPU има 24GB RAM, какъв е максималният $n$ при standard attention? (hint: $\sqrt{24 \times 1024^3 / 4}$)
4. Flash attention има O(n) memory. Какво означава това за практиката?

---

# Решения

## Решение 1
1. Скрит слой: $512 \times 4 = 2048$
2. $W_1$: $512 \times 2048 = 1,048,576$
3. $W_2$: $2048 \times 512 = 1,048,576$
4. Общо: $2 \times 1,048,576 = 2,097,152$ (~2M параметъра)

## Решение 2
1. output = $[1.1, 1.8, 3.3]$
2. Градиентът се раздвоява: $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial \text{out}} \cdot (1 + \frac{\partial \text{Sublayer}}{\partial x})$. Членът "1" осигурява директен path.
3. Директният gradient path предотвратява vanishing gradients при много слоеве.

## Решение 3
1. $\mu = (2 + 4 + 6) / 3 = 4$
2. $\sigma^2 = \frac{1}{3}[(2-4)^2 + (4-4)^2 + (6-4)^2] = \frac{1}{3}[4 + 0 + 4] = \frac{8}{3} \approx 2.67$
3. $\sigma = \sqrt{2.67} \approx 1.63$
4. $\hat{x} = \frac{[2-4, 4-4, 6-4]}{1.63} = \frac{[-2, 0, 2]}{1.63} \approx [-1.22, 0, 1.22]$

## Решение 4
1. Pre-Norm има по-чист residual path (директно x + ...)
2. Pre-Norm (LLaMA, GPT-3, повечето модерни модели)
3. Residual stream не минава през normalization, запазвайки стабилен gradient flow

## Решение 5
1. Bidirectional:
```
[1 1 1 1]
[1 1 1 1]
[1 1 1 1]
[1 1 1 1]
```

2. Causal:
```
[1 0 0 0]
[1 1 0 0]
[1 1 1 0]
[1 1 1 1]
```

Decoder-only е natural за генерация (next-token prediction) и всички задачи могат да се формулират като генерация.

## Решение 6
1. MHA: $2 \times 32 \times 8192 \times 128 \times 2 = 134,217,728$ bytes $= 128$ MB
2. GQA: $2 \times 8 \times 8192 \times 128 \times 2 = 33,554,432$ bytes $= 32$ MB
3. MQA: $2 \times 1 \times 8192 \times 128 \times 2 = 4,194,304$ bytes $= 4$ MB

GQA спестява $(1 - 32/128) \times 100 = 75\%$ памет спрямо MHA.

## Решение 7
1. Attention: $4 \times 768 \times 768 = 2,359,296$
2. FFN: $768 \times 3072 + 3072 \times 768 = 4,718,592$
3. LayerNorm: $2 \times 2 \times 768 = 3,072$
4. Общо: $2,359,296 + 4,718,592 + 3,072 = 7,080,960$ (~7M на блок)

## Решение 8
1. b) Относителната позиция (5 - 3 = 2)
2. Езикът зависи от относителни позиции (съседни думи, отдалечени референции), не от абсолютни
3. Моделът може да генерализира към по-дълги последователности, защото не е виждал абсолютни позиции > training length

## Решение 9
1. Scale factor: $4096 / 16384 = 0.25$
2. Позиция 8000: $8000 \times 0.25 = 2000$
3. 2000 < 4096, така че е в training range

## Решение 10
1. Standard: $32000^2 = 1,024,000,000$ (>1 милиард)
2. Sliding window: $32000 \times 4096 = 131,072,000$ (~131M)
3. $1024M / 131M \approx 7.8$ пъти по-малко

Информацията се пренася "каскадно" през слоевете — всеки слой разширява ефективния receptive field.

## Решение 11
1. Без cache: $\frac{100 \times 101}{2} = 5050$ изчисления
2. С cache: 100 изчисления (по 1 за всеки нов токен)
3. $5050 / 100 = 50.5$ пъти по-ефективен

## Решение 12
1. $8192^2 = 67,108,864$ елемента
2. $67,108,864 \times 4 / 1024^2 = 256$ MB
3. $\sqrt{24 \times 1024^3 / 4} = \sqrt{6,442,450,944} \approx 80,265$ токена (при идеални условия, реално много по-малко)
4. Flash attention премахва O(n²) memory ограничението, позволявайки много по-дълги последователности без да свършва паметта
