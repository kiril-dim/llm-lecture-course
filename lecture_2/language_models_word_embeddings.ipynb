{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –õ–µ–∫—Ü–∏—è 2: –ï–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –Ω–∞ –¥—É–º–∏\n",
    "\n",
    "## –û—Ç Bag-of-Words –∫—ä–º —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏\n",
    "\n",
    "**–ü—Ä–æ–¥—ä–ª–∂–∏—Ç–µ–ª–Ω–æ—Å—Ç:** 2-2.5 —á–∞—Å–∞  \n",
    "**–ü—Ä–µ–¥–ø–æ—Å—Ç–∞–≤–∫–∞:** –õ–µ–∫—Ü–∏—è 1 (ML –æ—Å–Ω–æ–≤–∏, NLP –º–æ—Ç–∏–≤–∞—Ü–∏—è)  \n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. –û—Ç Bag-of-Words –∫—ä–º –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "\n",
    "### –ö–∞–∫–≤–æ –Ω–∞—É—á–∏—Ö–º–µ –≤ –õ–µ–∫—Ü–∏—è 1?\n",
    "\n",
    "–í –ø—Ä–µ–¥–∏—à–Ω–∞—Ç–∞ –ª–µ–∫—Ü–∏—è –≤–∏–¥—è—Ö–º–µ **Bag of Words (BoW)** –∏ **TF-IDF** - –ø—Ä–æ—Å—Ç–∏ –Ω–æ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏ –º–µ—Ç–æ–¥–∏ –∑–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –Ω–∞ —Ç–µ–∫—Å—Ç.\n",
    "\n",
    "### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ Bag-of-Words\n",
    "\n",
    "‚ùå **–ù—è–º–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞ —Ä–µ–¥ –Ω–∞ –¥—É–º–∏—Ç–µ**\n",
    "- \"–∫—É—á–µ—Ç–æ –≥–æ–Ω–∏ –∫–æ—Ç–∫–∞—Ç–∞\" = \"–∫–æ—Ç–∫–∞—Ç–∞ –≥–æ–Ω–∏ –∫—É—á–µ—Ç–æ\"\n",
    "- –†–∞–∑–ª–∏—á–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ, —Å—ä—â–æ—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ!\n",
    "\n",
    "‚ùå **–ù—è–º–∞ –ø–æ–Ω—è—Ç–∏–µ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∞ –ø—Ä–∏–ª–∏–∫–∞**\n",
    "- \"–∫—Ä–∞–ª\" –∏ \"–∫—Ä–∞–ª–∏—Ü–∞\" —Å–∞ –Ω–∞–ø—ä–ª–Ω–æ —Ä–∞–∑–ª–∏—á–Ω–∏ –≤ BoW\n",
    "- –ù–æ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ —Å–∞ –º–Ω–æ–≥–æ –±–ª–∏–∑–∫–∏!\n",
    "\n",
    "‚ùå **–†–∞–∑—Ä–µ–¥–µ–Ω–∏, –≤–∏—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è**\n",
    "- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç = —Ä–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞ (10,000+)\n",
    "- –ü–æ–≤–µ—á–µ—Ç–æ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ —Å–∞ 0\n",
    "\n",
    "### –¶–µ–Ω—Ç—Ä–∞–ª–µ–Ω –≤—ä–ø—Ä–æ—Å: –ö–∞–∫–≤–æ –æ–∑–Ω–∞—á–∞–≤–∞ –¥—É–º–∏—Ç–µ –¥–∞ –∏–º–∞—Ç \"–∑–Ω–∞—á–µ–Ω–∏–µ\"?\n",
    "\n",
    "–¢—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞:\n",
    "\n",
    "**1. –°–∏–º–≤–æ–ª–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è (Symbolic)**\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞—Ç –∑–Ω–∞–Ω–∏—è: WordNet, knowledge graphs\n",
    "- –ï–∫—Å–ø–µ—Ä—Ç–Ω–∏ —Å–∏—Å—Ç–µ–º–∏ —Å —Ä—ä—á–Ω–æ –¥–µ—Ñ–∏–Ω–∏—Ä–∞–Ω–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è\n",
    "\n",
    "**2. –î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ (Distributional)**\n",
    "- \"–©–µ –ø–æ–∑–Ω–∞–µ—à –¥—É–º–∞ –ø–æ –∫–æ–º–ø–∞–Ω–∏—è—Ç–∞, –∫–æ—è—Ç–æ –¥—ä—Ä–∂–∏\" (Firth, 1957)\n",
    "- –ë–∞–∑–∏—Ä–∞–Ω–∏ –Ω–∞ —Å—ä–≤–º–µ—Å—Ç–Ω–∞ —Å—Ä–µ—â–∞–µ–º–æ—Å—Ç –≤ —Ç–µ–∫—Å—Ç\n",
    "\n",
    "**3. –†–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è (Distributed)**\n",
    "- –ü–ª—ä—Ç–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏ (embeddings)\n",
    "- –ù–∞—É—á–µ–Ω–∏ –æ—Ç –¥–∞–Ω–Ω–∏ (Word2Vec, GloVe)\n",
    "\n",
    "### –ü—ä—Ç–Ω–∞ –∫–∞—Ä—Ç–∞ –∑–∞ —Ç–∞–∑–∏ –ª–µ–∫—Ü–∏—è\n",
    "\n",
    "```\n",
    "WordNet ‚Üí N-gram –º–æ–¥–µ–ª–∏ ‚Üí –î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ‚Üí Word2Vec\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–∏—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏—Ç–µ\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì –í—Å–∏—á–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏ —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. –°–∏–º–≤–æ–ª–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è: WordNet\n",
    "\n",
    "### –ö–∞–∫–≤–æ –µ WordNet?\n",
    "\n",
    "**WordNet** –µ –ª–µ–∫—Å–∏–∫–∞–ª–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–Ω–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏—è –µ–∑–∏–∫, –∫–æ—è—Ç–æ –æ—Ä–≥–∞–Ω–∏–∑–∏—Ä–∞ –¥—É–º–∏—Ç–µ –≤ –º—Ä–µ–∂–∞ –æ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏ –≤—Ä—ä–∑–∫–∏.\n",
    "\n",
    "- –°—ä–∑–¥–∞–¥–µ–Ω–∞ –≤ Princeton University (1985-)\n",
    "- –û—Ä–≥–∞–Ω–∏–∑–∏—Ä–∞–Ω–∞ –≤ **synsets** (synonym sets) - –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –æ—Ç —Å–∏–Ω–æ–Ω–∏–º–∏\n",
    "- –°—ä–¥—ä—Ä–∂–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è\n",
    "\n",
    "### –ö–ª—é—á–æ–≤–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –≤ WordNet\n",
    "\n",
    "**–°–∏–Ω–æ–Ω–∏–º–∏ (Synonyms)**\n",
    "- {car, automobile, motorcar, machine}\n",
    "- –î—É–º–∏ —Å –ø–æ–¥–æ–±–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ synset\n",
    "\n",
    "**–•–∏–ø–µ—Ä–æ–Ω–∏–º–∏/–•–∏–ø–æ–Ω–∏–º–∏ (Hypernyms/Hyponyms)**\n",
    "- –ô–µ—Ä–∞—Ä—Ö–∏—á–Ω–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è \"–µ –≤–∏–¥\"\n",
    "- animal ‚Üí dog ‚Üí beagle\n",
    "- dog –µ —Ö–∏–ø–æ–Ω–∏–º –Ω–∞ animal\n",
    "- animal –µ —Ö–∏–ø–µ—Ä–æ–Ω–∏–º –Ω–∞ dog\n",
    "\n",
    "**–ú–µ—Ä–æ–Ω–∏–º–∏ (Meronyms)**\n",
    "- –û—Ç–Ω–æ—à–µ–Ω–∏—è \"—á–∞—Å—Ç –æ—Ç\"\n",
    "- wheel –µ —á–∞—Å—Ç –æ—Ç car\n",
    "- engine –µ —á–∞—Å—Ç –æ—Ç car\n",
    "\n",
    "**–•–æ–ª–æ–Ω–∏–º–∏ (Holonyms)**\n",
    "- –û–±—Ä–∞—Ç–Ω–æ—Ç–æ –Ω–∞ –º–µ—Ä–æ–Ω–∏–º\n",
    "- car —Å—ä–¥—ä—Ä–∂–∞ wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–ø–∏—Ç –∑–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ nltk –∏ WordNet\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    \n",
    "    # –û–ø–∏—Ç –∑–∞ –∏–∑—Ç–µ–≥–ª—è–Ω–µ –Ω–∞ WordNet –¥–∞–Ω–Ω–∏ –∞–∫–æ –Ω–µ —Å–∞ –Ω–∞–ª–∏—á–Ω–∏\n",
    "    try:\n",
    "        wn.synsets('dog')\n",
    "        print(\"‚úì WordNet –µ –Ω–∞–ª–∏—á–µ–Ω!\")\n",
    "    except:\n",
    "        print(\"–ò–∑—Ç–µ–≥–ª—è–Ω–µ –Ω–∞ WordNet –¥–∞–Ω–Ω–∏...\")\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('omw-1.4', quiet=True)\n",
    "        print(\"‚úì WordNet –µ –Ω–∞–ª–∏—á–µ–Ω!\")\n",
    "        \n",
    "    WORDNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  NLTK –Ω–µ –µ –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω. –ú–æ–ª—è –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–π—Ç–µ —Å: pip install nltk\")\n",
    "    WORDNET_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WORDNET_AVAILABLE:\n",
    "    print(\"üîç –ò–∑—Å–ª–µ–¥–≤–∞–Ω–µ –Ω–∞ WordNet\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Synsets –∑–∞ –¥—É–º–∞—Ç–∞ \"dog\"\n",
    "    word = \"dog\"\n",
    "    synsets = wn.synsets(word)\n",
    "    \n",
    "    print(f\"Synsets –∑–∞ '{word}':\")\n",
    "    for i, syn in enumerate(synsets[:3], 1):\n",
    "        print(f\"\\n{i}. {syn.name()}\")\n",
    "        print(f\"   –î–µ—Ñ–∏–Ω–∏—Ü–∏—è: {syn.definition()}\")\n",
    "        print(f\"   –ü—Ä–∏–º–µ—Ä–∏: {syn.examples()}\")\n",
    "        print(f\"   –õ–µ–º–∏: {[lemma.name() for lemma in syn.lemmas()]}\")\n",
    "    \n",
    "    # –•–∏–ø–µ—Ä–æ–Ω–∏–º–∏ (–ø–æ-–æ–±—â–∏ –ø–æ–Ω—è—Ç–∏—è)\n",
    "    dog_syn = wn.synset('dog.n.01')\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"\\nüîº –•–∏–ø–µ—Ä–æ–Ω–∏–º–∏ –Ω–∞ 'dog' (–ø–æ-–æ–±—â–∏ –ø–æ–Ω—è—Ç–∏—è):\")\n",
    "    for hyper in dog_syn.hypernyms():\n",
    "        print(f\"  ‚Ä¢ {hyper.name()}: {hyper.definition()}\")\n",
    "    \n",
    "    # –•–∏–ø–æ–Ω–∏–º–∏ (–ø–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏ –ø–æ–Ω—è—Ç–∏—è)\n",
    "    print(f\"\\nüîΩ –•–∏–ø–æ–Ω–∏–º–∏ –Ω–∞ 'dog' (–ø–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏ –ø–æ–Ω—è—Ç–∏—è):\")\n",
    "    hyponyms = dog_syn.hyponyms()[:5]\n",
    "    for hypo in hyponyms:\n",
    "        print(f\"  ‚Ä¢ {hypo.name()}: {hypo.definition()}\")\n",
    "    \n",
    "    # –ô–µ—Ä–∞—Ä—Ö–∏—è\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\\nüå≥ –ô–µ—Ä–∞—Ä—Ö–∏—è (–æ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –∫—ä–º –æ–±—â–æ):\")\n",
    "    paths = dog_syn.hypernym_paths()\n",
    "    if paths:\n",
    "        path = paths[0]  # –í–∑–µ–º–∞–º–µ –ø—ä—Ä–≤–∞—Ç–∞ –ø—ä—Ç–µ–∫–∞\n",
    "        for syn in reversed(path[-5:]):  # –ü–æ—Å–ª–µ–¥–Ω–∏—Ç–µ 5 –Ω–∏–≤–∞\n",
    "            print(f\"  {'  ' * (len(path) - path.index(syn))}‚Üì {syn.name()}\")\n",
    "    \n",
    "    # –°–µ–º–∞–Ω—Ç–∏—á–Ω–∞ –ø—Ä–∏–ª–∏–∫–∞\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\\nüìè –°–µ–º–∞–Ω—Ç–∏—á–Ω–∞ –ø—Ä–∏–ª–∏–∫–∞ (path similarity):\")\n",
    "    \n",
    "    cat = wn.synset('cat.n.01')\n",
    "    car = wn.synset('car.n.01')\n",
    "    \n",
    "    dog_cat_sim = dog_syn.path_similarity(cat)\n",
    "    dog_car_sim = dog_syn.path_similarity(car)\n",
    "    \n",
    "    print(f\"  dog ‚Üî cat: {dog_cat_sim:.3f}\")\n",
    "    print(f\"  dog ‚Üî car: {dog_car_sim:.3f}\")\n",
    "    print(\"\\n  üí° –ü–æ-–≤–∏—Å–æ–∫–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ = –ø–æ-–≥–æ–ª—è–º–∞ –ø—Ä–∏–ª–∏–∫–∞\")\n",
    "else:\n",
    "    print(\"WordNet –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ç–∞ –µ –ø—Ä–æ–ø—É—Å–Ω–∞—Ç–∞ (NLTK –Ω–µ –µ –Ω–∞–ª–∏—á–µ–Ω)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ WordNet\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∏:**\n",
    "\n",
    "‚ùå **–ò–Ω—Ç–µ–Ω–∑–∏–≤–µ–Ω —á–æ–≤–µ—à–∫–∏ —Ç—Ä—É–¥**\n",
    "- –ò–∑–∏—Å–∫–≤–∞ –µ–∫—Å–ø–µ—Ä—Ç–∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏\n",
    "- –°–∫—ä–ø–æ –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –∏ –ø–æ–¥–¥—Ä—ä–∂–∫–∞\n",
    "\n",
    "‚ùå **–õ–∏–ø—Å–≤–∞—Ç –Ω–æ–≤–∏ –¥—É–º–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è**\n",
    "- \"selfie\", \"cryptocurrency\", \"ChatGPT\"\n",
    "- –ï–∑–∏–∫—ä—Ç –µ–≤–æ–ª—é–∏—Ä–∞ –ø–æ-–±—ä—Ä–∑–æ –æ—Ç WordNet\n",
    "\n",
    "‚ùå **–°—É–±–µ–∫—Ç–∏–≤–Ω–∏ –ø—Ä–µ—Ü–µ–Ω–∫–∏**\n",
    "- –ö–æ–∏ –¥—É–º–∏ —Å–∞ —Å–∏–Ω–æ–Ω–∏–º–∏?\n",
    "- –ö–æ–ª–∫–æ –±–ª–∏–∑–∫–∏ —Å–∞ –¥–≤–∞ –∫–æ–Ω—Ü–µ–ø—Ç–∞?\n",
    "\n",
    "‚ùå **–ù—è–º–∞ –Ω—é–∞–Ω—Å–∏ –≤ –ø—Ä–∏–ª–∏–∫–∞—Ç–∞**\n",
    "- –î–∏—Å–∫—Ä–µ—Ç–Ω–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (–∏–º–∞/–Ω—è–º–∞)\n",
    "- –ù–µ –º–æ–∂–µ –¥–∞ –∫–∞–∂–µ \"–∫—Ä–∞–ª –µ 0.8 –ø–æ–¥–æ–±–µ–Ω –Ω–∞ –∫—Ä–∞–ª–∏—Ü–∞\"\n",
    "\n",
    "‚ùå **–ù–µ —É–ª–∞–≤—è –∫–æ–Ω—Ç–µ–∫—Å—Ç-–∑–∞–≤–∏—Å–∏–º–æ –∑–Ω–∞—á–µ–Ω–∏–µ**\n",
    "- \"–±–∞–Ω–∫–∞\" –º–æ–∂–µ –¥–∞ –æ–∑–Ω–∞—á–∞–≤–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞ –∏–Ω—Å—Ç–∏—Ç—É—Ü–∏—è –∏–ª–∏ —Ä–µ—á–µ–Ω –±—Ä—è–≥\n",
    "- WordNet –∏–º–∞ –æ—Ç–¥–µ–ª–Ω–∏ synsets, –Ω–æ –Ω–µ –∑–Ω–∞–µ –∫–æ–π –¥–∞ –∏–∑–±–µ—Ä–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "\n",
    "**–í—ä–ø—Ä–µ–∫–∏ —Ç–æ–≤–∞:** WordNet –æ—Å—Ç–∞–≤–∞ —Ü–µ–Ω–µ–Ω —Ä–µ—Å—É—Ä—Å –∑–∞ –º–Ω–æ–≥–æ NLP –∑–∞–¥–∞—á–∏!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –µ–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏: N-–≥—Ä–∞–º–∏\n",
    "\n",
    "### –ö–∞–∫–≤–æ –µ –µ–∑–∏–∫–æ–≤ –º–æ–¥–µ–ª?\n",
    "\n",
    "**–ï–∑–∏–∫–æ–≤ –º–æ–¥–µ–ª (Language Model)** –µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞ –º–æ–¥–µ–ª, –∫–æ—è—Ç–æ –ø—Ä–∏—Å–≤–æ—è–≤–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏ –æ—Ç –¥—É–º–∏.\n",
    "\n",
    "**–§–æ—Ä–º–∞–ª–Ω–∞ –¥–µ—Ñ–∏–Ω–∏—Ü–∏—è:**\n",
    "$$P(w_1, w_2, ..., w_n)$$\n",
    "\n",
    "### –ó–∞—â–æ —Å–µ –Ω—É–∂–¥–∞–µ–º –æ—Ç –µ–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏?\n",
    "\n",
    "**–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è:**\n",
    "- **–†–∞–∑–ø–æ–∑–Ω–∞–≤–∞–Ω–µ –Ω–∞ —Ä–µ—á:** \"recognize speech\" —Å—Ä–µ—â—É \"wreck a nice beach\"\n",
    "- **–ú–∞—à–∏–Ω–µ–Ω –ø—Ä–µ–≤–æ–¥:** –ò–∑–±–æ—Ä –Ω–∞ –Ω–∞–π-–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–∏—è –ø—Ä–µ–≤–æ–¥\n",
    "- **–ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ —Ç–µ–∫—Å—Ç:** ChatGPT, –∞–≤—Ç–æ–∫–æ–º–ø–ª–∏–π—Ç\n",
    "- **–ü—Ä–∞–≤–æ–ø–∏—Å–Ω–∞ –∫–æ—Ä–µ–∫—Ü–∏—è:** \"I have goed\" ‚Üí \"I have gone\"\n",
    "\n",
    "### –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏\n",
    "\n",
    "**Chain Rule (–ø—Ä–∞–≤–∏–ª–æ –Ω–∞ –≤–µ—Ä–∏–≥–∞—Ç–∞):**\n",
    "\n",
    "$$P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i | w_1, ..., w_{i-1})$$\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º:** –¢–≤—ä—Ä–¥–µ –º–Ω–æ–≥–æ –≤—ä–∑–º–æ–∂–Ω–∏ –∏—Å—Ç–æ—Ä–∏–∏!\n",
    "\n",
    "–ó–∞ —Ä–µ—á–Ω–∏–∫ —Å 50,000 –¥—É–º–∏:\n",
    "- 1-–¥—É–º–∞ –∏—Å—Ç–æ—Ä–∏—è: 50,000 –≤—ä–∑–º–æ–∂–Ω–æ—Å—Ç–∏\n",
    "- 2-–¥—É–º–∏ –∏—Å—Ç–æ—Ä–∏—è: 2.5 –º–∏–ª–∏–∞—Ä–¥–∞\n",
    "- 3-–¥—É–º–∏ –∏—Å—Ç–æ—Ä–∏—è: 125 —Ç—Ä–∏–ª–∏–æ–Ω–∞!\n",
    "\n",
    "### N-gram –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è\n",
    "\n",
    "**Markov –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ –Ω–∞ –¥—É–º–∞ –∑–∞–≤–∏—Å–∏ —Å–∞–º–æ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ N-1 –¥—É–º–∏.\n",
    "\n",
    "**Unigram (N=1):**\n",
    "$$P(w_i)$$\n",
    "- –ò–≥–Ω–æ—Ä–∏—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–ø—ä–ª–Ω–æ\n",
    "\n",
    "**Bigram (N=2):**\n",
    "$$P(w_i | w_{i-1})$$\n",
    "- –ó–∞–≤–∏—Å–∏ —Å–∞–º–æ –æ—Ç –ø—Ä–µ–¥–∏—à–Ω–∞—Ç–∞ –¥—É–º–∞\n",
    "\n",
    "**Trigram (N=3):**\n",
    "$$P(w_i | w_{i-2}, w_{i-1})$$\n",
    "- –ó–∞–≤–∏—Å–∏ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ 2 –¥—É–º–∏\n",
    "\n",
    "### –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE):**\n",
    "\n",
    "–ë—Ä–æ–∏–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–º–µ:\n",
    "\n",
    "$$P(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$$\n",
    "\n",
    "–ö—ä–¥–µ—Ç–æ:\n",
    "- $C(w_{i-1}, w_i)$ = –±—Ä–æ–π –Ω–∞ –¥–≤–æ–π–∫–∞—Ç–∞ $(w_{i-1}, w_i)$ –≤ –∫–æ—Ä–ø—É—Å–∞\n",
    "- $C(w_{i-1})$ = –±—Ä–æ–π –Ω–∞ $w_{i-1}$ –≤ –∫–æ—Ä–ø—É—Å–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ù–µ–∫–∞ –¥–∞ –ø–æ—Å—Ç—Ä–æ–∏–º Bigram –º–æ–¥–µ–ª!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä–µ–Ω –∫–æ—Ä–ø—É—Å –Ω–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∏\n",
    "corpus = [\n",
    "    \"–∫–æ—Ç–∫–∞—Ç–∞ –≥–æ–Ω–∏ –º–∏—à–∫–∞—Ç–∞\",\n",
    "    \"–∫—É—á–µ—Ç–æ –≥–æ–Ω–∏ –∫–æ—Ç–∫–∞—Ç–∞\",\n",
    "    \"–∫–æ—Ç–∫–∞—Ç–∞ —Å–ø–∏ –Ω–∞ –¥–∏–≤–∞–Ω–∞\",\n",
    "    \"–∫—É—á–µ—Ç–æ —Å–ø–∏ –Ω–∞ –∫–∏–ª–∏–º–∞\",\n",
    "    \"–º–∏—à–∫–∞—Ç–∞ —è–¥–µ —Å–∏—Ä–µ–Ω–µ\",\n",
    "    \"–∫–æ—Ç–∫–∞—Ç–∞ —è–¥–µ —Ä–∏–±–∞\",\n",
    "    \"–∫—É—á–µ—Ç–æ —è–¥–µ –º–µ—Å–æ\",\n",
    "    \"–ø—Ç–∏—Ü–∞—Ç–∞ –ª–µ—Ç–∏ –≤–∏—Å–æ–∫–æ\",\n",
    "    \"–∫–æ—Ç–∫–∞—Ç–∞ –≥–æ–Ω–∏ –ø—Ç–∏—Ü–∞—Ç–∞\"\n",
    "]\n",
    "\n",
    "print(\"üìö –ù–∞—à–∏—è—Ç –∫–æ—Ä–ø—É—Å:\")\n",
    "for i, sent in enumerate(corpus, 1):\n",
    "    print(f\"{i}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel:\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.unigram_counts = Counter()\n",
    "        self.vocab = set()\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"–û–±—É—á–∞–≤–∞ –º–æ–¥–µ–ª–∞ –Ω–∞ –¥–∞–¥–µ–Ω –∫–æ—Ä–ø—É—Å\"\"\"\n",
    "        for sentence in corpus:\n",
    "            words = ['<START>'] + sentence.split() + ['<END>']\n",
    "            \n",
    "            for word in words:\n",
    "                self.vocab.add(word)\n",
    "            \n",
    "            # –ü—Ä–µ–±—Ä–æ—è–≤–∞–º–µ –±–∏–≥—Ä–∞–º–∏\n",
    "            for i in range(len(words) - 1):\n",
    "                w1, w2 = words[i], words[i+1]\n",
    "                self.bigram_counts[w1][w2] += 1\n",
    "                self.unigram_counts[w1] += 1\n",
    "    \n",
    "    def get_probability(self, w1, w2):\n",
    "        \"\"\"–í—Ä—ä—â–∞ P(w2 | w1)\"\"\"\n",
    "        if self.unigram_counts[w1] == 0:\n",
    "            return 0.0\n",
    "        return self.bigram_counts[w1][w2] / self.unigram_counts[w1]\n",
    "    \n",
    "    def get_next_word_distribution(self, word):\n",
    "        \"\"\"–í—Ä—ä—â–∞ —Ä–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ—Ç–æ –Ω–∞ —Å–ª–µ–¥–≤–∞—â–∏—Ç–µ –¥—É–º–∏ –¥–∞–¥–µ–Ω–∞ —Ç–µ–∫—É—â–∞ –¥—É–º–∞\"\"\"\n",
    "        if word not in self.bigram_counts:\n",
    "            return {}\n",
    "        \n",
    "        total = sum(self.bigram_counts[word].values())\n",
    "        return {w: count/total for w, count in self.bigram_counts[word].items()}\n",
    "    \n",
    "    def generate_sentence(self, max_length=15):\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä–∞ –∏–∑—Ä–µ—á–µ–Ω–∏–µ —Å –º–æ–¥–µ–ª–∞\"\"\"\n",
    "        sentence = ['<START>']\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            current_word = sentence[-1]\n",
    "            next_word_dist = self.get_next_word_distribution(current_word)\n",
    "            \n",
    "            if not next_word_dist or '<END>' in sentence:\n",
    "                break\n",
    "            \n",
    "            # –ò–∑–±–∏—Ä–∞–º–µ —Å–ª–µ–¥–≤–∞—â–∞ –¥—É–º–∞ –±–∞–∑–∏—Ä–∞–Ω–æ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏—Ç–µ\n",
    "            words = list(next_word_dist.keys())\n",
    "            probs = list(next_word_dist.values())\n",
    "            next_word = np.random.choice(words, p=probs)\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            \n",
    "            if next_word == '<END>':\n",
    "                break\n",
    "        \n",
    "        return ' '.join(sentence[1:-1])  # –ü—Ä–µ–º–∞—Ö–≤–∞–º–µ <START> –∏ <END>\n",
    "\n",
    "# –û–±—É—á–∞–≤–∞–º–µ –º–æ–¥–µ–ª–∞\n",
    "lm = BigramLanguageModel()\n",
    "lm.train(corpus)\n",
    "\n",
    "print(\"‚úì Bigram –º–æ–¥–µ–ª—ä—Ç –µ –æ–±—É—á–µ–Ω!\")\n",
    "print(f\"\\n–†–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞: {len(lm.vocab)} –¥—É–º–∏\")\n",
    "print(f\"–ë—Ä–æ–π –±–∏–≥—Ä–∞–º–∏: {sum(len(counts) for counts in lm.bigram_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑—Å–ª–µ–¥–≤–∞–Ω–µ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "print(\"üîç –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –±–∏–≥—Ä–∞–º–∏\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_bigrams = [\n",
    "    (\"–∫–æ—Ç–∫–∞—Ç–∞\", \"–≥–æ–Ω–∏\"),\n",
    "    (\"–∫–æ—Ç–∫–∞—Ç–∞\", \"—Å–ø–∏\"),\n",
    "    (\"–∫–æ—Ç–∫–∞—Ç–∞\", \"–ª–µ—Ç–∏\"),  # –ú–∞–ª–∫–æ –≤–µ—Ä–æ—è—Ç–Ω–æ!\n",
    "    (\"–∫—É—á–µ—Ç–æ\", \"—è–¥–µ\"),\n",
    "]\n",
    "\n",
    "for w1, w2 in test_bigrams:\n",
    "    prob = lm.get_probability(w1, w2)\n",
    "    print(f\"P({w2} | {w1}) = {prob:.3f}\")\n",
    "\n",
    "# –†–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥–≤–∞—â–∏ –¥—É–º–∏\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüìä –†–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: –ö–∞–∫–≤–æ –º–æ–∂–µ –¥–∞ —Å–ª–µ–¥–≤–∞ '–∫–æ—Ç–∫–∞—Ç–∞'?\\n\")\n",
    "\n",
    "next_dist = lm.get_next_word_distribution('–∫–æ—Ç–∫–∞—Ç–∞')\n",
    "sorted_next = sorted(next_dist.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for word, prob in sorted_next:\n",
    "    bar = '‚ñà' * int(prob * 50)\n",
    "    print(f\"{word:12s} {prob:.3f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –∏–∑—Ä–µ—á–µ–Ω–∏—è\n",
    "print(\"üé≤ –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –∏–∑—Ä–µ—á–µ–Ω–∏—è —Å Bigram –º–æ–¥–µ–ª–∞\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(5):\n",
    "    sentence = lm.generate_sentence()\n",
    "    print(f\"{i+1}. {sentence}\")\n",
    "\n",
    "print(\"\\nüí° –ó–∞–±–µ–ª–µ–∂–∫–∞: –ò–∑—Ä–µ—á–µ–Ω–∏—è—Ç–∞ —Å–ª–µ–¥–≤–∞—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ç–µ –º–æ–¥–µ–ª–∏ –æ—Ç –∫–æ—Ä–ø—É—Å–∞,\")\n",
    "print(\"   –Ω–æ –º–æ–∂–µ –¥–∞ –Ω–µ —Å–∞ –Ω–∞–ø—ä–ª–Ω–æ –≥—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–∏–ª–Ω–∏ –∏–ª–∏ —Å–º–∏—Å–ª–µ–Ω–∏.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–µ–¥–∏–∑–≤–∏–∫–∞—Ç–µ–ª—Å—Ç–≤–∞ –Ω–∞ N-gram –º–æ–¥–µ–ª–∏—Ç–µ\n",
    "\n",
    "**1. –†–∞–∑—Ä–µ–¥–µ–Ω–æ—Å—Ç (Sparsity)**\n",
    "- –ú–Ω–æ–≥–æ n-–≥—Ä–∞–º–∏ –Ω–∏–∫–æ–≥–∞ –Ω–µ —Å–µ —Å—Ä–µ—â–∞—Ç –≤ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ\n",
    "- $P(w_i | w_{i-1}) = 0$ –∑–∞ –Ω–µ–≤–∏–∂–¥–∞–Ω–∏ –¥–≤–æ–π–∫–∏\n",
    "- –ü—Ä–æ–±–ª–µ–º: –æ—Ç—Ö–≤—ä—Ä–ª—è –≤–∞–ª–∏–¥–Ω–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è\n",
    "\n",
    "**2. –°—ä—Ö—Ä–∞–Ω–µ–Ω–∏–µ**\n",
    "- –¢—Ä—è–±–≤–∞ –¥–∞ –ø–∞–∑–∏–º –±—Ä–æ—è—á–∏ –∑–∞ –≤—Å–∏—á–∫–∏ n-–≥—Ä–∞–º–∏\n",
    "- –ó–∞ –≥–æ–ª—è–º –∫–æ—Ä–ø—É—Å: –º–∏–ª–∏–æ–Ω–∏ –±–∏–≥—Ä–∞–º–∏, –º–∏–ª–∏–∞—Ä–¥–∏ —Ç—Ä–∏–≥—Ä–∞–º–∏\n",
    "\n",
    "**3. –õ–∏–ø—Å–∞ –Ω–∞ –æ–±–æ–±—â–µ–Ω–∏–µ**\n",
    "- \"–∫–æ—Ç–∫–∞—Ç–∞ –≥–æ–Ω–∏ –º–∏—à–∫–∞—Ç–∞\" –Ω–µ –ø–æ–º–∞–≥–∞ –∑–∞ \"–∫—É—á–µ—Ç–æ –≥–æ–Ω–∏ –º–∏—à–∫–∞—Ç–∞\"\n",
    "- –í—Å—è–∫–∞ –¥—É–º–∞ —Å–µ —Ç—Ä–µ—Ç–∏—Ä–∞ –∫–∞—Ç–æ –Ω–∞–ø—ä–ª–Ω–æ —Ä–∞–∑–ª–∏—á–Ω–∞\n",
    "\n",
    "### –†–µ—à–µ–Ω–∏–µ: Smoothing (–∏–∑–≥–ª–∞–∂–¥–∞–Ω–µ)\n",
    "\n",
    "**Add-one (Laplace) smoothing:**\n",
    "\n",
    "$$P(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$$\n",
    "\n",
    "–ö—ä–¥–µ—Ç–æ $V$ = —Ä–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞\n",
    "\n",
    "**–ü–æ-—Å–ª–æ–∂–Ω–∏ –º–µ—Ç–æ–¥–∏:**\n",
    "- Kneser-Ney smoothing\n",
    "- Backoff –∏ interpolation\n",
    "\n",
    "üìå **–°—ä–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–µ—à–µ–Ω–∏–µ:** –ù–µ–≤—Ä–æ–Ω–Ω–∏ –µ–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏ (–õ–µ–∫—Ü–∏—è 4-5)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. –û—Ü–µ–Ω–∫–∞ –Ω–∞ –µ–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "### –ö–∞–∫ –∏–∑–º–µ—Ä–≤–∞–º–µ –∫–∞—á–µ—Å—Ç–≤–æ—Ç–æ –Ω–∞ –µ–∑–∏–∫–æ–≤ –º–æ–¥–µ–ª?\n",
    "\n",
    "–ò—Å–∫–∞–º–µ –º–æ–¥–µ–ª, –∫–æ–π—Ç–æ:\n",
    "- –ü—Ä–∏—Å–≤–æ—è–≤–∞ –≤–∏—Å–æ–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç –Ω–∞ –≤–∞–ª–∏–¥–Ω–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è\n",
    "- –ü—Ä–∏—Å–≤–æ—è–≤–∞ –Ω–∏—Å–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç –Ω–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è\n",
    "\n",
    "### Perplexity (–æ–±—ä—Ä–∫–≤–∞–Ω–µ)\n",
    "\n",
    "**–ò–Ω—Ç—É–∏—Ü–∏—è:** –ö–æ–ª–∫–æ \"–∏–∑–Ω–µ–Ω–∞–¥–∞–Ω\" –µ –º–æ–¥–µ–ª—ä—Ç –æ—Ç —Ç–µ—Å—Ç–æ–≤–∏—Ç–µ –¥–∞–Ω–Ω–∏?\n",
    "\n",
    "**–î–µ—Ñ–∏–Ω–∏—Ü–∏—è:**\n",
    "\n",
    "$$PP(W) = P(w_1, w_2, ..., w_N)^{-1/N}$$\n",
    "\n",
    "–ò–ª–∏ –µ–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ:\n",
    "\n",
    "$$PP = 2^{H}$$\n",
    "\n",
    "–ö—ä–¥–µ—Ç–æ $H$ –µ cross-entropy:\n",
    "\n",
    "$$H = -\\frac{1}{N}\\sum_{i=1}^{N} \\log_2 P(w_i | w_1, ..., w_{i-1})$$\n",
    "\n",
    "### Cross-Entropy\n",
    "\n",
    "**Cross-entropy** –∏–∑–º–µ—Ä–≤–∞ —Å—Ä–µ–¥–Ω–æ—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–≤ –±–∏—Ç–æ–≤–µ), –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞ –∫–æ–¥–∏—Ä–∞–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏—Ç–µ.\n",
    "\n",
    "**–í—Ä—ä–∑–∫–∞ —Å —Ç–µ–æ—Ä–∏—è—Ç–∞ –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è—Ç–∞:**\n",
    "- –ü–æ-–¥–æ–±—ä—Ä –º–æ–¥–µ–ª ‚Üí –ø–æ-–º–∞–ª–∫–æ –∏–∑–Ω–µ–Ω–∞–¥–∞ ‚Üí –ø–æ-–Ω–∏—Å–∫–∞ cross-entropy\n",
    "- Cross-entropy –µ –º–∏–Ω–∏–º–∞–ª–Ω–∞ –∫–æ–≥–∞—Ç–æ $P$ —Å—ä–≤–ø–∞–¥–∞ —Å –∏—Å—Ç–∏–Ω—Å–∫–æ—Ç–æ —Ä–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n",
    "\n",
    "### –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –Ω–∞ Perplexity\n",
    "\n",
    "**–ü–æ-–Ω–∏—Å–∫–∞ perplexity = –ø–æ-–¥–æ–±—ä—Ä –º–æ–¥–µ–ª**\n",
    "\n",
    "Perplexity –º–æ–∂–µ –¥–∞ —Å–µ —Ç—ä–ª–∫—É–≤–∞ –∫–∞—Ç–æ **–µ—Ñ–µ–∫—Ç–∏–≤–µ–Ω branching factor**:\n",
    "- PP = 100 –æ–∑–Ω–∞—á–∞–≤–∞, —á–µ –º–æ–¥–µ–ª–∞ –µ \"–æ–±—ä—Ä–∫–∞–Ω\" —Å—è–∫–∞—à –∏–º–∞ 100 —Ä–∞–≤–Ω–æ–≤–µ—Ä–æ—è—Ç–Ω–∏ –∏–∑–±–æ—Ä–∞ –ø—Ä–∏ –≤—Å—è–∫–∞ —Å—Ç—ä–ø–∫–∞\n",
    "- PP = 10 –µ –ø–æ-–¥–æ–±—Ä–µ\n",
    "\n",
    "**–¢–∏–ø–∏—á–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏:**\n",
    "- Bigram –º–æ–¥–µ–ª –Ω–∞ Penn Treebank: ~150-200\n",
    "- Trigram –º–æ–¥–µ–ª: ~100-150  \n",
    "- –°—ä–≤—Ä–µ–º–µ–Ω–Ω–∏ –Ω–µ–≤—Ä–æ–Ω–Ω–∏ LM: ~20-50\n",
    "- –ù–∞–π-–¥–æ–±—Ä–∏—Ç–µ LLM: ~5-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –∑–∞ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ perplexity\n",
    "def compute_perplexity(model, test_corpus):\n",
    "    \"\"\"\n",
    "    –ò–∑—á–∏—Å–ª—è–≤–∞ perplexity –Ω–∞ –º–æ–¥–µ–ª –Ω–∞ —Ç–µ—Å—Ç–æ–≤ –∫–æ—Ä–ø—É—Å\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0\n",
    "    word_count = 0\n",
    "    \n",
    "    for sentence in test_corpus:\n",
    "        words = ['<START>'] + sentence.split() + ['<END>']\n",
    "        \n",
    "        for i in range(len(words) - 1):\n",
    "            w1, w2 = words[i], words[i+1]\n",
    "            prob = model.get_probability(w1, w2)\n",
    "            \n",
    "            # –ò–∑–±—è–≥–≤–∞–º–µ log(0)\n",
    "            if prob > 0:\n",
    "                log_prob_sum += np.log2(prob)\n",
    "            else:\n",
    "                # Penalize heavily for unseen bigrams\n",
    "                log_prob_sum += np.log2(1e-10)\n",
    "            \n",
    "            word_count += 1\n",
    "    \n",
    "    # Cross-entropy\n",
    "    cross_entropy = -log_prob_sum / word_count\n",
    "    \n",
    "    # Perplexity\n",
    "    perplexity = 2 ** cross_entropy\n",
    "    \n",
    "    return perplexity, cross_entropy\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤ –∫–æ—Ä–ø—É—Å (–Ω–æ–≤–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è)\n",
    "test_corpus = [\n",
    "    \"–∫–æ—Ç–∫–∞—Ç–∞ –≥–æ–Ω–∏ –º–∏—à–∫–∞—Ç–∞\",\n",
    "    \"–∫—É—á–µ—Ç–æ —Å–ø–∏ –Ω–∞ –¥–∏–≤–∞–Ω–∞\",\n",
    "    \"–ø—Ç–∏—Ü–∞—Ç–∞ –ª–µ—Ç–∏ –≤–∏—Å–æ–∫–æ\"\n",
    "]\n",
    "\n",
    "print(\"üìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ Bigram –º–æ–¥–µ–ª–∞\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Perplexity –Ω–∞ –æ–±—É—á–∏—Ç–µ–ª–Ω–∏—è –∫–æ—Ä–ø—É—Å\n",
    "train_pp, train_ce = compute_perplexity(lm, corpus)\n",
    "print(f\"–¢—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–µ–Ω –Ω–∞–±–æ—Ä:\")\n",
    "print(f\"  Cross-entropy: {train_ce:.3f} bits\")\n",
    "print(f\"  Perplexity: {train_pp:.3f}\")\n",
    "\n",
    "# Perplexity –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—è –∫–æ—Ä–ø—É—Å\n",
    "test_pp, test_ce = compute_perplexity(lm, test_corpus)\n",
    "print(f\"\\n–¢–µ—Å—Ç–æ–≤ –Ω–∞–±–æ—Ä:\")\n",
    "print(f\"  Cross-entropy: {test_ce:.3f} bits\")\n",
    "print(f\"  Perplexity: {test_pp:.3f}\")\n",
    "\n",
    "print(\"\\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(f\"   –ú–æ–¥–µ–ª—ä—Ç –µ '–æ–±—ä—Ä–∫–∞–Ω' —Å—è–∫–∞—à –∏–º–∞ ~{test_pp:.0f} —Ä–∞–≤–Ω–æ–≤–µ—Ä–æ—è—Ç–Ω–∏ –∏–∑–±–æ—Ä–∞\")\n",
    "print(f\"   –ø—Ä–∏ –≤—Å—è–∫–∞ —Å—Ç—ä–ø–∫–∞ (–∑–∞ —Ç–µ—Å—Ç–æ–≤–∏—è –Ω–∞–±–æ—Ä).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞—â–æ Perplexity –µ –≤–∞–∂–Ω–∞?\n",
    "\n",
    "**Perplexity –µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ –∑–∞ –í–°–ò–ß–ö–ò –µ–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏:**\n",
    "- N-gram –º–æ–¥–µ–ª–∏ ‚úì\n",
    "- RNN –µ–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏ ‚úì (–õ–µ–∫—Ü–∏—è 4)\n",
    "- Transformer –º–æ–¥–µ–ª–∏ ‚úì (–õ–µ–∫—Ü–∏—è 5)\n",
    "- GPT, BERT –∏ —Å—ä–≤—Ä–µ–º–µ–Ω–Ω–∏ LLM ‚úì (–õ–µ–∫—Ü–∏—è 6-8)\n",
    "\n",
    "**–í—Ä—ä–∑–∫–∞ —Å—ä—Å —Å–ª–µ–¥–≤–∞—â–∏—Ç–µ –ª–µ–∫—Ü–∏–∏:**\n",
    "- –©–µ —è –∏–∑–ø–æ–ª–∑–≤–∞–º–µ –∑–∞ –¥–∞ —Å—Ä–∞–≤–Ω—è–≤–∞–º–µ —Ä–∞–∑–ª–∏—á–Ω–∏ –º–æ–¥–µ–ª–∏\n",
    "- –©–µ –≤–∏–¥–∏–º –∫–∞–∫ –Ω–µ–≤—Ä–æ–Ω–Ω–∏—Ç–µ LM –¥—Ä–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥–æ–±—Ä—è–≤–∞—Ç perplexity\n",
    "- –©–µ —Ä–∞–∑–±–µ—Ä–µ–º scaling laws: –∫–∞–∫ perplexity –Ω–∞–º–∞–ª—è–≤–∞ —Å —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞ –º–æ–¥–µ–ª–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. –î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∞\n",
    "\n",
    "### –î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–∞—Ç–∞ —Ö–∏–ø–æ—Ç–µ–∑–∞\n",
    "\n",
    "> **\"You shall know a word by the company it keeps\"**  \n",
    "> ‚Äî J.R. Firth (1957)\n",
    "\n",
    "**–¶–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –∏–¥–µ—è:**\n",
    "- –î—É–º–∏, –∫–æ–∏—Ç–æ —Å–µ –ø–æ—è–≤—è–≤–∞—Ç –≤ –ø–æ–¥–æ–±–Ω–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏, –∏–º–∞—Ç –ø–æ–¥–æ–±–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "- \"–ö—É—á–µ—Ç–æ –≥–æ–Ω–∏ –∫–æ—Ç–∫–∞—Ç–∞\" vs \"–ö—É—á–µ—Ç–æ –≥–æ–Ω–∏ –º–∏—à–∫–∞—Ç–∞\"\n",
    "- \"–∫–æ—Ç–∫–∞—Ç–∞\" –∏ \"–º–∏—à–∫–∞—Ç–∞\" –∏–º–∞—Ç –ø–æ–¥–æ–±–Ω–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏ ‚Üí –ø–æ–¥–æ–±–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "\n",
    "### Co-occurrence –º–∞—Ç—Ä–∏—Ü–∏\n",
    "\n",
    "**–î–≤–µ –æ—Å–Ω–æ–≤–Ω–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞:**\n",
    "\n",
    "**1. Word-word –º–∞—Ç—Ä–∏—Ü–∞**\n",
    "- –ë—Ä–æ–∏–º —Å—ä—Å–µ–¥–Ω–∏ –¥—É–º–∏ –≤ –ø—Ä–æ–∑–æ—Ä–µ—Ü (–Ω–∞–ø—Ä. ¬±2 –¥—É–º–∏)\n",
    "- –†–µ–¥–æ–≤–µ –∏ –∫–æ–ª–æ–Ω–∏ = –¥—É–º–∏\n",
    "- –°—Ç–æ–π–Ω–æ—Å—Ç = –∫–æ–ª–∫–æ –ø—ä—Ç–∏ –¥—É–º–∏—Ç–µ —Å–µ —Å—Ä–µ—â–∞—Ç –∑–∞–µ–¥–Ω–æ\n",
    "\n",
    "**2. Word-document –º–∞—Ç—Ä–∏—Ü–∞**\n",
    "- –†–µ–¥–æ–≤–µ = –¥—É–º–∏\n",
    "- –ö–æ–ª–æ–Ω–∏ = –¥–æ–∫—É–º–µ–Ω—Ç–∏\n",
    "- –°—Ç–æ–π–Ω–æ—Å—Ç = –∫–æ–ª–∫–æ –ø—ä—Ç–∏ –¥—É–º–∞ —Å–µ –ø–æ—è–≤—è–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç\n",
    "- –¢–æ–≤–∞ –µ –≤—Å—ä—â–Ω–æ—Å—Ç Bag-of-Words –æ—Ç –õ–µ–∫—Ü–∏—è 1!\n",
    "\n",
    "### –û—Ç –±—Ä–æ—è—á–∏ –∫—ä–º –≤–µ–∫—Ç–æ—Ä–∏\n",
    "\n",
    "–í—Å—è–∫–∞ –¥—É–º–∞ —Å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤—è –∫–∞—Ç–æ:\n",
    "- **–í–µ–∫—Ç–æ—Ä** –æ—Ç context counts\n",
    "- **–†–µ–¥** –≤ co-occurrence –º–∞—Ç—Ä–∏—Ü–∞—Ç–∞\n",
    "\n",
    "### –ò–∑–º–µ—Ä–≤–∞–Ω–µ –Ω–∞ –ø—Ä–∏–ª–∏–∫–∞\n",
    "\n",
    "**Cosine Similarity:**\n",
    "\n",
    "$$\\text{sim}(\\mathbf{v}, \\mathbf{w}) = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{||\\mathbf{v}|| \\cdot ||\\mathbf{w}||} = \\frac{\\sum_{i} v_i w_i}{\\sqrt{\\sum_i v_i^2} \\cdot \\sqrt{\\sum_i w_i^2}}$$\n",
    "\n",
    "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
    "- –ò–∑–º–µ—Ä–≤–∞ —ä–≥—ä–ª–∞ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∏ (–Ω–µ –¥—ä–ª–∂–∏–Ω–∞—Ç–∞)\n",
    "- –°—Ç–æ–π–Ω–æ—Å—Ç –º–µ–∂–¥—É -1 –∏ 1\n",
    "- 1 = –∏–¥–µ–Ω—Ç–∏—á–Ω–∏ –ø–æ—Å–æ–∫–∏\n",
    "- 0 = –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–Ω–∏ (–Ω–µ–∑–∞–≤–∏—Å–∏–º–∏)\n",
    "- -1 = –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ù–µ–∫–∞ –¥–∞ –ø–æ—Å—Ç—Ä–æ–∏–º Co-occurrence –º–∞—Ç—Ä–∏—Ü–∞!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ-–±–æ–≥–∞—Ç –∫–æ—Ä–ø—É—Å –∑–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
    "extended_corpus = [\n",
    "    \"–∫–æ—Ç–∫–∞—Ç–∞ –≥–æ–Ω–∏ –º–∏—à–∫–∞—Ç–∞ –≤ –≥—Ä–∞–¥–∏–Ω–∞—Ç–∞\",\n",
    "    \"–∫—É—á–µ—Ç–æ –≥–æ–Ω–∏ –∫–æ—Ç–∫–∞—Ç–∞ –≤ –ø–∞—Ä–∫–∞\",\n",
    "    \"–∫–æ—Ç–∫–∞—Ç–∞ —Å–ø–∏ –Ω–∞ –¥–∏–≤–∞–Ω–∞ –≤–∫—ä—â–∏\",\n",
    "    \"–∫—É—á–µ—Ç–æ —Å–ø–∏ –Ω–∞ –∫–∏–ª–∏–º–∞ –≤–∫—ä—â–∏\",\n",
    "    \"–º–∏—à–∫–∞—Ç–∞ —è–¥–µ —Å–∏—Ä–µ–Ω–µ –≤ –∫—É—Ö–Ω—è—Ç–∞\",\n",
    "    \"–∫–æ—Ç–∫–∞—Ç–∞ —è–¥–µ —Ä–∏–±–∞ –≤ –∫—É—Ö–Ω—è—Ç–∞\",\n",
    "    \"–∫—É—á–µ—Ç–æ —è–¥–µ –º–µ—Å–æ –≤ –≥—Ä–∞–¥–∏–Ω–∞—Ç–∞\",\n",
    "    \"–ø—Ç–∏—Ü–∞—Ç–∞ –ª–µ—Ç–∏ –≤–∏—Å–æ–∫–æ –≤ –Ω–µ–±–µ—Ç–æ\",\n",
    "    \"–∫–æ—Ç–∫–∞—Ç–∞ –≥–æ–Ω–∏ –ø—Ç–∏—Ü–∞—Ç–∞ –≤ –ø–∞—Ä–∫–∞\",\n",
    "    \"–ø—Ç–∏—Ü–∞—Ç–∞ –ø–µ–µ —Å—É—Ç—Ä–∏–Ω –≤ –¥—ä—Ä–≤–æ—Ç–æ\",\n",
    "    \"–∫—É—á–µ—Ç–æ –ª–∞–µ —Å–∏–ª–Ω–æ –Ω–∞–≤—ä–Ω\",\n",
    "    \"–∫–æ—Ç–∫–∞—Ç–∞ –º—è—É–∫–∞ —Ç–∏—Ö–æ –≤–∫—ä—â–∏\"\n",
    "]\n",
    "\n",
    "print(\"üìö –†–∞–∑—à–∏—Ä–µ–Ω –∫–æ—Ä–ø—É—Å:\")\n",
    "for i, sent in enumerate(extended_corpus, 1):\n",
    "    print(f\"{i:2d}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(corpus, window_size=2):\n",
    "    \"\"\"\n",
    "    –ü–æ—Å—Ç—Ä–æ—è–≤–∞ co-occurrence –º–∞—Ç—Ä–∏—Ü–∞ –æ—Ç –∫–æ—Ä–ø—É—Å\n",
    "    \n",
    "    Args:\n",
    "        corpus: —Å–ø–∏—Å—ä–∫ –æ—Ç –∏–∑—Ä–µ—á–µ–Ω–∏—è\n",
    "        window_size: —Ä–∞–∑–º–µ—Ä –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏—è –ø—Ä–æ–∑–æ—Ä–µ—Ü\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame —Å co-occurrence –º–∞—Ç—Ä–∏—Ü–∞\n",
    "    \"\"\"\n",
    "    # –°—ä–±–∏—Ä–∞–º–µ –≤—Å–∏—á–∫–∏ –¥—É–º–∏\n",
    "    all_words = []\n",
    "    for sent in corpus:\n",
    "        all_words.extend(sent.split())\n",
    "    \n",
    "    # –†–µ—á–Ω–∏–∫ –∏ –∏–Ω–¥–µ–∫—Å–∏\n",
    "    vocab = sorted(set(all_words))\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–º–µ –º–∞—Ç—Ä–∏—Ü–∞—Ç–∞\n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    \n",
    "    # –ü–æ–ø—ä–ª–≤–∞–º–µ –º–∞—Ç—Ä–∏—Ü–∞—Ç–∞\n",
    "    for sent in corpus:\n",
    "        words = sent.split()\n",
    "        for i, target_word in enumerate(words):\n",
    "            target_idx = word_to_idx[target_word]\n",
    "            \n",
    "            # –ì–ª–µ–¥–∞–º–µ –≤ –ø—Ä–æ–∑–æ—Ä–µ—Ü–∞ –æ–∫–æ–ª–æ –¥—É–º–∞—Ç–∞\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(words), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j:  # –ù–µ –±—Ä–æ–∏–º –¥—É–º–∞—Ç–∞ —Å—ä—Å —Å–µ–±–µ —Å–∏\n",
    "                    context_word = words[j]\n",
    "                    context_idx = word_to_idx[context_word]\n",
    "                    matrix[target_idx, context_idx] += 1\n",
    "    \n",
    "    return pd.DataFrame(matrix, index=vocab, columns=vocab)\n",
    "\n",
    "# –ü–æ—Å—Ç—Ä–æ—è–≤–∞–º–µ –º–∞—Ç—Ä–∏—Ü–∞—Ç–∞\n",
    "cooccur_matrix = build_cooccurrence_matrix(extended_corpus, window_size=2)\n",
    "\n",
    "print(\"\\nüìä Co-occurrence –º–∞—Ç—Ä–∏—Ü–∞ (—á–∞—Å—Ç–∏—á–Ω–∞)\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# –ü–æ–∫–∞–∑–≤–∞–º–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∏ –¥—É–º–∏\n",
    "interesting_words = ['–∫–æ—Ç–∫–∞—Ç–∞', '–∫—É—á–µ—Ç–æ', '–º–∏—à–∫–∞—Ç–∞', '–ø—Ç–∏—Ü–∞—Ç–∞', '–≥–æ–Ω–∏', '—è–¥–µ', '—Å–ø–∏']\n",
    "subset = cooccur_matrix.loc[interesting_words, interesting_words]\n",
    "print(subset.to_string())\n",
    "\n",
    "print(f\"\\n–ü—ä–ª–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞: {cooccur_matrix.shape[0]} x {cooccur_matrix.shape[1]}\")\n",
    "print(f\"–ë—Ä–æ–π –Ω–µ–Ω—É–ª–µ–≤–∏ –µ–ª–µ–º–µ–Ω—Ç–∏: {np.count_nonzero(cooccur_matrix.values)}\")\n",
    "sparsity = 1 - np.count_nonzero(cooccur_matrix.values) / cooccur_matrix.size\n",
    "print(f\"–†–∞–∑—Ä–µ–¥–µ–Ω–æ—Å—Ç: {sparsity:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞—Ç–∞\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(subset, annot=True, fmt='.0f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Co-occurrence count'},\n",
    "            square=True)\n",
    "plt.title('Co-occurrence –º–∞—Ç—Ä–∏—Ü–∞ (–∏–∑–±—Ä–∞–Ω–∏ –¥—É–º–∏)', fontsize=14, pad=15)\n",
    "plt.xlabel('–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞ –¥—É–º–∞', fontsize=12)\n",
    "plt.ylabel('–¶–µ–ª–µ–≤–∞ –¥—É–º–∞', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"   –ü–æ-—Ç—ä–º–Ω–∏—Ç–µ –∫–ª–µ—Ç–∫–∏ –ø–æ–∫–∞–∑–≤–∞—Ç –ø–æ-—á–µ—Å—Ç–æ —Å—Ä–µ—â–∞–Ω–∏ —Å—ä–≤–º–µ—Å—Ç–Ω–∏ –ø–æ—è–≤–∏.\")\n",
    "print(\"   –ó–∞–±–µ–ª–µ–∂–µ—Ç–µ, —á–µ '–∫–æ—Ç–∫–∞—Ç–∞', '–∫—É—á–µ—Ç–æ' –∏ '–º–∏—à–∫–∞—Ç–∞' –∏–º–∞—Ç –ø–æ–¥–æ–±–Ω–∏ –º–æ–¥–µ–ª–∏!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞–º–∏—Ä–∞–Ω–µ –Ω–∞ –Ω–∞–π-—Å—Ö–æ–¥–Ω–∏ –¥—É–º–∏ —Å cosine similarity\n",
    "def find_most_similar(word, matrix, top_n=5):\n",
    "    \"\"\"\n",
    "    –ù–∞–º–∏—Ä–∞ –Ω–∞–π-—Å—Ö–æ–¥–Ω–∏—Ç–µ –¥—É–º–∏ –Ω–∞ –¥–∞–¥–µ–Ω–∞ –¥—É–º–∞\n",
    "    \"\"\"\n",
    "    if word not in matrix.index:\n",
    "        return []\n",
    "    \n",
    "    # –í–µ–∫—Ç–æ—Ä –Ω–∞ –¥—É–º–∞—Ç–∞\n",
    "    word_vector = matrix.loc[word].values.reshape(1, -1)\n",
    "    \n",
    "    # –ò–∑—á–∏—Å–ª—è–≤–∞–º–µ cosine similarity —Å –≤—Å–∏—á–∫–∏ –¥—É–º–∏\n",
    "    similarities = cosine_similarity(word_vector, matrix.values)[0]\n",
    "    \n",
    "    # –°—ä–∑–¥–∞–≤–∞–º–µ —Å–ø–∏—Å—ä–∫ —Å (–¥—É–º–∞, —Å—Ö–æ–¥—Å—Ç–≤–æ)\n",
    "    word_similarities = list(zip(matrix.index, similarities))\n",
    "    \n",
    "    # –°–æ—Ä—Ç–∏—Ä–∞–º–µ –∏ –≤–∑–µ–º–∞–º–µ —Ç–æ–ø N (–±–µ–∑ —Å–∞–º–∞—Ç–∞ –¥—É–º–∞)\n",
    "    word_similarities = [(w, s) for w, s in word_similarities if w != word]\n",
    "    word_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return word_similarities[:top_n]\n",
    "\n",
    "print(\"üîç –ù–∞–π-—Å—Ö–æ–¥–Ω–∏ –¥—É–º–∏ (cosine similarity)\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_words = ['–∫–æ—Ç–∫–∞—Ç–∞', '–∫—É—á–µ—Ç–æ', '–≥–æ–Ω–∏', '–≤–∫—ä—â–∏']\n",
    "\n",
    "for word in test_words:\n",
    "    similar = find_most_similar(word, cooccur_matrix, top_n=5)\n",
    "    print(f\"\\n'{word}' –µ –Ω–∞–π-—Å—Ö–æ–¥–Ω–∞ —Å:\")\n",
    "    for similar_word, similarity in similar:\n",
    "        bar = '‚ñà' * int(similarity * 30)\n",
    "        print(f\"  {similar_word:12s} {similarity:.3f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–±–ª–µ–º–∏ —Å Co-occurrence –º–∞—Ç—Ä–∏—Ü–∏\n",
    "\n",
    "**1. –ú–Ω–æ–≥–æ –≤–∏—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–∏**\n",
    "- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç = —Ä–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞ (10,000+ –¥—É–º–∏)\n",
    "- –ú–∞—Ç—Ä–∏—Ü–∞ 10,000 x 10,000 = 100 –º–∏–ª–∏–æ–Ω–∞ –µ–ª–µ–º–µ–Ω—Ç–∞\n",
    "\n",
    "**2. –†–∞–∑—Ä–µ–¥–µ–Ω–∏ (Sparse)**\n",
    "- –ü–æ–≤–µ—á–µ—Ç–æ –µ–ª–µ–º–µ–Ω—Ç–∏ —Å–∞ 0\n",
    "- –ù–µ–µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∑–∞ —Å—ä—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∏–∑—á–∏—Å–ª–µ–Ω–∏—è\n",
    "\n",
    "**3. –ù—è–º–∞—Ç —è—Å–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ**\n",
    "- –ö–∞–∫–≤–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–≤–∞—Ç –∏–∑–º–µ—Ä–µ–Ω–∏—è—Ç–∞?\n",
    "- –¢—Ä—É–¥–Ω–æ –∑–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–∞–Ω–µ\n",
    "\n",
    "**4. –ò–∑—á–∏—Å–ª–∏—Ç–µ–ª–Ω–æ —Å–∫—ä–ø–∏**\n",
    "- –°—Ç—Ä–æ–µ–Ω–µ –Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞—Ç–∞ –∏–∑–∏—Å–∫–≤–∞ —Å–∫–∞–Ω–∏—Ä–∞–Ω–µ –Ω–∞ —Ü–µ–ª–∏—è –∫–æ—Ä–ø—É—Å\n",
    "- –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –ø—Ä–∏–ª–∏–∫–∏ –∏–∑–∏—Å–∫–≤–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –≤–∏—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ:** Dense embeddings (Word2Vec) ‚Üí —Å–ª–µ–¥–≤–∞—â–∞ —Å–µ–∫—Ü–∏—è!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Word Embeddings: Word2Vec\n",
    "\n",
    "### Dense vs Sparse –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è\n",
    "\n",
    "**Sparse (—Ä–∞–∑—Ä–µ–¥–µ–Ω–∏):**\n",
    "- One-hot vectors: [0, 0, 0, 1, 0, ..., 0]\n",
    "- Co-occurrence vectors: [0, 3, 0, 0, 1, 0, ..., 5]\n",
    "- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç = —Ä–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞ (10,000+)\n",
    "- –ü–æ–≤–µ—á–µ—Ç–æ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ —Å–∞ 0\n",
    "\n",
    "**Dense (–ø–ª—ä—Ç–Ω–∏):**\n",
    "- Continuous vectors: [0.23, -0.45, 0.67, ..., 0.12]\n",
    "- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç: 50-300 (–º–Ω–æ–≥–æ –ø–æ-–º–∞–ª–∫–∞!)\n",
    "- –í—Å–∏—á–∫–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ —Å–∞ –Ω–µ–Ω—É–ª–µ–≤–∏\n",
    "\n",
    "**–ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞ –Ω–∞ dense:**\n",
    "- ‚úÖ **–ï—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç:** –ü–æ-–º–∞–ª–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏, –ø–æ-–±—ä—Ä–∑–∏ –∏–∑—á–∏—Å–ª–µ–Ω–∏—è\n",
    "- ‚úÖ **–û–±–æ–±—â–µ–Ω–∏–µ:** –°—Ö–æ–¥–Ω–∏ –¥—É–º–∏ –∏–º–∞—Ç —Å—Ö–æ–¥–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏\n",
    "- ‚úÖ **–ù–∞—É—á–µ–Ω–∏ –æ—Ç –¥–∞–Ω–Ω–∏:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –æ—Ç–∫—Ä–∏–≤–∞—Ç –≤–∞–∂–Ω–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏\n",
    "\n",
    "### Word2Vec: –ú–æ—Ç–∏–≤–∞—Ü–∏—è\n",
    "\n",
    "**–¶–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –∏–¥–µ—è:**\n",
    "- –ù–∞—É—á–∏ embeddings, –∫–æ–∏—Ç–æ —É–ª–∞–≤—è—Ç —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏ –≤—Ä—ä–∑–∫–∏\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞ –¥–∞ –ø—Ä–µ–¥–≤–∏–¥–∏—à –¥—É–º–∞—Ç–∞ (–∏–ª–∏ –æ–±—Ä–∞—Ç–Ω–æ)\n",
    "- **\"You shall know a word by the company it keeps\"** - –Ω–æ –Ω–∞—É—á–µ–Ω–æ —Å –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞!\n",
    "\n",
    "**–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –æ—Ç:** –¢–æ–º–∞—à –ú–∏–∫–æ–ª–æ–≤ –∏ –µ–∫–∏–ø –≤ Google (2013)\n",
    "\n",
    "### –î–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∏\n",
    "\n",
    "#### 1. Skip-gram\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥–≤–∏–¥–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏ –¥—É–º–∏ –æ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –¥—É–º–∞\n",
    "\n",
    "```\n",
    "–í—Ö–æ–¥:  [—Ü–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –¥—É–º–∞]\n",
    "–ò–∑—Ö–æ–¥: [–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏ –¥—É–º–∏]\n",
    "```\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "- –ò–∑—Ä–µ—á–µ–Ω–∏–µ: \"–∫–æ—Ç–∫–∞—Ç–∞ –≥–æ–Ω–∏ –º–∏—à–∫–∞—Ç–∞ –≤ –≥—Ä–∞–¥–∏–Ω–∞—Ç–∞\"\n",
    "- –¶–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –¥—É–º–∞: \"–º–∏—à–∫–∞—Ç–∞\"\n",
    "- –ö–æ–Ω—Ç–µ–∫—Å—Ç (window=2): [\"–≥–æ–Ω–∏\", \"–≤\", \"–≥—Ä–∞–¥–∏–Ω–∞—Ç–∞\"]\n",
    "\n",
    "**–ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞:**\n",
    "- –î–æ–±—ä—Ä –∑–∞ —Ä–µ–¥–∫–∏ –¥—É–º–∏\n",
    "- –ü–æ-–¥–æ–±—Ä–µ —É–ª–∞–≤—è —Ä–µ–¥–∫–∏ –≤—Ä—ä–∑–∫–∏\n",
    "\n",
    "#### 2. CBOW (Continuous Bag-of-Words)\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥–≤–∏–¥–∏ —Ü–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –¥—É–º–∞ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "\n",
    "```\n",
    "–í—Ö–æ–¥:  [–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏ –¥—É–º–∏]\n",
    "–ò–∑—Ö–æ–¥: [—Ü–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –¥—É–º–∞]\n",
    "```\n",
    "\n",
    "**–ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞:**\n",
    "- –ü–æ-–±—ä—Ä–∑ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ\n",
    "- –ü–æ-–¥–æ–±—ä—Ä –∑–∞ —á–µ—Å—Ç–∏ –¥—É–º–∏\n",
    "\n",
    "### –û–±—É—á–∏—Ç–µ–ª–Ω–∞ —Ü–µ–ª\n",
    "\n",
    "**Skip-gram objective:**\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} | w_t)$$\n",
    "\n",
    "–ö—ä–¥–µ—Ç–æ:\n",
    "- $T$ = –±—Ä–æ–π –¥—É–º–∏ –≤ –∫–æ—Ä–ø—É—Å–∞\n",
    "- $c$ = —Ä–∞–∑–º–µ—Ä –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏—è –ø—Ä–æ–∑–æ—Ä–µ—Ü\n",
    "- $w_t$ = —Ü–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –¥—É–º–∞\n",
    "- $w_{t+j}$ = –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞ –¥—É–º–∞\n",
    "\n",
    "**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ —Å–µ –º–æ–¥–µ–ª–∏—Ä–∞ —Å softmax:**\n",
    "\n",
    "$$P(w_O | w_I) = \\frac{\\exp(\\mathbf{v}_{w_O}^\\top \\mathbf{v}_{w_I})}{\\sum_{w=1}^{V} \\exp(\\mathbf{v}_w^\\top \\mathbf{v}_{w_I})}$$\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º:** Softmax –µ —Å–∫—ä–ø –∑–∞ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ (—Å—É–º–∞ –ø–æ —Ü–µ–ª–∏—è —Ä–µ—á–Ω–∏–∫!)\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ:** Negative sampling - –≤–º–µ—Å—Ç–æ –¥–∞ –∏–∑—á–∏—Å–ª—è–≤–∞–º–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç –∑–∞ –≤—Å–∏—á–∫–∏ –¥—É–º–∏, —Å—Ä–∞–≤–Ω—è–≤–∞–º–µ —Å–∞–º–æ —Å –Ω—è–∫–æ–ª–∫–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏ –ø—Ä–∏–º–µ—Ä–∞.\n",
    "\n",
    "### –ö–∞–∫–≤–æ –Ω–∞—É—á–∞–≤–∞ Word2Vec?\n",
    "\n",
    "**1. –°–µ–º–∞–Ω—Ç–∏—á–Ω–∞ –ø—Ä–∏–ª–∏–∫–∞**\n",
    "- `king` ‚âà `queen`\n",
    "- `dog` ‚âà `puppy`\n",
    "- `car` ‚âà `automobile`\n",
    "\n",
    "**2. –°–∏–Ω—Ç–∞–∫—Ç–∏—á–Ω–∏ –º–æ–¥–µ–ª–∏**\n",
    "- `walking` ‚âà `running` ‚âà `swimming`\n",
    "- –ì–ª–∞–≥–æ–ª–∏ –≤ –µ–¥–Ω–æ –∏ —Å—ä—â–æ –≤—Ä–µ–º–µ\n",
    "\n",
    "**3. –ê–Ω–∞–ª–æ–≥–∏–∏**\n",
    "- `king - man + woman ‚âà queen`\n",
    "- `Paris - France + Italy ‚âà Rome`\n",
    "- `walking - walk + swim ‚âà swimming`\n",
    "\n",
    "**–í–µ–∫—Ç–æ—Ä–Ω–∞ –∞—Ä–∏—Ç–º–µ—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç–∏!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å gensim (–∞–∫–æ –µ –Ω–∞–ª–∏—á–µ–Ω)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–ª–∏ gensim –µ –Ω–∞–ª–∏—á–µ–Ω\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    from gensim.models.word2vec import LineSentence\n",
    "    GENSIM_AVAILABLE = True\n",
    "    print(\"‚úì Gensim –µ –Ω–∞–ª–∏—á–µ–Ω!\")\n",
    "except ImportError:\n",
    "    GENSIM_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Gensim –Ω–µ –µ –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω. –ú–æ–ª—è –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–π—Ç–µ —Å: pip install gensim\")\n",
    "    print(\"   –ó–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —â–µ –ø–æ–∫–∞–∂–µ–º –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª–Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENSIM_AVAILABLE:\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞ –∫–æ—Ä–ø—É—Å –∑–∞ Word2Vec\n",
    "    # Word2Vec –æ—á–∞–∫–≤–∞ —Å–ø–∏—Å—ä–∫ –æ—Ç —Å–ø–∏—Å—ä—Ü–∏ —Å –¥—É–º–∏\n",
    "    w2v_corpus = [sent.split() for sent in extended_corpus]\n",
    "    \n",
    "    print(\"üèãÔ∏è –û–±—É—á–∞–≤–∞–Ω–µ –Ω–∞ Word2Vec –º–æ–¥–µ–ª...\\n\")\n",
    "    \n",
    "    # –û–±—É—á–∞–≤–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª\n",
    "    # vector_size = —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç –Ω–∞ embeddings (–Ω–∞–º–∞–ª–µ–Ω–∞ –∑–∞ –º–∞–ª–∫–∏—è –∫–æ—Ä–ø—É—Å)\n",
    "    # window = —Ä–∞–∑–º–µ—Ä –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏—è –ø—Ä–æ–∑–æ—Ä–µ—Ü\n",
    "    # min_count = –º–∏–Ω–∏–º–∞–ª–µ–Ω –±—Ä–æ–π —Å—Ä–µ—â–∞–Ω–∏—è –Ω–∞ –¥—É–º–∞\n",
    "    # sg = 1 –∑–∞ skip-gram, 0 –∑–∞ CBOW\n",
    "    model = Word2Vec(\n",
    "        sentences=w2v_corpus,\n",
    "        vector_size=50,\n",
    "        window=3,\n",
    "        min_count=1,\n",
    "        sg=1,  # Skip-gram\n",
    "        epochs=100,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì –ú–æ–¥–µ–ª—ä—Ç –µ –æ–±—É—á–µ–Ω!\")\n",
    "    print(f\"\\n–†–µ—á–Ω–∏–∫: {len(model.wv)} –¥—É–º–∏\")\n",
    "    print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç –Ω–∞ embeddings: {model.wv.vector_size}\")\n",
    "    \n",
    "    # –ù–∞–π-—Å—Ö–æ–¥–Ω–∏ –¥—É–º–∏\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\\nüîç –ù–∞–π-—Å—Ö–æ–¥–Ω–∏ –¥—É–º–∏:\\n\")\n",
    "    \n",
    "    test_words = ['–∫–æ—Ç–∫–∞—Ç–∞', '–∫—É—á–µ—Ç–æ', '–≥–æ–Ω–∏', '—Å–ø–∏']\n",
    "    \n",
    "    for word in test_words:\n",
    "        if word in model.wv:\n",
    "            similar = model.wv.most_similar(word, topn=3)\n",
    "            print(f\"'{word}':\")\n",
    "            for similar_word, similarity in similar:\n",
    "                print(f\"  ‚Ä¢ {similar_word}: {similarity:.3f}\")\n",
    "            print()\n",
    "    \n",
    "    # –ê–Ω–∞–ª–æ–≥–∏–∏ (–º–æ–∂–µ –¥–∞ –Ω–µ —Ä–∞–±–æ—Ç—è—Ç –¥–æ–±—Ä–µ –Ω–∞ –º–∞–ª—ä–∫ –∫–æ—Ä–ø—É—Å)\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüßÆ –û–ø–∏—Ç –∑–∞ –∞–Ω–∞–ª–æ–≥–∏–∏ (–º–æ–∂–µ –¥–∞ –Ω–µ —Ä–∞–±–æ—Ç–∏ –ø–µ—Ä—Ñ–µ–∫—Ç–Ω–æ –Ω–∞ –º–∞–ª—ä–∫ –∫–æ—Ä–ø—É—Å):\\n\")\n",
    "    \n",
    "    try:\n",
    "        # –∫–æ—Ç–∫–∞—Ç–∞ –≥–æ–Ω–∏ X, –∫—É—á–µ—Ç–æ –≥–æ–Ω–∏ ?\n",
    "        result = model.wv.most_similar(\n",
    "            positive=['–∫—É—á–µ—Ç–æ', '–º–∏—à–∫–∞—Ç–∞'],\n",
    "            negative=['–∫–æ—Ç–∫–∞—Ç–∞'],\n",
    "            topn=1\n",
    "        )\n",
    "        print(f\"–∫–æ—Ç–∫–∞—Ç–∞:–º–∏—à–∫–∞—Ç–∞ :: –∫—É—á–µ—Ç–æ:?\")\n",
    "        print(f\"  ‚Üí {result[0][0]} (similarity: {result[0][1]:.3f})\")\n",
    "    except:\n",
    "        print(\"  –ù–µ–¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –¥–∞–Ω–Ω–∏ –∑–∞ –∞–Ω–∞–ª–æ–≥–∏–∏ –Ω–∞ —Ç–æ–∑–∏ –º–∞–ª—ä–∫ –∫–æ—Ä–ø—É—Å\")\n",
    "    \n",
    "    print(\"\\nüí° –ó–∞–±–µ–ª–µ–∂–∫–∞: –ó–∞ –ø–æ-–¥–æ–±—Ä–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ —Å–µ –Ω—É–∂–¥–∞–µ–º –æ—Ç –ø–æ-–≥–æ–ª—è–º –∫–æ—Ä–ø—É—Å!\")\n",
    "    print(\"   –¢–∏–ø–∏—á–Ω–æ Word2Vec —Å–µ –æ–±—É—á–∞–≤–∞ –Ω–∞ –º–∏–ª–∏–æ–Ω–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ç–∞ –Ω–∞ Word2Vec –µ –ø—Ä–æ–ø—É—Å–Ω–∞—Ç–∞ (gensim –Ω–µ –µ –Ω–∞–ª–∏—á–µ–Ω)\")\n",
    "    print(\"\\n–ó–∞ –¥–∞ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–∞—Ç–µ —Å Word2Vec:\")\n",
    "    print(\"  1. –ò–Ω—Å—Ç–∞–ª–∏—Ä–∞–π—Ç–µ gensim: pip install gensim\")\n",
    "    print(\"  2. –ò–∑–ø—ä–ª–Ω–µ—Ç–µ –æ—Ç–Ω–æ–≤–æ —Ç–∞–∑–∏ –∫–ª–µ—Ç–∫–∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª–Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª–Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–¥–æ—Ä–∏ –±–µ–∑ gensim)\n",
    "# –°–∏–º—É–ª–∏—Ä–∞–º–µ 2D –ø—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ word embeddings\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# –ì—Ä—É–ø–∏ –¥—É–º–∏\n",
    "animals = ['–∫–æ—Ç–∫–∞—Ç–∞', '–∫—É—á–µ—Ç–æ', '–º–∏—à–∫–∞—Ç–∞', '–ø—Ç–∏—Ü–∞—Ç–∞']\n",
    "actions = ['–≥–æ–Ω–∏', '—Å–ø–∏', '—è–¥–µ', '–ª–µ—Ç–∏', '–ª–∞–µ', '–º—è—É–∫–∞', '–ø–µ–µ']\n",
    "places = ['–≥—Ä–∞–¥–∏–Ω–∞—Ç–∞', '–ø–∞—Ä–∫–∞', '–≤–∫—ä—â–∏', '–∫—É—Ö–Ω—è—Ç–∞', '–¥–∏–≤–∞–Ω–∞', '–∫–∏–ª–∏–º–∞', '–¥—ä—Ä–≤–æ—Ç–æ']\n",
    "\n",
    "# –°–∏–º—É–ª–∏—Ä–∞–º–µ –ø–æ–∑–∏—Ü–∏–∏ (–≤ —Ä–µ–∞–ª–Ω–æ—Å—Ç—Ç–∞ –∏–¥–≤–∞—Ç –æ—Ç t-SNE –Ω–∞ embeddings)\n",
    "# –ñ–∏–≤–æ—Ç–Ω–∏ —Å–∞ –±–ª–∏–∑–æ –µ–¥–Ω–∏ –¥–æ –¥—Ä—É–≥–∏\n",
    "animal_positions = np.random.randn(len(animals), 2) * 0.3 + np.array([0, 2])\n",
    "\n",
    "# –î–µ–π—Å—Ç–≤–∏—è —Å–∞ –≤ –¥—Ä—É–≥ –∫–ª—ä—Å—Ç–µ—Ä\n",
    "action_positions = np.random.randn(len(actions), 2) * 0.3 + np.array([2, 0])\n",
    "\n",
    "# –ú–µ—Å—Ç–∞ —Å–∞ –≤ —Ç—Ä–µ—Ç–∏ –∫–ª—ä—Å—Ç–µ—Ä\n",
    "place_positions = np.random.randn(len(places), 2) * 0.3 + np.array([-1.5, -1])\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# –ñ–∏–≤–æ—Ç–Ω–∏\n",
    "plt.scatter(animal_positions[:, 0], animal_positions[:, 1], \n",
    "           s=200, c='red', alpha=0.6, marker='o', label='–ñ–∏–≤–æ—Ç–Ω–∏')\n",
    "for i, word in enumerate(animals):\n",
    "    plt.annotate(word, (animal_positions[i, 0], animal_positions[i, 1]),\n",
    "                fontsize=11, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# –î–µ–π—Å—Ç–≤–∏—è\n",
    "plt.scatter(action_positions[:, 0], action_positions[:, 1], \n",
    "           s=200, c='blue', alpha=0.6, marker='s', label='–î–µ–π—Å—Ç–≤–∏—è')\n",
    "for i, word in enumerate(actions):\n",
    "    plt.annotate(word, (action_positions[i, 0], action_positions[i, 1]),\n",
    "                fontsize=11, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# –ú–µ—Å—Ç–∞\n",
    "plt.scatter(place_positions[:, 0], place_positions[:, 1], \n",
    "           s=200, c='green', alpha=0.6, marker='^', label='–ú–µ—Å—Ç–∞')\n",
    "for i, word in enumerate(places):\n",
    "    plt.annotate(word, (place_positions[i, 0], place_positions[i, 1]),\n",
    "                fontsize=11, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.xlabel('–ò–∑–º–µ—Ä–µ–Ω–∏–µ 1', fontsize=12)\n",
    "plt.ylabel('–ò–∑–º–µ—Ä–µ–Ω–∏–µ 2', fontsize=12)\n",
    "plt.title('–ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª–Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ Word Embeddings\\n(2D –ø—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ)', \n",
    "         fontsize=14, pad=15)\n",
    "plt.legend(fontsize=11, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5, alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"   ‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–Ω–æ —Å—Ö–æ–¥–Ω–∏ –¥—É–º–∏ —Å–µ –≥—Ä—É–ø–∏—Ä–∞—Ç –∑–∞–µ–¥–Ω–æ\")\n",
    "print(\"   ‚Ä¢ –ñ–∏–≤–æ—Ç–Ω–∏—Ç–µ —Ñ–æ—Ä–º–∏—Ä–∞—Ç –∫–ª—ä—Å—Ç–µ—Ä\")\n",
    "print(\"   ‚Ä¢ –î–µ–π—Å—Ç–≤–∏—è—Ç–∞ —Ñ–æ—Ä–º–∏—Ä–∞—Ç –¥—Ä—É–≥ –∫–ª—ä—Å—Ç–µ—Ä\")\n",
    "print(\"   ‚Ä¢ –ú–µ—Å—Ç–∞—Ç–∞ —Ñ–æ—Ä–º–∏—Ä–∞—Ç —Ç—Ä–µ—Ç–∏ –∫–ª—ä—Å—Ç–µ—Ä\")\n",
    "print(\"\\n   –í —Ä–µ–∞–ª–Ω–∏—Ç–µ embeddings —Ç–µ–∑–∏ –∫–ª—ä—Å—Ç–µ—Ä–∏ —Å–µ –ø–æ—è–≤—è–≤–∞—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–∏—Å—Ç—Ä–∞—Å—Ç–∏—è\n",
    "\n",
    "**1. –ï–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä –Ω–∞ –¥—É–º–∞ (–Ω—è–º–∞ –ø–æ–ª–∏—Å–µ–º–∏—è)**\n",
    "- \"–±–∞–Ω–∫–∞\" –∏–º–∞ –µ–¥–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ\n",
    "- –ù–æ –º–æ–∂–µ –¥–∞ –æ–∑–Ω–∞—á–∞–≤–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞ –∏–Ω—Å—Ç–∏—Ç—É—Ü–∏—è –∏–ª–∏ —Ä–µ—á–µ–Ω –±—Ä—è–≥\n",
    "- Word2Vec —É—Å—Ä–µ–¥–Ω—è–≤–∞ –≤—Å–∏—á–∫–∏ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "\n",
    "**2. –£–ª–∞–≤—è –ø—Ä–∏—Å—Ç—Ä–∞—Å—Ç–∏—è –≤ –¥–∞–Ω–Ω–∏—Ç–µ**\n",
    "- –ê–∫–æ –∫–æ—Ä–ø—É—Å—ä—Ç —Å—ä–¥—ä—Ä–∂–∞ —Å—Ç–µ—Ä–µ–æ—Ç–∏–ø–∏, embeddings —â–µ –≥–∏ —É–ª–∞–≤—è—Ç\n",
    "- –ò–∑–≤–µ—Å—Ç–Ω–∏ –ø—Ä–∏–º–µ—Ä–∏:\n",
    "  - `doctor - man + woman ‚âà nurse`\n",
    "  - `programmer - man + woman ‚âà homemaker`\n",
    "- –ï—Ç–∏—á–Ω–∏ –≤—ä–ø—Ä–æ—Å–∏ –∑–∞ NLP —Å–∏—Å—Ç–µ–º–∏!\n",
    "\n",
    "**3. –°—Ç–∞—Ç–∏—á–Ω–∏ (–Ω–µ —Å–µ –ø—Ä–æ–º–µ–Ω—è—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)**\n",
    "- –í–µ–¥–Ω—ä–∂ –æ–±—É—á–µ–Ω–∏, embeddings —Å–∞ —Ñ–∏–∫—Å–∏—Ä–∞–Ω–∏\n",
    "- –ù–µ —Å–µ –∞–¥–∞–ø—Ç–∏—Ä–∞—Ç –∫—ä–º —Ä–∞–∑–ª–∏—á–Ω–∏ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ –∑–∞ 1 –∏ 3:** Contextual embeddings (ELMo, BERT) ‚Üí **–õ–µ–∫—Ü–∏—è 5**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Å—ä–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "\n",
    "### –ò–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ pre-trained embeddings\n",
    "\n",
    "**–ü–æ–ø—É–ª—è—Ä–Ω–∏ pre-trained –º–æ–¥–µ–ª–∏:**\n",
    "\n",
    "**1. Google News Word2Vec**\n",
    "- 3 –º–∏–ª–∏–æ–Ω–∞ –¥—É–º–∏\n",
    "- 300 –∏–∑–º–µ—Ä–µ–Ω–∏—è\n",
    "- –û–±—É—á–µ–Ω –Ω–∞ Google News –∫–æ—Ä–ø—É—Å\n",
    "\n",
    "**2. GloVe (Global Vectors)**\n",
    "- Stanford NLP\n",
    "- –†–∞–∑–ª–∏—á–Ω–∏ –≤–µ—Ä—Å–∏–∏: Wikipedia, Twitter, Common Crawl\n",
    "- 50d, 100d, 200d, 300d\n",
    "\n",
    "**3. FastText**\n",
    "- Facebook AI\n",
    "- –ë–∞–∑–∏—Ä–∞–Ω –Ω–∞ –ø–æ–¥–¥—É–º–∏ (subwords)\n",
    "- –ú–æ–∂–µ –¥–∞ —Ä–∞–±–æ—Ç–∏ —Å Out-Of-Vocabulary (OOV) –¥—É–º–∏!\n",
    "\n",
    "**–ö–æ–≥–∞ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞–º–µ pre-trained?**\n",
    "- ‚úÖ –ú–∞–ª–∫–∏ datasets (transfer learning)\n",
    "- ‚úÖ –û–±—â–∏ —Ç–µ–∫—Å—Ç–æ–≤–µ (–Ω–æ–≤–∏–Ω–∏, —É–µ–±)\n",
    "- ‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏ –∏–∑—á–∏—Å–ª–∏—Ç–µ–ª–Ω–∏ —Ä–µ—Å—É—Ä—Å–∏\n",
    "\n",
    "### –û–±—É—á–∞–≤–∞–Ω–µ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–∏ embeddings\n",
    "\n",
    "**–ö–æ–≥–∞ –¥–∞ –æ–±—É—á–∏–º —Å–≤–æ–∏?**\n",
    "- ‚úÖ –°–ø–µ—Ü–∏—Ñ–∏—á–µ–Ω –¥–æ–º–µ–π–Ω (–º–µ–¥–∏—Ü–∏–Ω–∞, –ø—Ä–∞–≤–æ, —Ñ–∏–Ω–∞–Ω—Å–∏)\n",
    "- ‚úÖ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω —Ä–µ—á–Ω–∏–∫\n",
    "- ‚úÖ –î–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –¥–∞–Ω–Ω–∏ (–º–∏–ª–∏–æ–Ω–∏ –¥—É–º–∏)\n",
    "\n",
    "**–•–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏:**\n",
    "- `vector_size`: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç (50-300)\n",
    "- `window`: –†–∞–∑–º–µ—Ä –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏—è –ø—Ä–æ–∑–æ—Ä–µ—Ü (5-10)\n",
    "- `min_count`: –ú–∏–Ω–∏–º–∞–ª–Ω–∞ —á–µ—Å—Ç–æ—Ç–∞ –Ω–∞ –¥—É–º–∞ (1-5)\n",
    "- `sg`: Skip-gram (1) –∏–ª–∏ CBOW (0)\n",
    "- `epochs`: –ë—Ä–æ–π –µ–ø–æ—Ö–∏ (5-20)\n",
    "\n",
    "### –ò–∑–±–æ—Ä –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ\n",
    "\n",
    "**Bag-of-Words / TF-IDF**\n",
    "- ‚úÖ –ü—Ä–æ—Å—Ç–∏ baseline –º–æ–¥–µ–ª–∏\n",
    "- ‚úÖ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–∏\n",
    "- ‚úÖ –†–∞–±–æ—Ç—è—Ç –∑–∞ –º–∞–ª–∫–∏ datasets\n",
    "- ‚ùå –ù–µ —É–ª–∞–≤—è—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∞\n",
    "\n",
    "**Word2Vec / GloVe**\n",
    "- ‚úÖ –£–ª–∞–≤—è—Ç —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏ –≤—Ä—ä–∑–∫–∏\n",
    "- ‚úÖ Dense –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è\n",
    "- ‚úÖ Transfer learning\n",
    "- ‚ùå –°—Ç–∞—Ç–∏—á–Ω–∏ (–Ω—è–º–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç)\n",
    "- ‚ùå –ü—Ä–æ–±–ª–µ–º —Å –ø–æ–ª–∏—Å–µ–º–∏—è\n",
    "\n",
    "**Contextual Embeddings (BERT, ELMo)**\n",
    "- ‚úÖ –†–∞–∑–ª–∏—á–Ω–∏ embeddings –∑–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏\n",
    "- ‚úÖ State-of-the-art performance\n",
    "- ‚ùå –ü–æ-—Å–ª–æ–∂–Ω–∏ –∏ –±–∞–≤–Ω–∏\n",
    "- ‚ùå –ò–∑–∏—Å–∫–≤–∞—Ç –ø–æ–≤–µ—á–µ —Ä–µ—Å—É—Ä—Å–∏\n",
    "\n",
    "üìå **–©–µ –≤–∏–¥–∏–º contextual embeddings –≤ –õ–µ–∫—Ü–∏—è 5!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ö–∞–∫ –¥–∞ –∑–∞—Ä–µ–¥–∏–º pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ pre-trained embeddings\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if GENSIM_AVAILABLE:\n",
    "    print(\"–ü—Ä–∏–º–µ—Ä –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ Google News Word2Vec:\")\n",
    "    print(\"\"\"\\n# –ò–∑—Ç–µ–≥–ª—è–Ω–µ (–µ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ):\n",
    "# wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# –ó–∞—Ä–µ–∂–¥–∞–Ω–µ:\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin.gz', \n",
    "    binary=True\n",
    ")\n",
    "\n",
    "# –ò–∑–ø–æ–ª–∑–≤–∞–Ω–µ:\n",
    "similar = model.most_similar('king', topn=5)\n",
    "analogy = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\n–ü—Ä–∏–º–µ—Ä –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ GloVe:\")\n",
    "    print(\"\"\"\\n# –ò–∑—Ç–µ–≥–ª—è–Ω–µ –æ—Ç: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "# –ó–∞—Ä–µ–∂–¥–∞–Ω–µ:\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_file = 'glove.6B.100d.txt'\n",
    "word2vec_file = 'glove.6B.100d.word2vec.txt'\n",
    "\n",
    "# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ —Ñ–æ—Ä–º–∞—Ç\n",
    "glove2word2vec(glove_file, word2vec_file)\n",
    "\n",
    "# –ó–∞—Ä–µ–∂–¥–∞–Ω–µ\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_file)\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  –ó–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ pre-trained embeddings –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–π—Ç–µ gensim:\")\n",
    "    print(\"   pip install gensim\")\n",
    "\n",
    "print(\"\\nüí° –°—ä–≤–µ—Ç: Pre-trained embeddings —Å–∞ –æ—Ç–ª–∏—á–µ–Ω –Ω–∞—á–∏–Ω –¥–∞ –∑–∞–ø–æ—á–Ω–µ—Ç–µ!\")\n",
    "print(\"   –í–∏–Ω–∞–≥–∏ –ø—Ä–æ–±–≤–∞–π—Ç–µ –≥–æ—Ç–æ–≤–∏ embeddings –ø—Ä–µ–¥–∏ –¥–∞ –æ–±—É—á–∞–≤–∞—Ç–µ —Å–≤–æ–∏.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. –û–±–æ–±—â–µ–Ω–∏–µ –∏ –º–æ—Å—Ç –∫—ä–º –õ–µ–∫—Ü–∏—è 3\n",
    "\n",
    "### üéì –ö–ª—é—á–æ–≤–∏ –∏–∑–≤–æ–¥–∏\n",
    "\n",
    "#### –î–Ω–µ—Å –Ω–∞—É—á–∏—Ö–º–µ:\n",
    "\n",
    "**1. –¢—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞ –∑–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ**\n",
    "- **–°–∏–º–≤–æ–ª–Ω–∏:** WordNet - –µ–∫—Å–ø–µ—Ä—Ç–Ω–∏ –∑–Ω–∞–Ω–∏—è, –Ω–æ —Å–∫—ä–ø–∏ –∏ –Ω–µ–ø—ä–ª–Ω–∏\n",
    "- **–î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–∏:** Co-occurrence –º–∞—Ç—Ä–∏—Ü–∏ - —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏, –Ω–æ —Ä–∞–∑—Ä–µ–¥–µ–Ω–∏\n",
    "- **–†–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏:** Word2Vec - dense, –Ω–∞—É—á–µ–Ω–∏, —É–ª–∞–≤—è—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∞\n",
    "\n",
    "**2. –ï–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏**\n",
    "- **N-gram –º–æ–¥–µ–ª–∏:** –ü—Ä–µ–¥—Å–∫–∞–∑–≤–∞—Ç –¥—É–º–∏ –±–∞–∑–∏—Ä–∞–Ω–æ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—è\n",
    "- **Perplexity:** –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ –∑–∞ –æ—Ü–µ–Ω–∫–∞\n",
    "- –û—Å–Ω–æ–≤–∞ –∑–∞ —Ä–∞–∑–±–∏—Ä–∞–Ω–µ –Ω–∞ —Å—ä–≤—Ä–µ–º–µ–Ω–Ω–∏ LLM\n",
    "\n",
    "**3. Word2Vec**\n",
    "- Skip-gram vs CBOW\n",
    "- –£–ª–∞–≤—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏ –∏ —Å–∏–Ω—Ç–∞–∫—Ç–∏—á–Ω–∏ –º–æ–¥–µ–ª–∏\n",
    "- –í–µ–∫—Ç–æ—Ä–Ω–∞ –∞—Ä–∏—Ç–º–µ—Ç–∏–∫–∞: `king - man + woman ‚âà queen`\n",
    "- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: –µ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä –Ω–∞ –¥—É–º–∞, —Å—Ç–∞—Ç–∏—á–Ω–∏\n",
    "\n",
    "**4. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Å—ä–æ–±—Ä–∞–∂–µ–Ω–∏—è**\n",
    "- Pre-trained embeddings –∑–∞ transfer learning\n",
    "- –ò–∑–±–æ—Ä –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –∑–∞–≤–∏—Å–∏ –æ—Ç –∑–∞–¥–∞—á–∞—Ç–∞\n",
    "\n",
    "### üîú –°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "**–ö—Ä–∏—Ç–∏—á–Ω–∏ –≤—ä–ø—Ä–æ—Å–∏, –∫–æ–∏—Ç–æ —â–µ –æ—Ç–≥–æ–≤–æ—Ä–∏–º:**\n",
    "\n",
    "‚ùì –ö–∞–∫–≤–æ —Ç–æ—á–Ω–æ –µ \"–¥—É–º–∞\"?  \n",
    "‚ùì –ö–∞–∫ –æ–±—Ä–∞–±–æ—Ç–≤–∞–º–µ \"New York\" –∏–ª–∏ \"don't\"?  \n",
    "‚ùì –ö–∞–∫ —Ä–∞–±–æ—Ç–∏–º —Å Out-Of-Vocabulary –¥—É–º–∏?  \n",
    "‚ùì –ö–∞–∫–≤–æ –µ BPE (Byte Pair Encoding)?  \n",
    "‚ùì –ö–∞–∫ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–∞—Ç GPT, BERT –∏ —Å—ä–≤—Ä–µ–º–µ–Ω–Ω–∏—Ç–µ LLM?  \n",
    "\n",
    "**–ó–∞—â–æ –µ –≤–∞–∂–Ω–æ:**\n",
    "- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è—Ç–∞ –µ –ö–†–ò–¢–ò–ß–ù–ê –∑–∞ –≤—Å–∏—á–∫–∏ —Å—ä–≤—Ä–µ–º–µ–Ω–Ω–∏ LLM\n",
    "- ChatGPT, Claude –∏–∑–ø–æ–ª–∑–≤–∞—Ç subword —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "- –ü—Ä–∞–≤–∏–ª–Ω–∞—Ç–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ–¥–æ–±—Ä—è–≤–∞ performance –¥—Ä–∞—Å—Ç–∏—á–Ω–æ\n",
    "\n",
    "### üìö –ü—Ä–µ–ø–æ—Ä—ä—á–∏—Ç–µ–ª–Ω–æ —á–µ—Ç–µ–Ω–µ\n",
    "\n",
    "**–û—Å–Ω–æ–≤–Ω–∏ —Å—Ç–∞—Ç–∏–∏:**\n",
    "\n",
    "1. **\"Efficient Estimation of Word Representations in Vector Space\"** - Mikolov et al. (2013)\n",
    "   - –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞—Ç–∞ Word2Vec —Å—Ç–∞—Ç–∏—è\n",
    "\n",
    "2. **\"Distributed Representations of Words and Phrases\"** - Mikolov et al. (2013)\n",
    "   - Negative sampling –∏ phrase embeddings\n",
    "\n",
    "3. **\"GloVe: Global Vectors for Word Representation\"** - Pennington et al. (2014)\n",
    "   - –ê–ª—Ç–µ—Ä–Ω–∞—Ç–∏–≤–µ–Ω –º–µ—Ç–æ–¥ –∑–∞ word embeddings\n",
    "\n",
    "**–£—á–µ–±–Ω–∏—Ü–∏:**\n",
    "\n",
    "1. **\"Speech and Language Processing\"** - Jurafsky & Martin (3rd ed.)\n",
    "   - Chapter 3: N-gram Language Models\n",
    "   - Chapter 6: Vector Semantics and Embeddings\n",
    "\n",
    "2. **Stanford CS224N Lecture 2:** Word Vectors\n",
    "\n",
    "### üèãÔ∏è –£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∑–∞ –≤–∫—ä—â–∏\n",
    "\n",
    "**1. N-gram –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏**\n",
    "- –û–±—É—á–µ—Ç–µ trigram –º–æ–¥–µ–ª –Ω–∞ –∫–æ—Ä–ø—É—Å\n",
    "- –°—Ä–∞–≤–Ω–µ—Ç–µ perplexity –Ω–∞ unigram, bigram, trigram\n",
    "- –ì–µ–Ω–µ—Ä–∏—Ä–∞–π—Ç–µ —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–ª–∏—á–Ω–∏ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "**2. Word2Vec –∏–∑—Å–ª–µ–¥–≤–∞–Ω–µ**\n",
    "- –ó–∞—Ä–µ–¥–µ—Ç–µ pre-trained Word2Vec (Google News)\n",
    "- –ù–∞–º–µ—Ä–µ—Ç–µ –Ω–∞–π-—Å—Ö–æ–¥–Ω–∏ –¥—É–º–∏ –∑–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ concepts\n",
    "- –¢–µ—Å—Ç–≤–∞–π—Ç–µ –∞–Ω–∞–ª–æ–≥–∏–∏: `man:woman :: king:?`\n",
    "- –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–π—Ç–µ –∫–∞–∫–≤–∏ –≤—Ä—ä–∑–∫–∏ —É–ª–∞–≤—è\n",
    "\n",
    "**3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è**\n",
    "- –ò–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–π—Ç–µ sentiment classifier —Å:\n",
    "  - Bag-of-words\n",
    "  - TF-IDF\n",
    "  - Averaged Word2Vec embeddings\n",
    "- –°—Ä–∞–≤–Ω–µ—Ç–µ performance –∏ interpretability\n",
    "\n",
    "**4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è**\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞–π—Ç–µ t-SNE –∑–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ Word2Vec\n",
    "- –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–∞–π—Ç–µ –∫–ª—ä—Å—Ç–µ—Ä–∏ –æ—Ç —Å—Ö–æ–¥–Ω–∏ –¥—É–º–∏\n",
    "- –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–π—Ç–µ –∫–∞–∫–≤–∏ semantic regions –æ—Ç–∫—Ä–∏–≤–∞–º–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## –ö—Ä–∞–π –Ω–∞ –õ–µ–∫—Ü–∏—è 2\n",
    "\n",
    "### –ë–ª–∞–≥–æ–¥–∞—Ä—è –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ—Ç–æ! üéâ\n",
    "\n",
    "**–í—ä–ø—Ä–æ—Å–∏? ü§î**\n",
    "\n",
    "---\n",
    "\n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ subword models  \n",
    "**–î–∞—Ç–∞:** [TBD]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
