# Кратки упражнения: Лекция 3

Следните упражнения са за самостоятелна работа по време на лекцията или веднага след нея. Очаквано време: 2-3 минути за упражнение.

---

## Упражнение 1: OOV проблем

Даден е речник: `{the, cat, sat, on, mat, dog, runs}`

Кои думи ще бъдат OOV (Out-of-Vocabulary) в следните изречения?

1. "the cat sat on the mat"
2. "the kitten runs quickly"
3. "my dog sat quietly"

**Въпрос:** Какъв процент от думите във второто изречение са OOV?

---

## Упражнение 2: Word vs Character токенизация

Даден е текстът: `"Hello world"`

**Задачи:**

1. Токенизирайте с word-level: колко токена?
2. Токенизирайте с character-level: колко токена?
3. Ако имате 1000 такива изречения, кой подход ще има по-дълги последователности?

**Въпрос:** Ако context window е 512 токена, колко "Hello world" изречения се събират при всеки подход?

---

## Упражнение 3: BPE Merge операция

Даден е речник след няколко BPE merge операции:

```
[l o w </w>] : 5
[l o w e r </w>] : 2
[n e w e s t </w>] : 6
```

Честота на двойките:

- (e, s): 6
- (o, w): 7
- (l, o): 7
- (s, t): 6

**Въпроси:**

1. Коя двойка ще бъде избрана за следващия merge?
2. Ако (o, w) се обедини, как ще изглежда речникът?

---

## Упражнение 4: BPE Encoding

Дадени са merge правила (в ред на научаване):

1. (e, s) → es
2. (es, t) → est
3. (l, o) → lo
4. (lo, w) → low

Токенизирайте думата `"lowest"` стъпка по стъпка:

```
Начало: ['l', 'o', 'w', 'e', 's', 't', '</w>']
След merge 1: ?
След merge 2: ?
След merge 3: ?
След merge 4: ?
```

---

## Упражнение 5: Unigram вероятности

Дадени са вероятности на токени:

- P("un") = 0.02
- P("happy") = 0.01
- P("ness") = 0.005
- P("unhappy") = 0.001
- P("happiness") = 0.0005

Изчислете вероятността на две сегментации на "unhappiness":

1. $P([\text{"un"}, \text{"happy"}, \text{"ness"}]) = ?$
2. $P([\text{"un"}, \text{"happiness"}]) = ?$

**Въпрос:** Коя сегментация ще избере Unigram tokenizer?

---

## Упражнение 6: Vocabulary size trade-off

Даден е embedding слой с размерност 768.

Изчислете броя параметри за различни размери на речника:

| Vocab Size | Параметри |
|------------|-----------|
| 30,000 (BERT) | ? |
| 50,000 (GPT-2) | ? |
| 100,000 (GPT-4) | ? |

**Въпрос:** С колко MB се увеличава моделът при преминаване от 50K към 100K? (float32 = 4 bytes)

---

## Упражнение 7: Токени vs думи

Текст: `"The quick brown fox jumps over the lazy dog."`

- Думи: 9
- GPT-2 токени: 9

Нов текст: `"Supercalifragilisticexpialidocious"`

- Думи: 1
- GPT-2 токени: 10

**Въпроси:**

1. Защо дългата дума има толкова много токени?
2. Ако API таксува $0.001 на токен, колко струва всеки текст?

---

## Упражнение 8: Multilingual bias

GPT-2 токенизира следните изречения:

| Език | Текст | Символи | Токени |
|------|-------|---------|--------|
| English | "Hello, how are you?" | 19 | 6 |
| Bulgarian | "Здравей, как си?" | 17 | 13 |
| Chinese | "你好，你好吗？" | 7 | 10 |

Изчислете chars/token за всеки език.

**Въпроси:**

1. Кой език е най-"ефективен"?
2. Ако context window е 4096 токена, колко символа текст се събират за всеки език?

---

## Упражнение 9: Специални токени

Свържете специалния токен с неговата употреба:

| Токен | Модел | Употреба |
|-------|-------|----------|
| [CLS] | BERT | ? |
| [MASK] | BERT | ? |
| [SEP] | BERT | ? |
| <\|endoftext\|> | GPT | ? |

Възможни употреби:

- A: Разделител между изречения
- B: Маскиран токен за MLM
- C: Край на документ
- D: Токен за класификация

---

## Упражнение 10: Tokenization pitfalls

GPT-2 токенизира:

```
"Hello"  → ['Hello']
"Hello " → ['Hello', 'Ġ']
" Hello" → ['ĠHello']
```

**Въпроси:**

1. Какво означава `Ġ` в GPT-2 tokenizer?
2. Защо `"Hello"` и `"Hello "` имат различен брой токени?
3. Ако конкатенирате `"Hello"` + `" world"`, различно ли ще се токенизира от `"Hello world"`?

---

## Упражнение 11: Числа в LLM

GPT-2 токенизира числа така:

| Число | Токени | Брой |
|-------|--------|------|
| "42" | ['42'] | 1 |
| "1234" | ['12', '34'] | 2 |
| "1000000" | ['100', '0000'] | 2 |

**Въпроси:**

1. Защо "42" е един токен, а "1234" е два?
2. Каква е математическата стойност на ['12', '34'] за модела?
3. Защо LLM понякога грешат в аритметика?

---

## Упражнение 12: Compression ratio

BPE tokenizer постига следните резултати:

| Текст | Символи | Токени | Chars/Token |
|-------|---------|--------|-------------|
| English news | 1000 | 250 | 4.0 |
| Python code | 1000 | 400 | 2.5 |
| DNA sequence | 1000 | 800 | 1.25 |

**Въпроси:**

1. Кой тип текст се компресира най-добре?
2. Защо код има по-нисък compression ratio от английски?
3. За какви типове данни BPE работи лошо?

---

# Решения

## Решение 1

1. "the cat sat on the mat" - 0 OOV (всички думи са в речника)
2. "the kitten runs quickly" - OOV: "kitten", "quickly" (2 от 4 думи)
3. "my dog sat quietly" - OOV: "my", "quietly" (2 от 4 думи)

Процент OOV във второто изречение: 2/4 = 50%

## Решение 2

1. Word-level: 2 токена ("Hello", "world")
2. Character-level: 11 токена ('H','e','l','l','o',' ','w','o','r','l','d')
3. Character-level ще има по-дълги последователности

При 512 токена:

- Word-level: 512/2 = 256 изречения
- Character-level: 512/11 ≈ 46 изречения

## Решение 3

1. Има tie между (o, w) и (l, o) с честота 7. Обикновено се избира първата по азбучен ред или първата намерена: (l, o).
2. Ако (o, w) се обедини:

```
[l ow </w>] : 5
[l ow e r </w>] : 2
[n e w e s t </w>] : 6
```

## Решение 4

```
Начало:      ['l', 'o', 'w', 'e', 's', 't', '</w>']
След merge 1: ['l', 'o', 'w', 'es', 't', '</w>']
След merge 2: ['l', 'o', 'w', 'est', '</w>']
След merge 3: ['lo', 'w', 'est', '</w>']
След merge 4: ['low', 'est', '</w>']
```

## Решение 5

1. $P([\text{"un"}, \text{"happy"}, \text{"ness"}]) = 0.02 \times 0.01 \times 0.005 = 0.000001 = 10^{-6}$
2. $P([\text{"un"}, \text{"happiness"}]) = 0.02 \times 0.0005 = 0.00001 = 10^{-5}$

Unigram ще избере ["un", "happiness"] (10× по-висока вероятност).

## Решение 6

| Vocab Size | Параметри |
|------------|-----------|
| 30,000 | 30,000 × 768 = 23,040,000 |
| 50,000 | 50,000 × 768 = 38,400,000 |
| 100,000 | 100,000 × 768 = 76,800,000 |

Увеличение от 50K към 100K:

- Допълнителни параметри: 50,000 × 768 = 38,400,000
- В MB: 38,400,000 × 4 bytes = 153,600,000 bytes ≈ 146.5 MB

## Решение 7

1. Дългата дума е рядка и се разбива на много subword части.
2. Цена:
   - "The quick brown fox...": 9 × $0.001 = $0.009
   - "Supercalifragilisticexpialidocious": 10 × $0.001 = $0.010

   Една дълга дума струва повече от 9 обикновени думи!

## Решение 8

Chars/token:

- English: 19/6 ≈ 3.2
- Bulgarian: 17/13 ≈ 1.3
- Chinese: 7/10 = 0.7

1. Английски е най-ефективен (най-висок chars/token).
2. При 4096 токена:
   - English: 4096 × 3.2 ≈ 13,107 символа
   - Bulgarian: 4096 × 1.3 ≈ 5,325 символа
   - Chinese: 4096 × 0.7 ≈ 2,867 символа

## Решение 9

- [CLS] → D: Токен за класификация
- [MASK] → B: Маскиран токен за MLM
- [SEP] → A: Разделител между изречения
- <|endoftext|> → C: Край на документ

## Решение 10

1. `Ġ` означава space преди токена (GPT-2 encode-ва spaces като част от следващия токен).
2. `"Hello "` има trailing space, който се токенизира отделно.
3. Да, може да е различно! `"Hello" + " world"` може да се токенизира като ['Hello', 'Ġworld'], докато `"Hello world"` може да е ['Hello', 'Ġworld'] или различно в зависимост от имплементацията.

## Решение 11

1. "42" е достатъчно честа комбинация да е в речника като един токен. "1234" се разбива защото е по-рядка.
2. Моделът не "вижда" математическа стойност - вижда само ['12', '34'] като два отделни токена без числова семантика.
3. LLM обработват числа като символни последователности, без вградено разбиране на математически операции. Това води до грешки особено при многоцифрени числа.

## Решение 12

1. English news се компресира най-добре (4.0 chars/token).
2. Кодът съдържа много специални символи, скоби, и променливи имена, които не се срещат често в training данните.
3. BPE работи лошо за:
   - Случайни данни (DNA, криптирани данни)
   - Езици с много малко training данни
   - Силно специализирани домейни
