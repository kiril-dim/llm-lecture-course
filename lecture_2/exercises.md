# Кратки упражнения: Лекция 2

Следните упражнения са за самостоятелна работа по време на лекцията или веднага след нея. Очаквано време: 2-3 минути за упражнение.

---

## Упражнение 1: Защо нелинейност?

Разгледайте следния код:

```python
W1 = np.array([[1, 2], [3, 4]])
W2 = np.array([[5, 6], [7, 8]])
x = np.array([1, 0])

# Два слоя БЕЗ активация
y = W2 @ (W1 @ x)
print("Два слоя:", y)

# Един еквивалентен слой
W_combined = W2 @ W1
print("Един слой:", W_combined @ x)
```

**Въпроси:**
1. Защо резултатите са еднакви?
2. Какво трябва да добавим между слоевете, за да има смисъл от дълбочина?

---

## Упражнение 2: Активационни функции на ръка

Изчислете **на ръка** (без код):

За $z = 2.0$:
- $\text{ReLU}(z) = ?$
- $\sigma(z) = \frac{1}{1 + e^{-z}} \approx ?$ (hint: $e^{-2} \approx 0.135$)

За $z = -1.0$:
- $\text{ReLU}(z) = ?$
- $\sigma(z) \approx ?$ (hint: $e^{1} \approx 2.718$)

---

## Упражнение 3: Embedding размерност

Имате речник с 10,000 думи и embedding размерност 128.

**Въпроси:**
1. Колко параметъра има embedding слоят?
2. Ако увеличим размерността до 256, с колко се увеличават параметрите?
3. Ако речникът е 50,000 думи (като GPT-2), колко параметъра само за embeddings?

---

## Упражнение 4: Forward pass на ръка

Дадена е мрежа с един скрит слой:
- Вход: $x = [1, 2]$
- Тегла: $W^{[1]} = [[0.5, -0.5], [0.5, 0.5]]$, $b^{[1]} = [0, 0]$
- Активация: ReLU

Изчислете $a^{[1]}$ стъпка по стъпка:

```
z^{[1]} = W^{[1]} @ x + b^{[1]} = ?
a^{[1]} = ReLU(z^{[1]}) = ?
```

---

## Упражнение 5: Binary Cross-Entropy

Моделът предсказва $\hat{y} = 0.9$ за пример с истински етикет $y = 1$.

Изчислете BCE loss:
$$L = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]$$

**Hint:** $\log(0.9) \approx -0.105$

**Въпрос:** Какъв би бил loss-ът ако $\hat{y} = 0.1$ за същия $y = 1$? ($\log(0.1) \approx -2.303$)

---

## Упражнение 6: Softmax

Дадени са logits: $z = [2.0, 1.0, 0.0]$

Изчислете softmax вероятностите:
$$\hat{y}_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$$

**Hints:** $e^2 \approx 7.39$, $e^1 \approx 2.72$, $e^0 = 1$

**Въпрос:** Сумата на всички вероятности трябва да е?

---

## Упражнение 7: Gradient Descent стъпка

Параметър $w = 2.0$, learning rate $\alpha = 0.1$, градиент $\frac{\partial L}{\partial w} = 3.0$.

Изчислете новата стойност на $w$ след една стъпка:
$$w_{new} = w - \alpha \cdot \frac{\partial L}{\partial w} = ?$$

**Въпрос:** Ако градиентът е отрицателен ($-3.0$), в коя посока ще се движи $w$?

---

## Упражнение 8: Pooling стратегии

Дадени са embeddings за изречение от 3 думи:
```
word1: [1, 2, 3]
word2: [4, 5, 6]
word3: [7, 8, 9]
```

Изчислете:
1. **Mean pooling:** средно по всяка размерност
2. **Max pooling:** максимум по всяка размерност

**Въпрос:** Каква информация губим с pooling?

---

## Упражнение 9: Брой параметри

Дадена е мрежа:
- Embedding: vocab=1000, dim=64
- Linear 1: 64 → 128 (+ bias)
- Linear 2: 128 → 1 (+ bias)

Изчислете общия брой параметри:
- Embedding: ?
- Linear 1 weights + bias: ?
- Linear 2 weights + bias: ?
- **Общо:** ?

---

## Упражнение 10: Overfitting диагностика

Наблюдавате следните криви при обучение:

| Epoch | Train Loss | Val Loss |
|-------|------------|----------|
| 1     | 0.8        | 0.85     |
| 5     | 0.3        | 0.4      |
| 10    | 0.1        | 0.5      |
| 15    | 0.05       | 0.7      |

**Въпроси:**
1. Има ли overfitting? От коя епоха?
2. Кои техники биха помогнали? (изберете 2-3)
   - Dropout
   - По-голям модел
   - Повече данни
   - Early stopping
   - По-висок learning rate
