{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –õ–µ–∫—Ü–∏—è 3: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "## –û—Ç —Ç–µ–∫—Å—Ç –∫—ä–º –º–æ–¥–µ–ª–∏\n",
    "\n",
    "**–ü—Ä–æ–¥—ä–ª–∂–∏—Ç–µ–ª–Ω–æ—Å—Ç:** 2-2.5 —á–∞—Å–∞\n",
    "**–ü—Ä–µ–¥–ø–æ—Å—Ç–∞–≤–∫–∞:** –õ–µ–∫—Ü–∏—è 2 (–ï–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏, word embeddings)\n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –ù–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. –ú–æ—Ç–∏–≤–∞—Ü–∏—è: –ö–∞–∫–≤–æ –µ \"–¥—É–º–∞\"?\n",
    "\n",
    "### –ü—Ä–æ–±–ª–µ–º—ä—Ç —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è—Ç–∞\n",
    "\n",
    "–í –õ–µ–∫—Ü–∏—è 2 –≤–∏–¥—è—Ö–º–µ **Word2Vec** - –º–æ—â–µ–Ω –Ω–∞—á–∏–Ω –¥–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º –¥—É–º–∏ –∫–∞—Ç–æ –≤–µ–∫—Ç–æ—Ä–∏. –ù–æ –∏–º–∞–º–µ –ø—Ä–æ–±–ª–µ–º:\n",
    "\n",
    "**–ö–∞–∫ —Ç–æ—á–Ω–æ –¥–µ—Ñ–∏–Ω–∏—Ä–∞–º–µ \"–¥—É–º–∞\"?**\n",
    "\n",
    "### –ü—Ä–µ–¥–∏–∑–≤–∏–∫–∞—Ç–µ–ª—Å—Ç–≤–∞\n",
    "\n",
    "#### 1. –ú–Ω–æ–≥–æ–¥—É–º–æ–≤–∏ –∏–∑—Ä–∞–∑–∏\n",
    "- \"New York\" - –µ–¥–Ω–∞ –∏–ª–∏ –¥–≤–µ –¥—É–º–∏?\n",
    "- \"ice cream\" - –µ–¥–Ω–æ –ø–æ–Ω—è—Ç–∏–µ, –¥–≤–µ –¥—É–º–∏\n",
    "\n",
    "#### 2. –ö–æ–Ω—Ç—Ä–∞–∫—Ü–∏–∏\n",
    "- \"don't\" ‚Üí \"do\" + \"n't\"?\n",
    "- \"I'm\" ‚Üí \"I\" + \"am\"?\n",
    "\n",
    "#### 3. –†–µ–¥–∫–∏ –¥—É–º–∏\n",
    "- \"ChatGPT\" –Ω–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—à–µ –ø—Ä–µ–¥–∏ 2022\n",
    "- –ö–∞–∫–≤–æ –ø—Ä–∞–≤–∏–º —Å –¥—É–º–∏ –∏–∑–≤—ä–Ω —Ä–µ—á–Ω–∏–∫–∞?\n",
    "\n",
    "### –†–µ—à–µ–Ω–∏—è\n",
    "\n",
    "```\n",
    "–î—É–º–∏      ‚Üí –ü—Ä–æ—Å—Ç, –Ω–æ –æ–≥—Ä–æ–º–µ–Ω —Ä–µ—á–Ω–∏–∫\n",
    "–°–∏–º–≤–æ–ª–∏   ‚Üí –ú–∞–ª—ä–∫ —Ä–µ—á–Ω–∏–∫, –Ω–æ –≥—É–±–∏–º —Å–µ–º–∞–Ω—Ç–∏–∫–∞\n",
    "Subwords  ‚Üí –ù–∞–π-–¥–æ–±—Ä–æ—Ç–æ –æ—Ç –¥–≤–∞—Ç–∞ —Å–≤–µ—Ç–∞! ‚úì\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–∏—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì –í—Å–∏—á–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏ —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. –ü—Ä–æ—Å—Ç–∏ –ø–æ–¥—Ö–æ–¥–∏ –∑–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "### 2.1 Word-level —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "**–ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞:**\n",
    "- ‚úÖ –ü—Ä–æ—Å—Ç –∏ –∏–Ω—Ç—É–∏—Ç–∏–≤–µ–Ω\n",
    "- ‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–Ω–æ –∑–Ω–∞—á–∏–º–∏ –µ–¥–∏–Ω–∏—Ü–∏\n",
    "\n",
    "**–ù–µ–¥–æ—Å—Ç–∞—Ç—ä—Ü–∏:**\n",
    "- ‚ùå –û–≥—Ä–æ–º–µ–Ω —Ä–µ—á–Ω–∏–∫ (100,000+ –¥—É–º–∏)\n",
    "- ‚ùå Out-of-vocabulary (OOV) –ø—Ä–æ–±–ª–µ–º\n",
    "- ‚ùå –ù—è–º–∞ —Å–ø–æ–¥–µ–ª—è–Ω–µ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏\n",
    "\n",
    "### 2.2 Character-level —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "**–ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞:**\n",
    "- ‚úÖ –ú–∞–ª—ä–∫ —Ä–µ—á–Ω–∏–∫ (26 –±—É–∫–≤–∏ + –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è)\n",
    "- ‚úÖ –ù—è–º–∞ OOV –ø—Ä–æ–±–ª–µ–º\n",
    "\n",
    "**–ù–µ–¥–æ—Å—Ç–∞—Ç—ä—Ü–∏:**\n",
    "- ‚ùå –ú–Ω–æ–≥–æ –¥—ä–ª–≥–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏\n",
    "- ‚ùå –°–∏–º–≤–æ–ª–∏—Ç–µ –Ω—è–º–∞—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∞\n",
    "- ‚ùå –ü–æ-—Ç—Ä—É–¥–Ω–æ –∑–∞ –º–æ–¥–µ–ª–∞ –¥–∞ –Ω–∞—É—á–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ word-level –∏ character-level —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "example_text = \"Hello, world! The weather is wonderful.\"\n",
    "\n",
    "print(\"üìù –ü—Ä–∏–º–µ—Ä–µ–Ω —Ç–µ–∫—Å—Ç:\")\n",
    "print(f\"'{example_text}'\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Word-level\n",
    "word_tokens = example_text.split()\n",
    "print(\"\\n1Ô∏è‚É£ Word-level —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:\")\n",
    "print(f\"–¢–æ–∫–µ–Ω–∏: {word_tokens}\")\n",
    "print(f\"–ë—Ä–æ–π —Ç–æ–∫–µ–Ω–∏: {len(word_tokens)}\")\n",
    "\n",
    "# Character-level\n",
    "char_tokens = list(example_text)\n",
    "print(\"\\n2Ô∏è‚É£ Character-level —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:\")\n",
    "print(f\"–ü—ä—Ä–≤–∏—Ç–µ 20 —Ç–æ–∫–µ–Ω–∞: {char_tokens[:20]}\")\n",
    "print(f\"–ë—Ä–æ–π —Ç–æ–∫–µ–Ω–∏: {len(char_tokens)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ:\")\n",
    "comparison = pd.DataFrame({\n",
    "    '–ú–µ—Ç–æ–¥': ['Word-level', 'Character-level'],\n",
    "    '–ë—Ä–æ–π —Ç–æ–∫–µ–Ω–∏': [len(word_tokens), len(char_tokens)],\n",
    "    '–†–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞': [len(set(word_tokens)), len(set(char_tokens))]\n",
    "})\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Subword Tokenization: Byte-Pair Encoding (BPE)\n",
    "\n",
    "### –û—Å–Ω–æ–≤–Ω–∞ –∏–¥–µ—è\n",
    "\n",
    "**BPE** –µ –∞–ª–≥–æ—Ä–∏—Ç—ä–º –∑–∞ –∫–æ–º–ø—Ä–µ—Å–∏—è, –∞–¥–∞–ø—Ç–∏—Ä–∞–Ω –∑–∞ NLP.\n",
    "\n",
    "**–§–∏–ª–æ—Å–æ—Ñ–∏—è:**\n",
    "- –ó–∞–ø–æ—á–≤–∞–º–µ —Å —Å–∏–º–≤–æ–ª–∏\n",
    "- –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ **—Å–ª–∏–≤–∞–º–µ –Ω–∞–π-—á–µ—Å—Ç–∞—Ç–∞ –¥–≤–æ–π–∫–∞**\n",
    "- –ü–æ–≤—Ç–∞—Ä—è–º–µ –¥–æ–∫–∞—Ç–æ –¥–æ—Å—Ç–∏–≥–Ω–µ–º –∂–µ–ª–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä\n",
    "\n",
    "### BPE –ê–ª–≥–æ—Ä–∏—Ç—ä–º\n",
    "\n",
    "**–û–±—É—á–µ–Ω–∏–µ:**\n",
    "```\n",
    "1. –ó–∞–ø–æ—á–Ω–∏ —Å —Å–∏–º–≤–æ–ª–∏\n",
    "2. –î–æ–∫–∞—Ç–æ —Ä–µ—á–Ω–∏–∫ < —Ä–∞–∑–º–µ—Ä:\n",
    "   a. –ù–∞–º–µ—Ä–∏ –Ω–∞–π-—á–µ—Å—Ç–∞—Ç–∞ –¥–≤–æ–π–∫–∞\n",
    "   b. –°–ª–µ–π –≥–∏ –≤ –Ω–æ–≤ —Å–∏–º–≤–æ–ª\n",
    "   c. –ó–∞–º–µ–Ω–∏ –≤—Å–∏—á–∫–∏ —Å—Ä–µ—â–∞–Ω–∏—è\n",
    "```\n",
    "\n",
    "**–ò–∑–ø–æ–ª–∑–≤–∞ —Å–µ –≤:**\n",
    "- GPT-2, GPT-3, GPT-4\n",
    "- LLaMA\n",
    "- RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ—Å—Ç–∞ –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ BPE\n",
    "\n",
    "class SimpleBPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.merges = {}\n",
    "        self.merge_order = []\n",
    "\n",
    "    def get_pairs(self, word):\n",
    "        pairs = set()\n",
    "        for i in range(len(word) - 1):\n",
    "            pairs.add((word[i], word[i+1]))\n",
    "        return pairs\n",
    "\n",
    "    def train(self, corpus, num_merges):\n",
    "        # –ó–∞–ø–æ—á–≤–∞–º–µ —Å –¥—É–º–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏ –Ω–∞ —Å–∏–º–≤–æ–ª–∏\n",
    "        vocab = {}\n",
    "        for word, freq in corpus.items():\n",
    "            word_chars = list(word) + ['</w>']\n",
    "            vocab[tuple(word_chars)] = freq\n",
    "            self.vocab.update(word_chars)\n",
    "\n",
    "        print(f\"–ù–∞—á–∞–ª–µ–Ω —Ä–µ—á–Ω–∏–∫: {sorted(self.vocab)}\")\n",
    "        print(f\"–†–∞–∑–º–µ—Ä: {len(self.vocab)}\\n\")\n",
    "\n",
    "        # Merge –æ–ø–µ—Ä–∞—Ü–∏–∏\n",
    "        for merge_num in range(num_merges):\n",
    "            pair_counts = Counter()\n",
    "            for word, freq in vocab.items():\n",
    "                pairs = self.get_pairs(word)\n",
    "                for pair in pairs:\n",
    "                    pair_counts[pair] += freq\n",
    "\n",
    "            if not pair_counts:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pair_counts, key=pair_counts.get)\n",
    "            merged = ''.join(best_pair)\n",
    "\n",
    "            self.merges[best_pair] = merged\n",
    "            self.merge_order.append((best_pair, merged))\n",
    "            self.vocab.add(merged)\n",
    "\n",
    "            print(f\"–ò—Ç–µ—Ä–∞—Ü–∏—è {merge_num + 1}:\")\n",
    "            print(f\"  Merge: '{best_pair[0]}' + '{best_pair[1]}' ‚Üí '{merged}'\")\n",
    "            print(f\"  –ß–µ—Å—Ç–æ—Ç–∞: {pair_counts[best_pair]}\")\n",
    "\n",
    "            # –û–±–Ω–æ–≤—è–≤–∞–º–µ —Ä–µ—á–Ω–∏–∫–∞\n",
    "            new_vocab = {}\n",
    "            for word, freq in vocab.items():\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word):\n",
    "                    if i < len(word) - 1 and (word[i], word[i+1]) == best_pair:\n",
    "                        new_word.append(merged)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word.append(word[i])\n",
    "                        i += 1\n",
    "                new_vocab[tuple(new_word)] = freq\n",
    "            vocab = new_vocab\n",
    "\n",
    "        print(f\"\\n‚úì –§–∏–Ω–∞–ª–µ–Ω —Ä–µ—á–Ω–∏–∫: {len(self.vocab)} —Ç–æ–∫–µ–Ω–∞\")\n",
    "\n",
    "    def tokenize(self, word):\n",
    "        word = list(word) + ['</w>']\n",
    "        for pair, merged in self.merge_order:\n",
    "            i = 0\n",
    "            new_word = []\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                    new_word.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            word = new_word\n",
    "        return word\n",
    "\n",
    "print(\"‚úì SimpleBPE –∫–ª–∞—Å –¥–µ—Ñ–∏–Ω–∏—Ä–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–∞–≤–∞–º–µ BPE\n",
    "\n",
    "corpus = {\n",
    "    'low': 5,\n",
    "    'lower': 2,\n",
    "    'newest': 6,\n",
    "    'widest': 3,\n",
    "}\n",
    "\n",
    "print(\"üìö –ö–æ—Ä–ø—É—Å:\")\n",
    "for word, freq in corpus.items():\n",
    "    print(f\"  '{word}': {freq}\")\n",
    "print()\n",
    "\n",
    "bpe = SimpleBPE()\n",
    "bpe.train(corpus, num_merges=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç–≤–∞–º–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è—Ç–∞\n",
    "\n",
    "print(\"\\nüß™ –¢–µ—Å—Ç–≤–∞–Ω–µ\\n\")\n",
    "\n",
    "test_words = ['lowest', 'newer', 'wider', 'low']\n",
    "\n",
    "for word in test_words:\n",
    "    tokens = bpe.tokenize(word)\n",
    "    print(f\"'{word}': {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤ —Å—ä–≤—Ä–µ–º–µ–Ω–Ω–∏—Ç–µ LLM\n",
    "\n",
    "### –ü–æ–ø—É–ª—è—Ä–Ω–∏ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "| –ú–æ–¥–µ–ª | –ê–ª–≥–æ—Ä–∏—Ç—ä–º | Vocab Size |\n",
    "|-------|-----------|------------|\n",
    "| GPT-2/3 | BPE | 50,257 |\n",
    "| BERT | WordPiece | 30,522 |\n",
    "| LLaMA | BPE | 32,000 |\n",
    "| T5 | Unigram | 32,000 |\n",
    "\n",
    "### Byte-level BPE\n",
    "\n",
    "**–ú–æ–¥–µ—Ä–Ω–∏—è—Ç –ø–æ–¥—Ö–æ–¥:**\n",
    "- –†–∞–±–æ—Ç–∏ –Ω–∞ byte –Ω–∏–≤–æ (UTF-8)\n",
    "- 256 –±–∞–∑–æ–≤–∏ tokens (bytes)\n",
    "- **–ù–∏–∫–æ–≥–∞ –Ω—è–º–∞ unknown token!**\n",
    "\n",
    "### Context Window\n",
    "\n",
    "**–í–∞–∂–Ω–æ:**\n",
    "- \"8k context\" = 8,000 –¢–û–ö–ï–ù–ê, –Ω–µ –¥—É–º–∏!\n",
    "- API costs –±–∞–∑–∏—Ä–∞–Ω–∏ –Ω–∞ tokens\n",
    "- –°—Ä–µ–¥–Ω–æ 1 token ‚âà 0.75 –¥—É–º–∏ (–∞–Ω–≥–ª–∏–π—Å–∫–∏)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3.5 Byte-Level BPE: –ú–æ–¥–µ—Ä–Ω–∏—è—Ç –ø–æ–¥—Ö–æ–¥\n\n### –ü—Ä–æ–±–ª–µ–º—ä—Ç —Å Character-Level BPE\n\n**Character-level BPE –∏–º–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**\n- Unicode –∏–º–∞ 140,000+ —Å–∏–º–≤–æ–ª–∞\n- –ö–∞–∫ –æ–±—Ä–∞–±–æ—Ç–≤–∞–º–µ emoji? ü§ñüöÄüéâ\n- –ö–∞–∫ –æ–±—Ä–∞–±–æ—Ç–≤–∞–º–µ —Å–ø–µ—Ü–∏–∞–ª–Ω–∏ —Å–∏–º–≤–æ–ª–∏? ‚Ñ¢Ô∏è, ¬©, ¬Æ\n- –ö–∞–∫ –æ–±—Ä–∞–±–æ—Ç–≤–∞–º–µ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ –µ–∑–∏—Ü–∏?\n\n**–ü—Ä–∏–º–µ—Ä:**\n```\n\"Hello ‰∏ñÁïå ŸÖÿ±ÿ≠ÿ®ÿß üåç\" \n```\n- –õ–∞—Ç–∏–Ω–∏—Ü–∞, –∫–∏—Ç–∞–π—Å–∫–∏, –∞—Ä–∞–±—Å–∫–∏, emoji\n- Character-level BPE —Ç—Ä—è–±–≤–∞ –¥–∞ –∑–Ω–∞–µ –≤—Å–∏—á–∫–∏ —Ç–µ–∑–∏ —Å–∏–º–≤–æ–ª–∏!\n\n### –†–µ—à–µ–Ω–∏–µ—Ç–æ: Byte-Level Encoding\n\n**–ö–ª—é—á–æ–≤–∞ –∏–¥–µ—è:**\n- –í—Å–µ–∫–∏ —Ç–µ–∫—Å—Ç —Å–µ –∫–æ–¥–∏—Ä–∞ –∫–∞—Ç–æ **bytes** (UTF-8)\n- UTF-8 –∏–∑–ø–æ–ª–∑–≤–∞ 1-4 bytes –∑–∞ –≤—Å–µ–∫–∏ Unicode —Å–∏–º–≤–æ–ª\n- –í—ä–∑–º–æ–∂–Ω–∏ byte —Å—Ç–æ–π–Ω–æ—Å—Ç–∏: 0-255 (—Å–∞–º–æ 256!)\n- BPE —Ä–∞–±–æ—Ç–∏ –Ω–∞ byte –Ω–∏–≤–æ –≤–º–µ—Å—Ç–æ character –Ω–∏–≤–æ\n\n**–ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞:**\n- ‚úÖ **–£–Ω–∏–≤–µ—Ä—Å–∞–ª–Ω–æ:** –ú–æ–∂–µ –¥–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏ –í–°–ï–ö–ò —Ç–µ–∫—Å—Ç\n- ‚úÖ **–ú–∞–ª—ä–∫ –±–∞–∑–æ–≤ —Ä–µ—á–Ω–∏–∫:** –°–∞–º–æ 256 bytes\n- ‚úÖ **–ù–∏–∫–æ–≥–∞ –Ω—è–º–∞ [UNK]:** –í—Å–µ–∫–∏ —Å–∏–º–≤–æ–ª –º–æ–∂–µ –¥–∞ –±—ä–¥–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–µ–Ω\n- ‚úÖ **–ò–∑–ø–æ–ª–∑–≤–∞ —Å–µ –æ—Ç GPT-2/3/4**\n\n### UTF-8 Encoding Basics\n\n**UTF-8 –ø—Ä–∞–≤–∏–ª–∞:**\n- ASCII —Å–∏–º–≤–æ–ª–∏ (a-z, 0-9): 1 byte\n- –ï–≤—Ä–æ–ø–µ–π—Å–∫–∏ —Å–∏–º–≤–æ–ª–∏ (√°, √©, √±): 2 bytes\n- –ö–∏—Ä–∏–ª–∏—Ü–∞, –∫–∏—Ç–∞–π—Å–∫–∏, –∞—Ä–∞–±—Å–∫–∏: 2-3 bytes\n- Emoji: 3-4 bytes\n\n**–ü—Ä–∏–º–µ—Ä:**\n```\n\"Hello\" ‚Üí [72, 101, 108, 108, 111] (5 bytes)\n\"–ó–¥—Ä–∞–≤–µ–π\" ‚Üí 14 bytes (–∫–∏—Ä–∏–ª–∏—Ü–∞)\n\"‰Ω†Â•Ω\" ‚Üí 6 bytes (–∫–∏—Ç–∞–π—Å–∫–∏)\n\"ü§ñ\" ‚Üí 4 bytes (emoji)\n```\n\n### –ù–µ–∫–∞ –¥–∞ –≤–∏–¥–∏–º —Ç–æ–≤–∞ –≤ –¥–µ–π—Å—Ç–≤–∏–µ!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ UTF-8 encoding\n\ndef show_utf8_encoding(text):\n    \"\"\"–ü–æ–∫–∞–∑–≤–∞ –∫–∞–∫ —Ç–µ–∫—Å—Ç —Å–µ –∫–æ–¥–∏—Ä–∞ –∫–∞—Ç–æ bytes\"\"\"\n    print(f\"–¢–µ–∫—Å—Ç: '{text}'\")\n    print(f\"–î—ä–ª–∂–∏–Ω–∞ (—Å–∏–º–≤–æ–ª–∏): {len(text)}\")\n    \n    # –ö–æ–¥–∏—Ä–∞–Ω–µ –≤ UTF-8 bytes\n    encoded = text.encode('utf-8')\n    print(f\"UTF-8 bytes: {list(encoded)}\")\n    print(f\"–î—ä–ª–∂–∏–Ω–∞ (bytes): {len(encoded)}\")\n    print(f\"Hex –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ: {encoded.hex()}\")\n    \n    # –ü–æ–∫–∞–∑–≤–∞–º–µ –≤—Å–µ–∫–∏ —Å–∏–º–≤–æ–ª\n    print(\"\\n–°–∏–º–≤–æ–ª –ø–æ —Å–∏–º–≤–æ–ª:\")\n    for char in text:\n        char_bytes = char.encode('utf-8')\n        print(f\"  '{char}' ‚Üí {list(char_bytes)} ({len(char_bytes)} bytes)\")\n    print()\n\nprint(\"üî§ UTF-8 Encoding –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\\n\")\nprint(\"=\"*70)\n\n# ASCII —Ç–µ–∫—Å—Ç\nprint(\"\\n1Ô∏è‚É£ ASCII —Ç–µ–∫—Å—Ç (1 byte –Ω–∞ —Å–∏–º–≤–æ–ª):\")\nshow_utf8_encoding(\"Hello\")\n\n# –ö–∏—Ä–∏–ª–∏—Ü–∞\nprint(\"=\"*70)\nprint(\"\\n2Ô∏è‚É£ –ö–∏—Ä–∏–ª–∏—Ü–∞ (2 bytes –Ω–∞ —Å–∏–º–≤–æ–ª):\")\nshow_utf8_encoding(\"–ó–¥—Ä–∞–≤–µ–π\")\n\n# –°–º–µ—Å–µ–Ω —Ç–µ–∫—Å—Ç\nprint(\"=\"*70)\nprint(\"\\n3Ô∏è‚É£ –°–º–µ—Å–µ–Ω —Ç–µ–∫—Å—Ç:\")\nshow_utf8_encoding(\"Hello –ó–¥—Ä–∞–≤–µ–π\")\n\n# Emoji\nprint(\"=\"*70)\nprint(\"\\n4Ô∏è‚É£ Emoji (3-4 bytes):\")\nshow_utf8_encoding(\"ü§ñüöÄ\")\n\n# –ö–∏—Ç–∞–π—Å–∫–∏\nprint(\"=\"*70)\nprint(\"\\n5Ô∏è‚É£ –ö–∏—Ç–∞–π—Å–∫–∏ (3 bytes –Ω–∞ —Å–∏–º–≤–æ–ª):\")\nshow_utf8_encoding(\"‰Ω†Â•Ω‰∏ñÁïå\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Byte-Level BPE: –ö–∞–∫ —Ä–∞–±–æ—Ç–∏\n\nprint(\"üîß Byte-Level BPE Process\\n\")\nprint(\"=\"*70)\n\ntext = \"Hello ü§ñ\"\n\nprint(f\"–û—Ä–∏–≥–∏–Ω–∞–ª–µ–Ω —Ç–µ–∫—Å—Ç: '{text}'\\n\")\n\n# –°—Ç—ä–ø–∫–∞ 1: UTF-8 encoding\nutf8_bytes = text.encode('utf-8')\nprint(f\"–°—Ç—ä–ø–∫–∞ 1 - UTF-8 encoding:\")\nprint(f\"  Bytes: {list(utf8_bytes)}\")\nprint(f\"  –ë—Ä–æ–π bytes: {len(utf8_bytes)}\\n\")\n\n# –°—Ç—ä–ø–∫–∞ 2: –ë–∞–π—Ç–æ–≤–µ—Ç–µ —Å–∞ –Ω–∞—á–∞–ª–Ω–∏—è—Ç \"—Ä–µ—á–Ω–∏–∫\"\nprint(f\"–°—Ç—ä–ø–∫–∞ 2 - –ë–∞–∑–æ–≤ —Ä–µ—á–Ω–∏–∫:\")\nprint(f\"  –í—ä–∑–º–æ–∂–Ω–∏ bytes: 0-255 (256 —Ç–æ–∫–µ–Ω–∞)\")\nprint(f\"  –¢–µ–∫—É—â–∏ bytes: {set(utf8_bytes)}\\n\")\n\n# –°—Ç—ä–ø–∫–∞ 3: BPE merge –æ–ø–µ—Ä–∞—Ü–∏–∏\nprint(f\"–°—Ç—ä–ø–∫–∞ 3 - BPE merges:\")\nprint(f\"  BPE —Å–µ–≥–∞ —Ä–∞–±–æ—Ç–∏ –Ω–∞ byte –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏\")\nprint(f\"  –ß–µ—Å—Ç–æ —Å—Ä–µ—â–∞–Ω–∏ byte –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏ —Å–µ —Å–ª–∏–≤–∞—Ç\")\nprint(f\"  –ù–∞–ø—Ä–∏–º–µ—Ä: [72, 101] (He) –º–æ–∂–µ –¥–∞ —Å–µ —Å–ª–µ–µ –≤ –µ–¥–∏–Ω —Ç–æ–∫–µ–Ω\\n\")\n\n# –°—Ç—ä–ø–∫–∞ 4: Mapping –æ–±—Ä–∞—Ç–Ω–æ\nprint(f\"–°—Ç—ä–ø–∫–∞ 4 - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:\")\nprint(f\"  GPT-2 –∏–∑–ø–æ–ª–∑–≤–∞ —Å–ø–µ—Ü–∏–∞–ª–Ω–æ mapping:\")\nprint(f\"  Bytes —Å–µ map–≤–∞—Ç –∫—ä–º printable Unicode —Å–∏–º–≤–æ–ª–∏\")\n\n# GPT-2 byte encoder –ø—Ä–∏–º–µ—Ä\ngpt2_byte_to_unicode_sample = {\n    33: '!', 34: '\"', 35: '#', 72: 'H', 101: 'e', 108: 'l', 111: 'o'\n}\nprint(f\"  –ü—Ä–∏–º–µ—Ä mapping: {gpt2_byte_to_unicode_sample}\")\n\nprint(\"\\nüí° –†–µ–∑—É–ª—Ç–∞—Ç:\")\nprint(\"   ‚Ä¢ –ù–∏–∫–æ–≥–∞ –Ω—è–º–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∏ —Å–∏–º–≤–æ–ª–∏\")\nprint(\"   ‚Ä¢ –†–∞–±–æ—Ç–∏ —Å –í–°–ï–ö–ò Unicode —Ç–µ–∫—Å—Ç\")\nprint(\"   ‚Ä¢ GPT-2/3/4 –∏–∑–ø–æ–ª–∑–≤–∞—Ç —Ç–æ–∑–∏ –ø–æ–¥—Ö–æ–¥\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Byte-Level vs Character-Level BPE\n\n**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ:**\n\n| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | Character-Level | Byte-Level |\n|---------------|-----------------|------------|\n| –ë–∞–∑–æ–≤ —Ä–µ—á–Ω–∏–∫ | ~100 chars | 256 bytes |\n| Unicode –ø–æ–¥–¥—Ä—ä–∂–∫–∞ | –ß–∞—Å—Ç–∏—á–Ω–∞ | –ü—ä–ª–Ω–∞ |\n| Unknown tokens | –í—ä–∑–º–æ–∂–Ω–∏ | –ù–∏–∫–æ–≥–∞ |\n| Emoji | –ü—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–∏ | –†–∞–±–æ—Ç—è—Ç –ø–µ—Ä—Ñ–µ–∫—Ç–Ω–æ |\n| Multilingual | –ò–∑–∏—Å–∫–≤–∞ –≥–æ–ª—è–º vocab | –†–∞–±–æ—Ç–∏ –Ω–∞–≤—Å—è–∫—ä–¥–µ |\n| –ò–∑–ø–æ–ª–∑–≤–∞ —Å–µ –≤ | –°—Ç–∞—Ä–∏ –º–æ–¥–µ–ª–∏ | GPT-2/3/4, LLaMA |\n\n### GPT-2 Byte-Level BPE –î–µ—Ç–∞–π–ª–∏\n\n**GPT-2 —Ä–µ—á–Ω–∏–∫:**\n```\n50,257 tokens = 256 bytes + 50,000 merges + 1 special token\n```\n\n**–ó–∞—â–æ 256 bytes?**\n- UTF-8 –∏–∑–ø–æ–ª–∑–≤–∞ bytes (0-255)\n- –í—Å–µ–∫–∏ –≤—ä–∑–º–æ–∂–µ–Ω byte –µ –æ—Ç–¥–µ–ª–µ–Ω token\n- –ì–∞—Ä–∞–Ω—Ç–∏—Ä–∞ —á–µ –í–°–ï–ö–ò —Ç–µ–∫—Å—Ç –º–æ–∂–µ –¥–∞ –±—ä–¥–µ –∫–æ–¥–∏—Ä–∞–Ω\n\n**Byte-to-Unicode Mapping:**\n- GPT-2 –∏–∑–ø–æ–ª–∑–≤–∞ clever trick\n- Map–≤–∞ bytes –∫—ä–º printable Unicode range\n- –ü–æ–∑–≤–æ–ª—è–≤–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ tokens\n- –ò–∑–±—è–≥–≤–∞ –ø—Ä–æ–±–ª–µ–º–∏ —Å –∫–æ–Ω—Ç—Ä–æ–ª–Ω–∏ —Å–∏–º–≤–æ–ª–∏\n\n### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ—Ä\n\n**–í—ä–ø—Ä–æ—Å:** –ö–∞–∫ GPT-2 —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–∞ \"Hello ü§ñ\"?\n\n**–û—Ç–≥–æ–≤–æ—Ä:**\n1. \"Hello \" ‚Üí –ß–µ—Å—Ç–æ —Å—Ä–µ—â–∞–Ω–æ ‚Üí 1 token\n2. \"ü§ñ\" ‚Üí 4 bytes ‚Üí –≤–µ—Ä–æ—è—Ç–Ω–æ 1-2 tokens (–∑–∞–≤–∏—Å–∏ –æ—Ç merges)\n\n**–ó–∞ —Ä–∞–∑–ª–∏–∫–∞ –æ—Ç character-level:**\n- Character-level –º–æ–∂–µ –¥–∞ –Ω–µ –∑–Ω–∞–µ –∫–∞–∫–≤–æ –µ ü§ñ\n- Byte-level –í–ò–ù–ê–ì–ò –º–æ–∂–µ –¥–∞ –≥–æ –∫–æ–¥–∏—Ä–∞ (4 bytes)\n\n### –ó–∞—â–æ —Ç–æ–≤–∞ –µ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ\n\n**–ü—Ä–µ–¥–∏ Byte-Level BPE:**\n- –ú–æ–¥–µ–ª–∏ —Ç—Ä—è–±–≤–∞—à–µ –¥–∞ \"–∑–Ω–∞—è—Ç\" –≤—Å–∏—á–∫–∏ –≤—ä–∑–º–æ–∂–Ω–∏ —Å–∏–º–≤–æ–ª–∏\n- –ù–æ–≤–∏ emoji = –ø—Ä–æ–±–ª–µ–º\n- –°–ø–µ—Ü–∏–∞–ª–Ω–∏ —Å–∏–º–≤–æ–ª–∏ = [UNK]\n- –†–∞–∑–ª–∏—á–Ω–∏ –µ–∑–∏—Ü–∏ = –æ–≥—Ä–æ–º–µ–Ω vocab\n\n**–°–ª–µ–¥ Byte-Level BPE:**\n- ‚úÖ –†–∞–±–æ—Ç–∏ —Å –í–°–ï–ö–ò —Ç–µ–∫—Å—Ç\n- ‚úÖ –ù–∏–∫–æ–≥–∞ –Ω—è–º–∞ [UNK]\n- ‚úÖ –ù–æ–≤–∏ —Å–∏–º–≤–æ–ª–∏ = –Ω—è–º–∞ –ø—Ä–æ–±–ª–µ–º\n- ‚úÖ –£–Ω–∏–≤–µ—Ä—Å–∞–ª–Ω–æ —Ä–µ—à–µ–Ω–∏–µ\n\n**–¢–æ–≤–∞ –µ –µ–¥–Ω–∞ –æ—Ç –ø—Ä–∏—á–∏–Ω–∏—Ç–µ GPT-2/3/4 –¥–∞ —Å–∞ —Ç–æ–ª–∫–æ–≤–∞ –º–æ—â–Ω–∏!**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: Character vs Byte representation\n\nprint(\"üìä Character-Level vs Byte-Level Comparison\\n\")\nprint(\"=\"*70)\n\ntest_strings = [\n    \"Hello\",\n    \"–ó–¥—Ä–∞–≤–µ–π\", \n    \"‰Ω†Â•Ω\",\n    \"ŸÖÿ±ÿ≠ÿ®ÿß\",\n    \"ü§ñüöÄ\"\n]\n\nresults = []\n\nfor text in test_strings:\n    char_count = len(text)\n    byte_count = len(text.encode('utf-8'))\n    bytes_per_char = byte_count / char_count\n    \n    results.append({\n        '–¢–µ–∫—Å—Ç': text,\n        '–°–∏–º–≤–æ–ª–∏': char_count,\n        'Bytes': byte_count,\n        'Bytes/Char': f'{bytes_per_char:.1f}'\n    })\n\ndf = pd.DataFrame(results)\nprint(df.to_string(index=False))\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\\nüí° –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\nprint(\"   ‚Ä¢ ASCII: 1 byte –Ω–∞ —Å–∏–º–≤–æ–ª\")\nprint(\"   ‚Ä¢ –ö–∏—Ä–∏–ª–∏—Ü–∞: 2 bytes –Ω–∞ —Å–∏–º–≤–æ–ª\")\nprint(\"   ‚Ä¢ –ö–∏—Ç–∞–π—Å–∫–∏: 3 bytes –Ω–∞ —Å–∏–º–≤–æ–ª\")\nprint(\"   ‚Ä¢ Emoji: 4 bytes –Ω–∞ —Å–∏–º–≤–æ–ª\")\nprint(\"\\n   ‚Üí Character-level vocab —Ç—Ä—è–±–≤–∞ –¥–∞ —Å—ä–¥—ä—Ä–∂–∞ –≤—Å–∏—á–∫–∏ —Å–∏–º–≤–æ–ª–∏\")\nprint(\"   ‚Üí Byte-level vocab –≤–∏–Ω–∞–≥–∏ –µ —Å–∞–º–æ 256!\")\n\n# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\nplt.figure(figsize=(10, 6))\nlanguages = [r['–¢–µ–∫—Å—Ç'] for r in results]\nchar_counts = [r['–°–∏–º–≤–æ–ª–∏'] for r in results]\nbyte_counts = [r['Bytes'] for r in results]\n\nx = np.arange(len(languages))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(10, 6))\nbars1 = ax.bar(x - width/2, char_counts, width, label='–°–∏–º–≤–æ–ª–∏', color='steelblue', alpha=0.8)\nbars2 = ax.bar(x + width/2, byte_counts, width, label='Bytes', color='coral', alpha=0.8)\n\nax.set_xlabel('–¢–µ–∫—Å—Ç', fontsize=12)\nax.set_ylabel('–ë—Ä–æ–π', fontsize=12)\nax.set_title('Character Count vs Byte Count', fontsize=14, pad=15)\nax.set_xticks(x)\nax.set_xticklabels(languages)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úì Byte-level BPE —Ä–µ—à–∞–≤–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å multilingual text!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å –º–æ–¥–µ—Ä–Ω–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∏\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    print(\"üî¨ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∏\\n\")\n",
    "\n",
    "    text = \"Hello! GPT-3 and BERT use different tokenizers.\"\n",
    "    print(f\"–¢–µ–∫—Å—Ç: '{text}'\\n\")\n",
    "\n",
    "    # GPT-2\n",
    "    try:\n",
    "        gpt2 = AutoTokenizer.from_pretrained('gpt2')\n",
    "        gpt2_tokens = gpt2.tokenize(text)\n",
    "        print(f\"GPT-2 ({len(gpt2_tokens)} tokens):\")\n",
    "        print(f\"  {gpt2_tokens}\")\n",
    "    except:\n",
    "        print(\"GPT-2: –Ω–µ –º–æ–∂–µ –¥–∞ —Å–µ –∑–∞—Ä–µ–¥–∏\")\n",
    "\n",
    "    # BERT\n",
    "    try:\n",
    "        bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        bert_tokens = bert.tokenize(text)\n",
    "        print(f\"\\nBERT ({len(bert_tokens)} tokens):\")\n",
    "        print(f\"  {bert_tokens}\")\n",
    "    except:\n",
    "        print(\"\\nBERT: –Ω–µ –º–æ–∂–µ –¥–∞ —Å–µ –∑–∞—Ä–µ–¥–∏\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  transformers –Ω–µ –µ –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω\")\n",
    "    print(\"   pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. –û–±–æ–±—â–µ–Ω–∏–µ\n",
    "\n",
    "### üéì –ö–ª—é—á–æ–≤–∏ –∏–∑–≤–æ–¥–∏\n",
    "\n",
    "**1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è—Ç–∞ –µ –∫—Ä–∏—Ç–∏—á–Ω–∞**\n",
    "- –ë–∞–ª–∞–Ω—Å–∏—Ä–∞–º–µ —Ä–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞ vs –¥—ä–ª–∂–∏–Ω–∞ –Ω–∞ seq\n",
    "- Word-level: –ø—Ä–æ—Å—Ç, –Ω–æ OOV –ø—Ä–æ–±–ª–µ–º\n",
    "- Character-level: –Ω—è–º–∞ OOV, –Ω–æ –¥—ä–ª–≥–∏ sequences\n",
    "\n",
    "**2. Subword tokenization –µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ä—Ç**\n",
    "- **BPE:** –ù–∞–π-–ø–æ–ø—É–ª—è—Ä–µ–Ω (GPT, LLaMA)\n",
    "  - Greedy, frequency-based\n",
    "  - –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ–Ω\n",
    "- **Unigram:** –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–Ω (T5)\n",
    "- **WordPiece:** BERT\n",
    "\n",
    "**3. Byte-level BPE**\n",
    "- –†–∞–±–æ—Ç–∏ –Ω–∞ byte –Ω–∏–≤–æ\n",
    "- –ù–∏–∫–æ–≥–∞ –Ω—è–º–∞ UNK tokens\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞ —Å–µ –æ—Ç GPT-2/3/4\n",
    "\n",
    "**4. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏**\n",
    "- Context windows —Å–∞ –≤ –¢–û–ö–ï–ù–ò\n",
    "- API costs –∑–∞–≤–∏—Å—è—Ç –æ—Ç tokens\n",
    "- –ï–∑–∏–∫–æ–≤–∞ –ø—Ä–∏—Å—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç –µ –ø—Ä–æ–±–ª–µ–º\n",
    "\n",
    "### üîú –°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è: –ù–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏\n",
    "\n",
    "**–í–µ—á–µ –∑–Ω–∞–µ–º –∫–∞–∫ –¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–∞–º–µ. –ö–∞–∫–≤–æ —Å–ª–µ–¥–≤–∞?**\n",
    "\n",
    "- –ö–∞–∫ –æ–±—Ä–∞–±–æ—Ç–≤–∞–º–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏ –æ—Ç —Ç–æ–∫–µ–Ω–∏?\n",
    "- –ù–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏ –∏ backpropagation\n",
    "- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞ Transformers!\n",
    "\n",
    "### üìö –ü—Ä–µ–ø–æ—Ä—ä—á–∏—Ç–µ–ª–Ω–æ —á–µ—Ç–µ–Ω–µ\n",
    "\n",
    "1. **\"Neural Machine Translation of Rare Words with Subword Units\"** - Sennrich et al. (2016)\n",
    "2. **\"Language Models are Unsupervised Multitask Learners\"** - Radford et al. (2019) - GPT-2\n",
    "3. **HuggingFace Tokenizers Documentation**\n",
    "4. **OpenAI Tokenizer Tool** - https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## –ö—Ä–∞–π –Ω–∞ –õ–µ–∫—Ü–∏—è 3\n",
    "\n",
    "### –ë–ª–∞–≥–æ–¥–∞—Ä—è –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ—Ç–æ! üéâ\n",
    "\n",
    "**–í—ä–ø—Ä–æ—Å–∏? ü§î**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}