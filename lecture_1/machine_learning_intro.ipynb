{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# –õ–µ–∫—Ü–∏—è 1: –í—ä–≤–µ–¥–µ–Ω–∏–µ –≤ –º–∞—à–∏–Ω–Ω–æ—Ç–æ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞ NLP\n",
    "\n",
    "## –û—Ç –ø—Ä–æ–±–ª–µ–º –¥–æ —Ä–µ—à–µ–Ω–∏–µ: –û—Å–Ω–æ–≤–∏ –Ω–∞ ML —Å —Ç–µ–∫—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏\n",
    "\n",
    "**–ü—Ä–æ–¥—ä–ª–∂–∏—Ç–µ–ª–Ω–æ—Å—Ç:** 2-2.5 —á–∞—Å–∞  \n",
    "**–ü—Ä–µ–¥–ø–æ—Å—Ç–∞–≤–∫–∏:** –û—Å–Ω–æ–≤–∏ –Ω–∞ Python, –ª–∏–Ω–µ–π–Ω–∞ –∞–ª–≥–µ–±—Ä–∞, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏  \n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –ï–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –Ω–∞ –¥—É–º–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives",
   "metadata": {},
   "source": [
    "---\n",
    "## –¶–µ–ª–∏ –Ω–∞ –ª–µ–∫—Ü–∏—è—Ç–∞\n",
    "\n",
    "–°–ª–µ–¥ —Ç–∞–∑–∏ –ª–µ–∫—Ü–∏—è —â–µ –º–æ–∂–µ—Ç–µ:\n",
    "\n",
    "- –§–æ—Ä–º—É–ª–∏—Ä–∞—Ç–µ NLP –∑–∞–¥–∞—á–∞ –∫–∞—Ç–æ ML –ø—Ä–æ–±–ª–µ–º (–≤—Ö–æ–¥, –∏–∑—Ö–æ–¥, —Ö–∏–ø–æ—Ç–µ–∑–∞)\n",
    "- –†–∞–∑–±–∏—Ä–∞—Ç–µ —Ä–∞–∑–ª–∏–∫–∞—Ç–∞ –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª, –±–µ–∑ —É—á–∏—Ç–µ–ª –∏ —Å –ø–æ–¥—Å–∏–ª–≤–∞–Ω–µ\n",
    "- –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–∞—Ç–µ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ (overfitting) –∏ —ä–Ω–¥—ä—Ä—Ñ–∏—Ç–≤–∞–Ω–µ –≤ —Ç–µ–∫—Å—Ç–æ–≤–∏ –º–æ–¥–µ–ª–∏\n",
    "- –ò–∑–±–∏—Ä–∞—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â–∏ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ NLP —Å–∏—Å—Ç–µ–º–∏\n",
    "- –ü—Ä–∏–ª–∞–≥–∞—Ç–µ –æ—Å–Ω–æ–≤–Ω–∏—è ML workflow –∑–∞ —Ç–µ–∫—Å—Ç–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roadmap",
   "metadata": {},
   "source": [
    "### –ü—ä—Ç–Ω–∞ –∫–∞—Ä—Ç–∞ –∑–∞ –¥–Ω–µ—Å\n",
    "\n",
    "```\n",
    "–ú–æ—Ç–∏–≤–∞—Ü–∏—è ‚Üí –§–æ—Ä–º—É–ª–∏—Ä–∞–Ω–µ ‚Üí –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è ‚Üí –ü–∞—Ä–∞–¥–∏–≥–º–∏ ‚Üí –ì–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è ‚Üí –ú–µ—Ç—Ä–∏–∫–∏ ‚Üí –°–ª–µ–¥–≤–∞—â–∏ —Å—Ç—ä–ø–∫–∏\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad7339-ed0c-4350-88f1-f584e9bc27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–∏—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn –∑–∞ ML\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5f02d-9d40-47ae-bfd8-fc6fbcf86bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì –í—Å–∏—á–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏ —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. –ú–æ—Ç–∏–≤–∞—Ü–∏—è: –ó–∞—â–æ ML –∑–∞ –µ–∑–∏–∫?\n",
    "\n",
    "### –ö–∞–∫–≤–æ –µ –∏–∑–∫—É—Å—Ç–≤–µ–Ω –∏–Ω—Ç–µ–ª–µ–∫—Ç, –º–∞—à–∏–Ω–Ω–æ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –∏ NLP?\n",
    "\n",
    "**–ò–∑–∫—É—Å—Ç–≤–µ–Ω –∏–Ω—Ç–µ–ª–µ–∫—Ç (AI):** –®–∏—Ä–æ–∫–∞ –æ–±–ª–∞—Å—Ç, –∫–æ—è—Ç–æ —Å–µ –∑–∞–Ω–∏–º–∞–≤–∞ —Å—ä—Å —Å—ä–∑–¥–∞–≤–∞–Ω–µ—Ç–æ –Ω–∞ –∏–Ω—Ç–µ–ª–∏–≥–µ–Ω—Ç–Ω–∏ –º–∞—à–∏–Ω–∏.\n",
    "\n",
    "**–ú–∞—à–∏–Ω–Ω–æ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ (ML):** –ü–æ–¥—Ö–æ–¥ –≤ AI, –ø—Ä–∏ –∫–æ–π—Ç–æ —Å–∏—Å—Ç–µ–º–∏—Ç–µ —Å–µ —É—á–∞—Ç –æ—Ç –¥–∞–Ω–Ω–∏, –≤–º–µ—Å—Ç–æ –¥–∞ –±—ä–¥–∞—Ç –ø—Ä–æ–≥—Ä–∞–º–∏—Ä–∞–Ω–∏ –∏–∑—Ä–∏—á–Ω–æ.\n",
    "\n",
    "**–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω –µ–∑–∏–∫ (NLP):** –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ ML –∑–∞ —Ä–∞–∑–±–∏—Ä–∞–Ω–µ –∏ –≥–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ —á–æ–≤–µ—à–∫–∏ –µ–∑–∏–∫."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "language-challenges",
   "metadata": {},
   "source": [
    "### –ó–∞—â–æ –µ–∑–∏–∫—ä—Ç –µ —Ç—Ä—É–¥–µ–Ω –∑–∞ –∫–æ–º–ø—é—Ç—Ä–∏—Ç–µ?\n",
    "\n",
    "**–î–≤—É—Å–º–∏—Å–ª–µ–Ω–æ—Å—Ç (Ambiguity)**\n",
    "- \"–í–∏–¥—è—Ö —á–æ–≤–µ–∫ —Å —Ç–µ–ª–µ—Å–∫–æ–ø\" - –ö–æ–π –∏–º–∞ —Ç–µ–ª–µ—Å–∫–æ–ø–∞?\n",
    "- \"–ë–∞–Ω–∫–∞—Ç–∞ –µ –∑–∞—Ç–≤–æ—Ä–µ–Ω–∞\" - –§–∏–Ω–∞–Ω—Å–æ–≤–∞ –∏–Ω—Å—Ç–∏—Ç—É—Ü–∏—è –∏–ª–∏ —Ä–µ—á–µ–Ω –±—Ä—è–≥?\n",
    "\n",
    "**–ö–æ–Ω—Ç–µ–∫—Å—Ç**\n",
    "- \"–°—Ç—Ä–∞—Ö–æ—Ç–Ω–æ\" –º–æ–∂–µ –¥–∞ –µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª–Ω–æ (\"–°—Ç—Ä–∞—Ö–æ—Ç–µ–Ω —Ñ–∏–ª–º!\") –∏–ª–∏ —Å–∞—Ä–∫–∞—Å—Ç–∏—á–Ω–æ (\"–°—Ç—Ä–∞—Ö–æ—Ç–Ω–æ, –∑–∞–∫—ä—Å–Ω—è—Ö –ø–∞–∫...\")\n",
    "\n",
    "**–í–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç**\n",
    "- –ï–¥–Ω–æ –∏ —Å—ä—â–æ –∑–Ω–∞—á–µ–Ω–∏–µ, –º–Ω–æ–≥–æ –Ω–∞—á–∏–Ω–∏ –¥–∞ –≥–æ –∫–∞–∂–µ–º\n",
    "- \"–§–∏–ª–º—ä—Ç –º–∏ —Ö–∞—Ä–µ—Å–∞\" ‚âà \"–°—Ç—Ä–∞—Ö–æ—Ç–µ–Ω —Ñ–∏–ª–º\" ‚âà \"10/10, –ø—Ä–µ–ø–æ—Ä—ä—á–≤–∞–º\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambiguity-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: –ï–¥–Ω–∏ –∏ —Å—ä—â–∏ –¥—É–º–∏, —Ä–∞–∑–ª–∏—á–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "examples = [\n",
    "    (\"–¢–æ–∑–∏ —Ñ–∏–ª–º –µ –Ω–µ–≤–µ—Ä–æ—è—Ç–µ–Ω!\", \"–ø–æ–ª–æ–∂–∏—Ç–µ–ª–Ω–æ\"),\n",
    "    (\"–ù–µ–≤–µ—Ä–æ—è—Ç–Ω–æ, —á–µ –ø—É—Å–Ω–∞—Ö–∞ —Ç–∞–∫—ä–≤ —Ñ–∏–ª–º.\", \"–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª–Ω–æ\"),\n",
    "    (\"–ê–∫—Ç—å–æ—Ä–∏—Ç–µ –∏–≥—Ä–∞—è—Ç –¥–æ–±—Ä–µ.\", \"–ø–æ–ª–æ–∂–∏—Ç–µ–ª–Ω–æ\"),\n",
    "    (\"–ï, –¥–æ–±—Ä–µ –¥–µ... –ø–æ–Ω–µ —Å–≤—ä—Ä—à–∏.\", \"–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª–Ω–æ\"),\n",
    "]\n",
    "\n",
    "print(\"üé≠ –ü—Ä–∏–º–µ—Ä–∏ –∑–∞ –¥–≤—É—Å–º–∏—Å–ª–µ–Ω–æ—Å—Ç –≤ sentiment analysis:\\n\")\n",
    "for text, sentiment in examples:\n",
    "    emoji = \"üòä\" if sentiment == \"–ø–æ–ª–æ–∂–∏—Ç–µ–ª–Ω–æ\" else \"üòû\"\n",
    "    print(f\"{emoji} [{sentiment:12s}] \\\"{text}\\\"\")\n",
    "\n",
    "print(\"\\nüí° –ù–∞–±–ª—é–¥–µ–Ω–∏–µ: –î—É–º–∏ –∫–∞—Ç–æ '–Ω–µ–≤–µ—Ä–æ—è—Ç–µ–Ω' –∏ '–¥–æ–±—Ä–µ' –º–æ–≥–∞—Ç –¥–∞ —Å–∞ –∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª–Ω–∏, –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª–Ω–∏!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "success-stories",
   "metadata": {},
   "source": [
    "### –£—Å–ø–µ—Ö–∏ –Ω–∞ NLP –¥–Ω–µ—Å\n",
    "\n",
    "**–ú–∞—à–∏–Ω–µ–Ω –ø—Ä–µ–≤–æ–¥**\n",
    "- Google Translate, DeepL\n",
    "- –ü—Ä–µ–≤–æ–¥ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –≤ —Ä–µ–∞–ª–Ω–æ –≤—Ä–µ–º–µ\n",
    "\n",
    "**–†–∞–∑–≥–æ–≤–æ—Ä–Ω–∏ –∞—Å–∏—Å—Ç–µ–Ω—Ç–∏**\n",
    "- ChatGPT, Claude, Gemini\n",
    "- –û—Ç–≥–æ–≤–∞—Ä—è—Ç –Ω–∞ –≤—ä–ø—Ä–æ—Å–∏, –ø–∏—à–∞—Ç –∫–æ–¥, –≥–µ–Ω–µ—Ä–∏—Ä–∞—Ç —Ç–µ–∫—Å—Ç\n",
    "\n",
    "**–ë–∏–∑–Ω–µ—Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è**\n",
    "- Sentiment analysis –∑–∞ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏ –æ—Ç–∑–∏–≤–∏\n",
    "- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –∏–º–µ–π–ª–∏\n",
    "- –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. –§–æ—Ä–º—É–ª–∏—Ä–∞–Ω–µ –Ω–∞ ML –ø—Ä–æ–±–ª–µ–º\n",
    "\n",
    "### –ö–∞–∫–≤–æ –æ–∑–Ω–∞—á–∞–≤–∞ \"–º–∞—à–∏–Ω–∞ —Å–µ —É—á–∏\"?\n",
    "\n",
    "**–î–µ—Ñ–∏–Ω–∏—Ü–∏—è (Tom Mitchell, 1997):**\n",
    "\n",
    "> –ö–æ–º–ø—é—Ç—ä—Ä–Ω–∞ –ø—Ä–æ–≥—Ä–∞–º–∞ —Å–µ —É—á–∏ –æ—Ç –æ–ø–∏—Ç **E** –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –Ω–∞ –∑–∞–¥–∞—á–∞ **T** –∏ –º–µ—Ç—Ä–∏–∫–∞ **P**, –∞–∫–æ –Ω–µ–π–Ω–∞—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–Ω–æ—Å—Ç –ø–æ **T**, –∏–∑–º–µ—Ä–µ–Ω–∞ —Å **P**, —Å–µ –ø–æ–¥–æ–±—Ä—è–≤–∞ —Å –æ–ø–∏—Ç–∞ **E**.\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä: Spam —Ñ–∏–ª—Ç—ä—Ä**\n",
    "- **T (–ó–∞–¥–∞—á–∞):** –ö–ª–∞—Å–∏—Ñ–∏—Ü–∏—Ä–∞–π –∏–º–µ–π–ª –∫–∞—Ç–æ spam –∏–ª–∏ –Ω–µ-spam\n",
    "- **E (–û–ø–∏—Ç):** –ë–∞–∑–∞ –æ—Ç –µ—Ç–∏–∫–µ—Ç–∏—Ä–∞–Ω–∏ –∏–º–µ–π–ª–∏\n",
    "- **P (–ú–µ—Ç—Ä–∏–∫–∞):** –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª–Ω–æ –∫–ª–∞—Å–∏—Ñ–∏—Ü–∏—Ä–∞–Ω–∏ –∏–º–µ–π–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-notation",
   "metadata": {},
   "source": [
    "### –§–æ—Ä–º–∞–ª–Ω–∞ –Ω–æ—Ç–∞—Ü–∏—è\n",
    "\n",
    "**–í—Ö–æ–¥–Ω–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ** $\\mathcal{X}$: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –æ—Ç –≤—Å–∏—á–∫–∏ –≤—ä–∑–º–æ–∂–Ω–∏ –≤—Ö–æ–¥–æ–≤–µ (—Ç–µ–∫—Å—Ç–æ–≤–µ, –∏–∑—Ä–µ—á–µ–Ω–∏—è, –¥–æ–∫—É–º–µ–Ω—Ç–∏)\n",
    "\n",
    "**–ò–∑—Ö–æ–¥–Ω–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ** $\\mathcal{Y}$: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –æ—Ç –≤—ä–∑–º–æ–∂–Ω–∏ –∏–∑—Ö–æ–¥–∏ (–µ—Ç–∏–∫–µ—Ç–∏, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –ø—Ä–µ–≤–æ–¥–∏)\n",
    "\n",
    "**–¢—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏ –ø—Ä–∏–º–µ—Ä–∏:** $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})$\n",
    "\n",
    "**–•–∏–ø–æ—Ç–µ–∑–∞ (–º–æ–¥–µ–ª):** $h: \\mathcal{X} \\to \\mathcal{Y}$\n",
    "\n",
    "**–¶–µ–ª:** –ù–∞–º–µ—Ä–∏ —Ö–∏–ø–æ—Ç–µ–∑–∞ $h$, –∫–æ—è—Ç–æ –¥–æ–±—Ä–µ –ø—Ä–µ–¥—Å–∫–∞–∑–≤–∞ $y$ –∑–∞ –Ω–æ–≤–∏ $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spam-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–Ω–∫—Ä–µ—Ç–µ–Ω –ø—Ä–∏–º–µ—Ä: Spam –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "spam_examples = [\n",
    "    (\"–ß–µ—Å—Ç–∏—Ç–æ! –°–ø–µ—á–µ–ª–∏—Ö—Ç–µ $1,000,000! –ö–ª–∏–∫–Ω–µ—Ç–µ —Ç—É–∫!\", \"spam\"),\n",
    "    (\"–ó–¥—Ä–∞–≤–µ–π, —É—Ç—Ä–µ —â–µ —Å–µ –≤–∏–¥–∏–º –∑–∞ –∫–∞—Ñ–µ?\", \"not spam\"),\n",
    "    (\"–ë–ï–ó–ü–õ–ê–¢–ù–û!!! –ò–∑—Ç–µ–≥–ª–µ—Ç–µ —Å–µ–≥–∞!!!\", \"spam\"),\n",
    "    (\"–ò–∑–ø—Ä–∞—â–∞–º —Ç–∏ —Ñ–∞–π–ª–æ–≤–µ—Ç–µ –æ—Ç —Å—Ä–µ—â–∞—Ç–∞.\", \"not spam\"),\n",
    "    (\"–ö—É–ø–∏ –í–∏–∞–≥—Ä–∞ –Ω–∞ –Ω–∏—Å–∫–∏ —Ü–µ–Ω–∏!\", \"spam\"),\n",
    "    (\"–ü–æ—Ç–≤—ä—Ä–∂–¥–∞–≤–∞–º —É—á–∞—Å—Ç–∏–µ –≤ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è—Ç–∞.\", \"not spam\"),\n",
    "]\n",
    "\n",
    "print(\"üìß –¢—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏ –¥–∞–Ω–Ω–∏ –∑–∞ spam –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:\\n\")\n",
    "print(f\"{'–¢–µ–∫—Å—Ç':<50} {'–ï—Ç–∏–∫–µ—Ç':>10}\")\n",
    "print(\"=\" * 62)\n",
    "\n",
    "for text, label in spam_examples:\n",
    "    short_text = text[:47] + \"...\" if len(text) > 50 else text\n",
    "    print(f\"{short_text:<50} {label:>10}\")\n",
    "\n",
    "print(f\"\\nüí° –¢–æ–≤–∞ –µ X (–≤—Ö–æ–¥–æ–≤–µ) –∏ Y (–µ—Ç–∏–∫–µ—Ç–∏). –ú–æ–¥–µ–ª—ä—Ç —Ç—Ä—è–±–≤–∞ –¥–∞ –Ω–∞—É—á–∏ –≤—Ä—ä–∑–∫–∞—Ç–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. –û—Å–Ω–æ–≤–Ω–∞ ML —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è —Å —Ç–µ–∫—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏\n",
    "\n",
    "### –ü—Ä–∏–∑–Ω–∞—Ü–∏ (Features) –≤ NLP\n",
    "\n",
    "–ö–æ–º–ø—é—Ç—Ä–∏—Ç–µ —Ä–∞–±–æ—Ç—è—Ç —Å —á–∏—Å–ª–∞, –Ω–µ —Å —Ç–µ–∫—Å—Ç. –¢—Ä—è–±–≤–∞ –¥–∞ **–ø—Ä–µ–≤—ä—Ä–Ω–µ–º —Ç–µ–∫—Å—Ç–∞ –≤ —á–∏—Å–ª–∞**.\n",
    "\n",
    "**–ü–æ–¥—Ö–æ–¥–∏ –∑–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –Ω–∞ —Ç–µ–∫—Å—Ç:**\n",
    "- **Bag-of-Words (BoW):** –ë—Ä–æ–∏ –ø–æ—è–≤—è–≤–∞–Ω–∏—è –Ω–∞ –¥—É–º–∏\n",
    "- **TF-IDF:** –ü—Ä–µ—Ç–µ–≥–ª–µ–Ω–æ –±—Ä–æ–µ–Ω–µ (—Ä—è–¥–∫–∏—Ç–µ –¥—É–º–∏ —Ç–µ–∂–∞—Ç –ø–æ–≤–µ—á–µ)\n",
    "- **N-–≥—Ä–∞–º–∏:** –ë—Ä–æ–∏ –ø–æ—Ä–µ–¥–∏—Ü–∏ –æ—Ç N –¥—É–º–∏\n",
    "- **Embeddings:** –ü–ª—ä—Ç–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏ (–õ–µ–∫—Ü–∏—è 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bow-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: Bag-of-Words\n",
    "texts = [\n",
    "    \"–û–±–∏—á–∞–º —Ç–æ–∑–∏ —Ñ–∏–ª–º\",\n",
    "    \"–ú—Ä–∞–∑—è —Ç–æ–∑–∏ —Ñ–∏–ª–º\",\n",
    "    \"–§–∏–ª–º—ä—Ç –µ —Å—Ç—Ä–∞—Ö–æ—Ç–µ–Ω\"\n",
    "]\n",
    "\n",
    "# –°—ä–∑–¥–∞–≤–∞–º–µ BoW –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# –ü–æ–∫–∞–∑–≤–∞–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∞\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=feature_names, index=texts)\n",
    "\n",
    "print(\"üìä Bag-of-Words –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ:\\n\")\n",
    "print(bow_df.to_string())\n",
    "\n",
    "print(\"\\nüí° –í—Å—è–∫–∞ –∫–æ–ª–æ–Ω–∞ –µ –ø—Ä–∏–∑–Ω–∞–∫ (feature), –≤—Å–µ–∫–∏ —Ä–µ–¥ –µ –¥–æ–∫—É–º–µ–Ω—Ç.\")\n",
    "print(\"   –ó–∞–±–µ–ª–µ–∂–µ—Ç–µ: –≥—É–±–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞ —Ä–µ–¥–∞ –Ω–∞ –¥—É–º–∏—Ç–µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bow-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–±–ª–µ–º —Å Bag-of-Words: –≥—É–±–∏–º —Ä–µ–¥\n",
    "sentence1 = \"–∫—É—á–µ—Ç–æ –≥–æ–Ω–∏ –∫–æ—Ç–∫–∞—Ç–∞\"\n",
    "sentence2 = \"–∫–æ—Ç–∫–∞—Ç–∞ –≥–æ–Ω–∏ –∫—É—á–µ—Ç–æ\"\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([sentence1, sentence2])\n",
    "\n",
    "print(\"‚ùå –ü—Ä–æ–±–ª–µ–º: Bag-of-Words –≥—É–±–∏ —Ä–µ–¥–∞ –Ω–∞ –¥—É–º–∏—Ç–µ\\n\")\n",
    "print(f\"–ò–∑—Ä–µ—á–µ–Ω–∏–µ 1: '{sentence1}'\")\n",
    "print(f\"–ò–∑—Ä–µ—á–µ–Ω–∏–µ 2: '{sentence2}'\")\n",
    "print(f\"\\n–í–µ–∫—Ç–æ—Ä–∏: {X.toarray()}\")\n",
    "print(f\"–ï–¥–Ω–∞–∫–≤–∏ –ª–∏ —Å–∞? {np.array_equal(X[0].toarray(), X[1].toarray())}\")\n",
    "\n",
    "print(\"\\nüí° –†–∞–∑–ª–∏—á–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ, –Ω–æ –°–™–©–û–¢–û –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ!\")\n",
    "print(\"   –¢–æ–≤–∞ –µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ BoW. –©–µ –≤–∏–¥–∏–º —Ä–µ—à–µ–Ω–∏—è –≤ –õ–µ–∫—Ü–∏–∏ 2-5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labels-section",
   "metadata": {},
   "source": [
    "### –ï—Ç–∏–∫–µ—Ç–∏ (Labels)\n",
    "\n",
    "**–¢–∏–ø–æ–≤–µ –µ—Ç–∏–∫–µ—Ç–∏ –≤ NLP:**\n",
    "\n",
    "| –ó–∞–¥–∞—á–∞ | –ï—Ç–∏–∫–µ—Ç–∏ | –ü—Ä–∏–º–µ—Ä |\n",
    "|--------|---------|--------|\n",
    "| Sentiment analysis | {positive, negative, neutral} | \"–°—Ç—Ä–∞—Ö–æ—Ç–µ–Ω —Ñ–∏–ª–º!\" ‚Üí positive |\n",
    "| Spam detection | {spam, not spam} | \"–ë–ï–ó–ü–õ–ê–¢–ù–û!!!\" ‚Üí spam |\n",
    "| Topic classification | {sports, politics, tech, ...} | \"–õ–µ–≤—Å–∫–∏ –ø–æ–±–µ–¥–∏ –¶–°–ö–ê\" ‚Üí sports |\n",
    "| Named Entity Recognition | {B-PER, I-PER, B-ORG, O, ...} | \"–ò–≤–∞–Ω\" ‚Üí B-PER |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-splits",
   "metadata": {},
   "source": [
    "### –†–∞–∑–¥–µ–ª—è–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏—Ç–µ\n",
    "\n",
    "**–ó–∞—â–æ —Ä–∞–∑–¥–µ–ª—è–º–µ?** –ò—Å–∫–∞–º–µ –¥–∞ –æ—Ü–µ–Ω–∏–º –∫–∞–∫ –º–æ–¥–µ–ª—ä—Ç —Ä–∞–±–æ—Ç–∏ –Ω–∞ –ù–û–í–ò –¥–∞–Ω–Ω–∏, –∫–æ–∏—Ç–æ –Ω–µ –µ –≤–∏–∂–¥–∞–ª.\n",
    "\n",
    "**–¢—Ä–∏ –Ω–∞–±–æ—Ä–∞:**\n",
    "\n",
    "| –ù–∞–±–æ—Ä | –¶–µ–ª | –¢–∏–ø–∏—á–µ–Ω —Ä–∞–∑–º–µ—Ä |\n",
    "|-------|-----|----------------|\n",
    "| **–¢—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–µ–Ω (Train)** | –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–æ–¥–µ–ª–∞ | 60-80% |\n",
    "| **–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–µ–Ω (Validation)** | –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ —Ö–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ | 10-20% |\n",
    "| **–¢–µ—Å—Ç–æ–≤ (Test)** | –§–∏–Ω–∞–ª–Ω–∞ –æ—Ü–µ–Ω–∫–∞ | 10-20% |\n",
    "\n",
    "**–ó–ª–∞—Ç–Ω–æ –ø—Ä–∞–≤–∏–ª–æ:** –ù–∏–∫–æ–≥–∞ –Ω–µ —Ç—Ä–µ–Ω–∏—Ä–∞–π—Ç–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ —Ä–µ–∞–ª–µ–Ω dataset: 20 Newsgroups (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)\n",
    "categories = ['rec.sport.baseball', 'sci.space', 'comp.graphics', 'talk.politics.misc']\n",
    "\n",
    "print(\"üì• –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ 20 Newsgroups dataset...\\n\")\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "texts = newsgroups.data\n",
    "labels = newsgroups.target\n",
    "label_names = newsgroups.target_names\n",
    "\n",
    "print(f\"–ë—Ä–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∏: {len(texts)}\")\n",
    "print(f\"–ë—Ä–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {len(label_names)}\")\n",
    "print(f\"\\n–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:\")\n",
    "for i, name in enumerate(label_names):\n",
    "    count = sum(labels == i)\n",
    "    print(f\"  {i}: {name} ({count} –¥–æ–∫—É–º–µ–Ω—Ç–∞)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-example-doc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–∫–∞–∑–≤–∞–º–µ –ø—Ä–∏–º–µ—Ä–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç\n",
    "sample_idx = 42\n",
    "sample_text = texts[sample_idx]\n",
    "sample_label = label_names[labels[sample_idx]]\n",
    "\n",
    "print(f\"üìÑ –ü—Ä–∏–º–µ—Ä–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç (–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {sample_label})\\n\")\n",
    "print(\"=\" * 70)\n",
    "# –ü–æ–∫–∞–∑–≤–∞–º–µ –ø—ä—Ä–≤–∏—Ç–µ 500 —Å–∏–º–≤–æ–ª–∞\n",
    "print(sample_text[:500] + \"...\" if len(sample_text) > 500 else sample_text)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª—è–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏—Ç–µ\n",
    "# –ü—ä—Ä–≤–æ: train + temp (test + validation)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# –ü–æ—Å–ª–µ: —Ä–∞–∑–¥–µ–ª—è–º–µ temp –Ω–∞ validation –∏ test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"üìä –†–∞–∑–¥–µ–ª—è–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏—Ç–µ:\\n\")\n",
    "print(f\"  –¢—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–µ–Ω –Ω–∞–±–æ—Ä: {len(X_train)} –¥–æ–∫—É–º–µ–Ω—Ç–∞ ({len(X_train)/len(texts)*100:.0f}%)\")\n",
    "print(f\"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–µ–Ω –Ω–∞–±–æ—Ä: {len(X_val)} –¥–æ–∫—É–º–µ–Ω—Ç–∞ ({len(X_val)/len(texts)*100:.0f}%)\")\n",
    "print(f\"  –¢–µ—Å—Ç–æ–≤ –Ω–∞–±–æ—Ä:       {len(X_test)} –¥–æ–∫—É–º–µ–Ω—Ç–∞ ({len(X_test)/len(texts)*100:.0f}%)\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "print(f\"\\nüí° –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è (—Ä–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –∫–ª–∞—Å–æ–≤–µ):\")\n",
    "print(f\"   Train: {np.bincount(y_train)}\")\n",
    "print(f\"   Val:   {np.bincount(y_val)}\")\n",
    "print(f\"   Test:  {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. –ü–∞—Ä–∞–¥–∏–≥–º–∏ –Ω–∞ –º–∞—à–∏–Ω–Ω–æ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "### –¢—Ä–∏ –æ—Å–Ω–æ–≤–Ω–∏ –ø–∞—Ä–∞–¥–∏–≥–º–∏\n",
    "\n",
    "| –ü–∞—Ä–∞–¥–∏–≥–º–∞ | –î–∞–Ω–Ω–∏ | –¶–µ–ª | NLP –ø—Ä–∏–º–µ—Ä–∏ |\n",
    "|-----------|-------|-----|-------------|\n",
    "| **–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª** | X + Y | –ü—Ä–µ–¥—Å–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ Y –æ—Ç X | –ö–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, NER, –ø—Ä–µ–≤–æ–¥ |\n",
    "| **–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª** | –°–∞–º–æ X | –û—Ç–∫—Ä–∏–≤–∞–Ω–µ –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ | Topic modeling, clustering |\n",
    "| **–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥—Å–∏–ª–≤–∞–Ω–µ** | –°—ä—Å—Ç–æ—è–Ω–∏—è + –Ω–∞–≥—Ä–∞–¥–∏ | –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ –Ω–∞–≥—Ä–∞–¥–∞ | RLHF (–õ–µ–∫—Ü–∏—è 8) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supervised-learning",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª (Supervised Learning)\n",
    "\n",
    "**–ò–º–∞–º–µ:** –¢—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏ –ø—Ä–∏–º–µ—Ä–∏ —Å –µ—Ç–∏–∫–µ—Ç–∏ $(x^{(i)}, y^{(i)})$\n",
    "\n",
    "**–ò—Å–∫–∞–º–µ:** –ú–æ–¥–µ–ª $h(x)$, –∫–æ–π—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–≤–∞ $y$ –∑–∞ –Ω–æ–≤–∏ $x$\n",
    "\n",
    "**NLP –∑–∞–¥–∞—á–∏:**\n",
    "- **–ö–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ —Ç–µ–∫—Å—Ç:** Sentiment, spam, topic\n",
    "- **Sequence labeling:** NER, POS tagging\n",
    "- **Seq2seq:** –ú–∞—à–∏–Ω–µ–Ω –ø—Ä–µ–≤–æ–¥, summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-classifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Å—Ç —Ç–µ–∫—Å—Ç–æ–≤ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\n",
    "print(\"üéØ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\\n\")\n",
    "\n",
    "# –°—Ç—ä–ø–∫–∞ 1: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (—Ç–µ–∫—Å—Ç ‚Üí —á–∏—Å–ª–∞)\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"–°—Ç—ä–ø–∫–∞ 1: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\")\n",
    "print(f\"  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç –Ω–∞ –≤—Ö–æ–¥–∞: {X_train_vec.shape}\")\n",
    "print(f\"  (–¥–æ–∫—É–º–µ–Ω—Ç–∏ x –ø—Ä–∏–∑–Ω–∞—Ü–∏)\")\n",
    "\n",
    "# –°—Ç—ä–ø–∫–∞ 2: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–æ–¥–µ–ª\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "print(f\"\\n–°—Ç—ä–ø–∫–∞ 2: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ Logistic Regression\")\n",
    "print(f\"  ‚úì –ú–æ–¥–µ–ª—ä—Ç –µ –æ–±—É—á–µ–Ω!\")\n",
    "\n",
    "# –°—Ç—ä–ø–∫–∞ 3: –ü—Ä–µ–¥—Å–∫–∞–∑–≤–∞–Ω–µ\n",
    "y_train_pred = model.predict(X_train_vec)\n",
    "y_val_pred = model.predict(X_val_vec)\n",
    "\n",
    "print(f\"\\n–°—Ç—ä–ø–∫–∞ 3: –û—Ü–µ–Ω–∫–∞\")\n",
    "print(f\"  Train accuracy: {accuracy_score(y_train, y_train_pred):.3f}\")\n",
    "print(f\"  Val accuracy:   {accuracy_score(y_val, y_val_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–∫–∞–∑–≤–∞–º–µ –Ω—è–∫–æ–ª–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "print(\"üîÆ –ü—Ä–∏–º–µ—Ä–Ω–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    text = X_val[i][:100] + \"...\" if len(X_val[i]) > 100 else X_val[i]\n",
    "    true_label = label_names[y_val[i]]\n",
    "    pred_label = label_names[y_val_pred[i]]\n",
    "    correct = \"‚úì\" if y_val[i] == y_val_pred[i] else \"‚úó\"\n",
    "    \n",
    "    print(f\"{correct} –¢–µ–∫—Å—Ç: \\\"{text}\\\"\")\n",
    "    print(f\"  –ò—Å—Ç–∏–Ω—Å–∫–∏: {true_label}, –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω: {pred_label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsupervised-learning",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª (Unsupervised Learning)\n",
    "\n",
    "**–ò–º–∞–º–µ:** –°–∞–º–æ –¥–∞–Ω–Ω–∏ $x^{(i)}$ (–±–µ–∑ –µ—Ç–∏–∫–µ—Ç–∏)\n",
    "\n",
    "**–ò—Å–∫–∞–º–µ:** –û—Ç–∫—Ä–∏–µ–º —Å–∫—Ä–∏—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ –¥–∞–Ω–Ω–∏—Ç–µ\n",
    "\n",
    "**NLP –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è:**\n",
    "- **Topic modeling:** –û—Ç–∫—Ä–∏–≤–∞–Ω–µ –Ω–∞ —Ç–µ–º–∏ –≤ –∫–æ–ª–µ–∫—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∏\n",
    "- **Clustering:** –ì—Ä—É–ø–∏—Ä–∞–Ω–µ –Ω–∞ —Å—Ö–æ–¥–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏\n",
    "- **Word embeddings:** –ü—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –Ω–∞ –¥—É–º–∏ –∫–∞—Ç–æ –≤–µ–∫—Ç–æ—Ä–∏ (–õ–µ–∫—Ü–∏—è 2)\n",
    "- **Language modeling:** –ü—Ä–µ–¥–≤–∏–∂–¥–∞–Ω–µ –Ω–∞ —Å–ª–µ–¥–≤–∞—â–∞ –¥—É–º–∞ (–æ—Å–Ω–æ–≤–∞ –Ω–∞ LLM!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reinforcement-learning",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥—Å–∏–ª–≤–∞–Ω–µ (Reinforcement Learning)\n",
    "\n",
    "**–ò–º–∞–º–µ:** –ê–≥–µ–Ω—Ç, –∫–æ–π—Ç–æ –≤–∑–∏–º–∞ –¥–µ–π—Å—Ç–≤–∏—è –∏ –ø–æ–ª—É—á–∞–≤–∞ –Ω–∞–≥—Ä–∞–¥–∏\n",
    "\n",
    "**–ò—Å–∫–∞–º–µ:** –°—Ç—Ä–∞—Ç–µ–≥–∏—è, –∫–æ—è—Ç–æ –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–∞ –æ–±—â–∞—Ç–∞ –Ω–∞–≥—Ä–∞–¥–∞\n",
    "\n",
    "**–í NLP: RLHF (Reinforcement Learning from Human Feedback)**\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞ —Å–µ –∑–∞ alignment –Ω–∞ LLM (ChatGPT, Claude)\n",
    "- –ß–æ–≤–µ–∫ –æ—Ü–µ–Ω—è–≤–∞ –æ—Ç–≥–æ–≤–æ—Ä–∏—Ç–µ –Ω–∞ –º–æ–¥–µ–ª–∞\n",
    "- –ú–æ–¥–µ–ª—ä—Ç —Å–µ –æ–±—É—á–∞–≤–∞ –¥–∞ –≥–µ–Ω–µ—Ä–∏—Ä–∞ –ø–æ-–¥–æ–±—Ä–µ –æ—Ü–µ–Ω–µ–Ω–∏ –æ—Ç–≥–æ–≤–æ—Ä–∏\n",
    "\n",
    "üìå **–©–µ –≤–∏–¥–∏–º RLHF –≤ –¥–µ—Ç–∞–π–ª–∏ –≤ –õ–µ–∫—Ü–∏—è 8!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. –ì–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è: –ü—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ –∏ —ä–Ω–¥—ä—Ä—Ñ–∏—Ç–≤–∞–Ω–µ\n",
    "\n",
    "### –¶–µ–Ω—Ç—Ä–∞–ª–Ω–∏—è—Ç –ø—Ä–æ–±–ª–µ–º –≤ ML\n",
    "\n",
    "**–¶–µ–ª:** –ú–æ–¥–µ–ª, –∫–æ–π—Ç–æ —Ä–∞–±–æ—Ç–∏ –¥–æ–±—Ä–µ –Ω–∞ –ù–û–í–ò –¥–∞–Ω–Ω–∏ (–Ω–µ —Å–∞–º–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏—Ç–µ)\n",
    "\n",
    "**–î–≤–∞ –Ω–∞—á–∏–Ω–∞ –¥–∞ —Å–µ –ø—Ä–æ–≤–∞–ª–∏–º:**\n",
    "\n",
    "| –ü—Ä–æ–±–ª–µ–º | –û–ø–∏—Å–∞–Ω–∏–µ | –°–∏–º–ø—Ç–æ–º |\n",
    "|---------|----------|--------|\n",
    "| **–™–Ω–¥—ä—Ä—Ñ–∏—Ç–≤–∞–Ω–µ** | –ú–æ–¥–µ–ª —Ç–≤—ä—Ä–¥–µ –ø—Ä–æ—Å—Ç | –õ–æ—à–∞ —Ç–æ—á–Ω–æ—Å—Ç –Ω–∞–≤—Å—è–∫—ä–¥–µ |\n",
    "| **–ü—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ** | –ú–æ–¥–µ–ª –∑–∞–ø–æ–º–Ω—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏ –¥–∞–Ω–Ω–∏ | –í–∏—Å–æ–∫–∞ train, –Ω–∏—Å–∫–∞ test —Ç–æ—á–Ω–æ—Å—Ç |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overfitting-text",
   "metadata": {},
   "source": [
    "### –ü—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ –≤ —Ç–µ–∫—Å—Ç–æ–≤–∏ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "**–ö–∞–∫–≤–æ –æ–∑–Ω–∞—á–∞–≤–∞ –º–æ–¥–µ–ª –¥–∞ \"–∑–∞–ø–æ–º–Ω–∏\" —Ç–µ–∫—Å—Ç?**\n",
    "\n",
    "- –ù–∞—É—á–∞–≤–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏ —Ñ—Ä–∞–∑–∏ –æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏\n",
    "- –ù–µ —Ä–∞–∑–ø–æ–∑–Ω–∞–≤–∞ –ø–µ—Ä–∏—Ñ—Ä–∞–∑–∏—Ä–∞–Ω–∏—è\n",
    "- –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫—ä–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ–Ω —Ä–µ—á–Ω–∏–∫\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "- Train: \"–°—Ç—Ä–∞—Ö–æ—Ç–µ–Ω —Ñ–∏–ª–º, –ø—Ä–µ–ø–æ—Ä—ä—á–≤–∞–º!\" ‚Üí positive\n",
    "- Test: \"–ù–µ–≤–µ—Ä–æ—è—Ç–µ–Ω —Ñ–∏–ª–º, –≥–ª–µ–¥–∞–π—Ç–µ –≥–æ!\" ‚Üí ??? (–Ω–æ–≤–∏ –¥—É–º–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overfitting-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ —Å —Ä–∞–∑–ª–∏—á–Ω–∏ —Ä–∞–∑–º–µ—Ä–∏ –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞\n",
    "print(\"üìà –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ\\n\")\n",
    "\n",
    "# –í–∑–∏–º–∞–º–µ –º–∞–ª—ä–∫ subset –∑–∞ –ø–æ-—è—Å–Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
    "n_samples = 500\n",
    "X_train_small = X_train[:n_samples]\n",
    "y_train_small = y_train[:n_samples]\n",
    "\n",
    "vocab_sizes = [100, 500, 1000, 2000, 5000, 10000]\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å —Ä–∞–∑–ª–∏—á–µ–Ω —Ä–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞\n",
    "    vec = TfidfVectorizer(max_features=vocab_size, stop_words='english')\n",
    "    X_tr = vec.fit_transform(X_train_small)\n",
    "    X_v = vec.transform(X_val)\n",
    "    \n",
    "    # –û–±—É—á–µ–Ω–∏–µ\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    clf.fit(X_tr, y_train_small)\n",
    "    \n",
    "    # –û—Ü–µ–Ω–∫–∞\n",
    "    train_acc = accuracy_score(y_train_small, clf.predict(X_tr))\n",
    "    val_acc = accuracy_score(y_val, clf.predict(X_v))\n",
    "    \n",
    "    train_scores.append(train_acc)\n",
    "    val_scores.append(val_acc)\n",
    "    \n",
    "    print(f\"Vocab size: {vocab_size:5d} | Train: {train_acc:.3f} | Val: {val_acc:.3f}\")\n",
    "\n",
    "print(\"\\nüí° –ó–∞–±–µ–ª–µ–∂–µ—Ç–µ: –° —É–≤–µ–ª–∏—á–∞–≤–∞–Ω–µ –Ω–∞ vocab_size, train accuracy —Ä–∞—Å—Ç–µ,\")\n",
    "print(\"   –Ω–æ val accuracy –Ω–∞ –Ω—è–∫—ä–¥–µ —Å–ø–∏—Ä–∞ –¥–∞ —Å–µ –ø–æ–¥–æ–±—Ä—è–≤–∞ –∏–ª–∏ –¥–æ—Ä–∏ –ø–∞–¥–∞!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overfitting-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(vocab_sizes, train_scores, 'o-', label='–¢—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–µ–Ω –Ω–∞–±–æ—Ä', linewidth=2, markersize=8)\n",
    "plt.plot(vocab_sizes, val_scores, 's-', label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–µ–Ω –Ω–∞–±–æ—Ä', linewidth=2, markersize=8)\n",
    "\n",
    "# –ó–æ–Ω–∞ –Ω–∞ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ\n",
    "gap = np.array(train_scores) - np.array(val_scores)\n",
    "max_gap_idx = np.argmax(gap)\n",
    "plt.axvline(x=vocab_sizes[max_gap_idx], color='red', linestyle='--', alpha=0.5, \n",
    "            label='–ú–∞–∫—Å–∏–º–∞–ª–Ω–∞ —Ä–∞–∑–ª–∏–∫–∞ (–ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ)')\n",
    "\n",
    "plt.xlabel('–†–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞ (–±—Ä–æ–π –ø—Ä–∏–∑–Ω–∞—Ü–∏)', fontsize=12)\n",
    "plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç (Accuracy)', fontsize=12)\n",
    "plt.title('–ü—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ: Train vs Validation Accuracy', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"   ‚Ä¢ –ì–æ–ª—è–º–∞ —Ä–∞–∑–ª–∏–∫–∞ –º–µ–∂–¥—É train –∏ val = –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ\")\n",
    "print(\"   ‚Ä¢ –û–ø—Ç–∏–º–∞–ª–Ω–∏—è—Ç vocab_size –±–∞–ª–∞–Ω—Å–∏—Ä–∞ –º–µ–∂–¥—É –¥–≤–µ—Ç–µ –∫—Ä–∏–≤–∏\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves: –∫–∞–∫ —Å–µ –ø—Ä–æ–º–µ–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—Ç–∞ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ—Ç–æ –¥–∞–Ω–Ω–∏\n",
    "print(\"üìä –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ learning curves...\\n\")\n",
    "\n",
    "# –§–∏–∫—Å–∏—Ä–∞–º–µ vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "X_all_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "train_sizes, train_scores_lc, val_scores_lc = learning_curve(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    X_all_vec, y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# –°—Ä–µ–¥–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è\n",
    "train_mean = train_scores_lc.mean(axis=1)\n",
    "train_std = train_scores_lc.std(axis=1)\n",
    "val_mean = val_scores_lc.mean(axis=1)\n",
    "val_std = val_scores_lc.std(axis=1)\n",
    "\n",
    "print(\"‚úì Learning curves –∏–∑—á–∏—Å–ª–µ–Ω–∏!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning-curves-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(train_sizes, train_mean, 'o-', label='–¢—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–µ–Ω –Ω–∞–±–æ—Ä', linewidth=2)\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "\n",
    "plt.plot(train_sizes, val_mean, 's-', label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–µ–Ω –Ω–∞–±–æ—Ä (CV)', linewidth=2)\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
    "\n",
    "plt.xlabel('–ë—Ä–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏ –ø—Ä–∏–º–µ—Ä–∏', fontsize=12)\n",
    "plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç (Accuracy)', fontsize=12)\n",
    "plt.title('Learning Curves: –ö–∞–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ—Ç–æ –¥–∞–Ω–Ω–∏ –≤–ª–∏—è–µ –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—Ç–∞', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"   ‚Ä¢ –° –ø–æ–≤–µ—á–µ –¥–∞–Ω–Ω–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—Ç–∞ —Ç–æ—á–Ω–æ—Å—Ç —Å–µ –ø–æ–¥–æ–±—Ä—è–≤–∞\")\n",
    "print(\"   ‚Ä¢ –†–∞–∑–ª–∏–∫–∞—Ç–∞ –º–µ–∂–¥—É –∫—Ä–∏–≤–∏—Ç–µ –Ω–∞–º–∞–ª—è–≤–∞ (–ø–æ-–º–∞–ª–∫–æ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ)\")\n",
    "print(\"   ‚Ä¢ –ê–∫–æ –∫—Ä–∏–≤–∏—Ç–µ —Å–µ —Å–±–ª–∏–∂–∞–≤–∞—Ç ‚Üí –¥–æ–±—Ä–∞ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bias-variance",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff\n",
    "\n",
    "**–ì—Ä–µ—à–∫–∞—Ç–∞ –Ω–∞ –º–æ–¥–µ–ª–∞ —Å–µ —Ä–∞–∑–¥–µ–ª—è –Ω–∞:**\n",
    "\n",
    "$$\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}$$\n",
    "\n",
    "| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –û–ø–∏—Å–∞–Ω–∏–µ | –†–µ–∑—É–ª—Ç–∞—Ç –æ—Ç |\n",
    "|-----------|----------|-------------|\n",
    "| **Bias** | –°–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–∞ –≥—Ä–µ—à–∫–∞ | –¢–≤—ä—Ä–¥–µ –ø—Ä–æ—Å—Ç –º–æ–¥–µ–ª (—ä–Ω–¥—ä—Ä—Ñ–∏—Ç–≤–∞–Ω–µ) |\n",
    "| **Variance** | –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–Ω–æ—Å—Ç –∫—ä–º –¥–∞–Ω–Ω–∏ | –¢–≤—ä—Ä–¥–µ —Å–ª–æ–∂–µ–Ω –º–æ–¥–µ–ª (–ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ) |\n",
    "| **Noise** | –®—É–º –≤ –¥–∞–Ω–Ω–∏—Ç–µ | –ù–µ–∏–∑–±–µ–∂–µ–Ω |\n",
    "\n",
    "**–¶–µ–ª—Ç–∞:** –ù–∞–º–µ—Ä–µ—Ç–µ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É bias –∏ variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ NLP –º–æ–¥–µ–ª–∏\n",
    "\n",
    "### –ó–∞—â–æ accuracy –Ω–µ –µ –¥–æ—Å—Ç–∞—Ç—ä—á–Ω–∞?\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º: –î–∏—Å–±–∞–ª–∞–Ω—Å–∏—Ä–∞–Ω–∏ –∫–ª–∞—Å–æ–≤–µ**\n",
    "\n",
    "–ü—Ä–∏–º–µ—Ä: Spam detection\n",
    "- 95% –æ—Ç –∏–º–µ–π–ª–∏—Ç–µ —Å–∞ –Ω–µ-spam\n",
    "- –ú–æ–¥–µ–ª, –∫–æ–π—Ç–æ –≤–∏–Ω–∞–≥–∏ –∫–∞–∑–≤–∞ \"–Ω–µ-spam\" –∏–º–∞ 95% accuracy!\n",
    "- –ù–æ –ø—Ä–æ–ø—É—Å–∫–∞ –≤—Å–µ–∫–∏ spam..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imbalanced-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å accuracy –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å\n",
    "np.random.seed(42)\n",
    "\n",
    "# –°–∏–º—É–ª–∏—Ä–∞–º–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∏—Ä–∞–Ω–∏ –¥–∞–Ω–Ω–∏: 95% not-spam, 5% spam\n",
    "n_total = 1000\n",
    "n_spam = 50\n",
    "n_not_spam = n_total - n_spam\n",
    "\n",
    "y_true = np.array([1] * n_spam + [0] * n_not_spam)  # 1 = spam, 0 = not spam\n",
    "\n",
    "# \"–¢—ä–ø\" –º–æ–¥–µ–ª: –≤–∏–Ω–∞–≥–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–≤–∞ not-spam\n",
    "y_pred_naive = np.zeros(n_total)\n",
    "\n",
    "print(\"üéØ –ü—Ä–æ–±–ª–µ–º —Å accuracy –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∏—Ä–∞–Ω–∏ –¥–∞–Ω–Ω–∏\\n\")\n",
    "print(f\"–†–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {n_spam} spam ({n_spam/n_total*100:.0f}%), {n_not_spam} not-spam ({n_not_spam/n_total*100:.0f}%)\")\n",
    "print(f\"\\n'–¢—ä–ø' –º–æ–¥–µ–ª (–≤–∏–Ω–∞–≥–∏ –∫–∞–∑–≤–∞ 'not-spam'):\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_true, y_pred_naive):.1%}\")\n",
    "print(f\"  –ù–æ: –æ—Ç–∫—Ä–∏—Ç–∏ spam —Å—ä–æ–±—â–µ–Ω–∏—è = 0 –æ—Ç {n_spam}!\")\n",
    "\n",
    "print(\"\\nüí° Accuracy –º–æ–∂–µ –¥–∞ –µ –ø–æ–¥–≤–µ–∂–¥–∞—â–∞ –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∏—Ä–∞–Ω–∏ –∫–ª–∞—Å–æ–≤–µ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precision-recall",
   "metadata": {},
   "source": [
    "### Precision, Recall, F1-score\n",
    "\n",
    "**Confusion Matrix:**\n",
    "\n",
    "|  | Predicted Positive | Predicted Negative |\n",
    "|--|-------------------|-------------------|\n",
    "| **Actual Positive** | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "**–ú–µ—Ç—Ä–∏–∫–∏:**\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\"–û—Ç –≤—Å–∏—á–∫–∏, –∫–æ–∏—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ö–º–µ –∫–∞—Ç–æ positive, –∫–æ–ª–∫–æ —Å–∞ –∏—Å—Ç–∏–Ω—Å–∫–∏ positive?\"\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\"–û—Ç –≤—Å–∏—á–∫–∏ –∏—Å—Ç–∏–Ω—Å–∫–∏ positive, –∫–æ–ª–∫–æ –æ—Ç–∫—Ä–∏—Ö–º–µ?\"\n",
    "\n",
    "$$F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "–•–∞—Ä–º–æ–Ω–∏—á–Ω–∞ —Å—Ä–µ–¥–Ω–∞ –Ω–∞ precision –∏ recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ –Ω–∞—à–∏—è –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å –≤—Å–∏—á–∫–∏ –º–µ—Ç—Ä–∏–∫–∏\n",
    "y_test_pred = model.predict(X_test_vec)\n",
    "\n",
    "print(\"üìä –ü—ä–ª–Ω–∞ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_names))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[n.split('.')[-1] for n in label_names],\n",
    "            yticklabels=[n.split('.')[-1] for n in label_names])\n",
    "plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω –∫–ª–∞—Å', fontsize=12)\n",
    "plt.ylabel('–ò—Å—Ç–∏–Ω—Å–∫–∏ –∫–ª–∞—Å', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"   ‚Ä¢ –î–∏–∞–≥–æ–Ω–∞–ª—ä—Ç –ø–æ–∫–∞–∑–≤–∞ –ø—Ä–∞–≤–∏–ª–Ω–∏ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\")\n",
    "print(\"   ‚Ä¢ –ò–∑–≤—ä–Ω –¥–∏–∞–≥–æ–Ω–∞–ª–∞ —Å–∞ –≥—Ä–µ—à–∫–∏—Ç–µ\")\n",
    "print(\"   ‚Ä¢ –ú–æ–∂–µ–º –¥–∞ –≤–∏–¥–∏–º –∫–æ–∏ –∫–ª–∞—Å–æ–≤–µ —Å–µ –±—ä—Ä–∫–∞—Ç –Ω–∞–π-—á–µ—Å—Ç–æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "when-to-use",
   "metadata": {},
   "source": [
    "### –ö–æ–≥–∞ –∫–æ—è –º–µ—Ç—Ä–∏–∫–∞?\n",
    "\n",
    "| –°–∏—Ç—É–∞—Ü–∏—è | –í–∞–∂–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ | –ü—Ä–∏–º–µ—Ä |\n",
    "|----------|---------------|--------|\n",
    "| –°–∫—ä–ø–∏ False Positives | **Precision** | Spam –≤ inbox –µ –¥–æ—Å–∞–¥–Ω–æ |\n",
    "| –°–∫—ä–ø–∏ False Negatives | **Recall** | –ü—Ä–æ–ø—É—Å–∫–∞–Ω–µ –Ω–∞ –±–æ–ª–µ—Å—Ç –µ –æ–ø–∞—Å–Ω–æ |\n",
    "| –ë–∞–ª–∞–Ω—Å–∏—Ä–∞–Ω –∫–æ–º–ø—Ä–æ–º–∏—Å | **F1-score** | –ü–æ–≤–µ—á–µ—Ç–æ —Å–ª—É—á–∞–∏ |\n",
    "| –ë–∞–ª–∞–Ω—Å–∏—Ä–∞–Ω–∏ –∫–ª–∞—Å–æ–≤–µ | **Accuracy** | –ö–æ–≥–∞—Ç–æ –∫–ª–∞—Å–æ–≤–µ—Ç–µ —Å–∞ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∏ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nlp-specific-metrics",
   "metadata": {},
   "source": [
    "### –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ NLP (preview)\n",
    "\n",
    "**–ó–∞ –≥–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ —Ç–µ–∫—Å—Ç:**\n",
    "- **BLEU:** –°—Ä–∞–≤–Ω—è–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä–∞–Ω —Ç–µ–∫—Å—Ç —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Ç–µ–Ω (–º–∞—à–∏–Ω–µ–Ω –ø—Ä–µ–≤–æ–¥)\n",
    "- **ROUGE:** –ó–∞ summarization\n",
    "- **Perplexity:** –ö–æ–ª–∫–æ \"–∏–∑–Ω–µ–Ω–∞–¥–∞–Ω\" –µ –µ–∑–∏–∫–æ–≤–∏—è—Ç –º–æ–¥–µ–ª (–õ–µ–∫—Ü–∏—è 2)\n",
    "\n",
    "**–ó–∞ sequence labeling:**\n",
    "- **Token-level accuracy:** –ó–∞ –≤—Å–µ–∫–∏ token –ø–æ–æ—Ç–¥–µ–ª–Ω–æ\n",
    "- **Entity-level F1:** –ó–∞ NER - —Ü—è–ª–æ—Å—Ç–Ω–∏ entities\n",
    "\n",
    "üìå **–©–µ –≤–∏–¥–∏–º perplexity –≤ –¥–µ—Ç–∞–π–ª–∏ –≤ —Å–ª–µ–¥–≤–∞—â–∞—Ç–∞ –ª–µ–∫—Ü–∏—è!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. NLP-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏ –ø—Ä–µ–¥–∏–∑–≤–∏–∫–∞—Ç–µ–ª—Å—Ç–≤–∞ (Preview)\n",
    "\n",
    "### –ü—Ä–æ–±–ª–µ–º–∏, –∫–æ–∏—Ç–æ —â–µ —Ä–µ—à–∞–≤–∞–º–µ –≤ —Å–ª–µ–¥–≤–∞—â–∏—Ç–µ –ª–µ–∫—Ü–∏–∏\n",
    "\n",
    "**1. –ö–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–º–µ —Ç–µ–∫—Å—Ç –∫–∞—Ç–æ —á–∏—Å–ª–∞?** (–õ–µ–∫—Ü–∏—è 2)\n",
    "- BoW –≥—É–±–∏ —Ä–µ–¥ –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞\n",
    "- Word embeddings: –¥—É–º–∏ –∫–∞—Ç–æ –≤–µ–∫—Ç–æ—Ä–∏\n",
    "\n",
    "**2. –ö–∞–∫–≤–æ –µ \"–¥—É–º–∞\"?** (–õ–µ–∫—Ü–∏—è 3)\n",
    "- \"don't\" ‚Üí –µ–¥–Ω–∞ –∏–ª–∏ –¥–≤–µ –¥—É–º–∏?\n",
    "- \"New York\" ‚Üí –µ–¥–Ω–∞ –∏–ª–∏ –¥–≤–µ?\n",
    "- Tokenization –∏ subwords\n",
    "\n",
    "**3. –ö–∞–∫ –æ–±—Ä–∞–±–æ—Ç–≤–∞–º–µ –ø–æ—Ä–µ–¥–∏—Ü–∏?** (–õ–µ–∫—Ü–∏—è 4-5)\n",
    "- –ò–∑—Ä–µ—á–µ–Ω–∏—è—Ç–∞ –∏–º–∞—Ç —Ä–∞–∑–ª–∏—á–Ω–∞ –¥—ä–ª–∂–∏–Ω–∞\n",
    "- Recurrent Neural Networks ‚Üí Transformers\n",
    "\n",
    "**4. –ö–∞–∫ —Å–µ —Å–ø—Ä–∞–≤—è–º–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞?** (–õ–µ–∫—Ü–∏—è 5)\n",
    "- \"–±–∞–Ω–∫–∞\" = —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞ –∏–ª–∏ —Ä–µ—á–Ω–∞?\n",
    "- Attention –º–µ—Ö–∞–Ω–∏–∑—ä–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oov-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: Out-of-Vocabulary (OOV) –ø—Ä–æ–±–ª–µ–º\n",
    "train_texts = [\"–û–±–∏—á–∞–º –º–∞—à–∏–Ω–Ω–æ –æ–±—É—á–µ–Ω–∏–µ\", \"ML –µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ\"]\n",
    "test_texts = [\"Deep learning –µ —Å—Ç—Ä–∞—Ö–æ—Ç–Ω–æ\"]  # \"Deep\" –∏ \"learning\" –Ω–µ —Å–∞ –≤ train\n",
    "\n",
    "vec = CountVectorizer()\n",
    "vec.fit(train_texts)\n",
    "\n",
    "print(\"‚ùì Out-of-Vocabulary (OOV) –ø—Ä–æ–±–ª–µ–º\\n\")\n",
    "print(f\"–¢—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–µ–Ω —Ä–µ—á–Ω–∏–∫: {vec.get_feature_names_out()}\")\n",
    "print(f\"\\n–¢–µ—Å—Ç —Ç–µ–∫—Å—Ç: '{test_texts[0]}'\")\n",
    "print(f\"–í–µ–∫—Ç–æ—Ä: {vec.transform(test_texts).toarray()}\")\n",
    "\n",
    "print(\"\\nüí° 'Deep' –∏ 'learning' –Ω–µ —Å–∞ –≤ —Ä–µ—á–Ω–∏–∫–∞ ‚Üí –∏–≥–Ω–æ—Ä–∏—Ä–∞–Ω–∏!\")\n",
    "print(\"   –¢–æ–≤–∞ –µ —Å–µ—Ä–∏–æ–∑–µ–Ω –ø—Ä–æ–±–ª–µ–º –∑–∞ BoW.\")\n",
    "print(\"   –†–µ—à–µ–Ω–∏–µ: Subword tokenization (–õ–µ–∫—Ü–∏—è 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ NLP Workflow\n",
    "\n",
    "### –°—Ç—ä–ø–∫–∏ –∑–∞ —Ä–µ—à–∞–≤–∞–Ω–µ –Ω–∞ NLP –∑–∞–¥–∞—á–∞\n",
    "\n",
    "```\n",
    "1. –î–µ—Ñ–∏–Ω–∏—Ä–∞–π –∑–∞–¥–∞—á–∞—Ç–∞\n",
    "   ‚Üì\n",
    "2. –°—ä–±–µ—Ä–∏ –∏ —Ä–∞–∑–≥–ª–µ–¥–∞–π –¥–∞–Ω–Ω–∏—Ç–µ\n",
    "   ‚Üì\n",
    "3. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (preprocessing)\n",
    "   ‚Üì\n",
    "4. –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞—Ü–∏ / Tokenization\n",
    "   ‚Üì\n",
    "5. –†–∞–∑–¥–µ–ª–∏ –¥–∞–Ω–Ω–∏—Ç–µ (train/val/test)\n",
    "   ‚Üì\n",
    "6. –ò–∑–±–µ—Ä–∏ –∏ –æ–±—É—á–∏ –º–æ–¥–µ–ª\n",
    "   ‚Üì\n",
    "7. –û—Ü–µ–Ω–∏ –Ω–∞ validation set\n",
    "   ‚Üì\n",
    "8. Tune hyperparameters\n",
    "   ‚Üì\n",
    "9. –§–∏–Ω–∞–ª–Ω–∞ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ test set\n",
    "   ‚Üì\n",
    "10. Error analysis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-pitfalls",
   "metadata": {},
   "source": [
    "### –ß–µ—Å—Ç–∏ –≥—Ä–µ—à–∫–∏ –≤ NLP\n",
    "\n",
    "**1. Data leakage (–∏–∑—Ç–∏—á–∞–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏)**\n",
    "- Fit vectorizer –Ω–∞ test –¥–∞–Ω–Ω–∏\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ –±—ä–¥–µ—â–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–∏ time-series\n",
    "\n",
    "**2. Test set contamination**\n",
    "- –û—Å–æ–±–µ–Ω–æ –≤–∞–∂–Ω–æ –∑–∞ LLM: –º–æ–¥–µ–ª—ä—Ç –º–æ–∂–µ –¥–∞ –µ –≤–∏–∂–¥–∞–ª test –¥–∞–Ω–Ω–∏—Ç–µ –ø—Ä–∏ pretraining\n",
    "- \"Benchmark hacking\"\n",
    "\n",
    "**3. Vocabulary mismatch**\n",
    "- Train –Ω–∞ –µ–¥–∏–Ω –¥–æ–º–µ–π–Ω, test –Ω–∞ –¥—Ä—É–≥\n",
    "- OOV –¥—É–º–∏\n",
    "\n",
    "**4. –ò–≥–Ω–æ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ preprocessing**\n",
    "- Lowercasing, punctuation, special characters\n",
    "- –†–∞–∑–ª–∏—á–Ω–∏ preprocessing –∑–∞ train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis: –∫–∞–∫–≤–∏ –≥—Ä–µ—à–∫–∏ –ø—Ä–∞–≤–∏ –º–æ–¥–µ–ª—ä—Ç?\n",
    "print(\"üîç Error Analysis: –ö—ä–¥–µ –≥—Ä–µ—à–∏ –º–æ–¥–µ–ª—ä—Ç?\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –ù–∞–º–∏—Ä–∞–º–µ –≥—Ä–µ—à–Ω–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "errors = np.where(y_test != y_test_pred)[0]\n",
    "print(f\"–û–±—â–æ –≥—Ä–µ—à–∫–∏: {len(errors)} –æ—Ç {len(y_test)} ({len(errors)/len(y_test)*100:.1f}%)\\n\")\n",
    "\n",
    "# –ü–æ–∫–∞–∑–≤–∞–º–µ –Ω—è–∫–æ–ª–∫–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "print(\"–ü—Ä–∏–º–µ—Ä–Ω–∏ –≥—Ä–µ—à–∫–∏:\\n\")\n",
    "for i, idx in enumerate(errors[:5]):\n",
    "    text = X_test[idx][:150] + \"...\" if len(X_test[idx]) > 150 else X_test[idx]\n",
    "    true_label = label_names[y_test[idx]].split('.')[-1]\n",
    "    pred_label = label_names[y_test_pred[idx]].split('.')[-1]\n",
    "    \n",
    "    print(f\"–ü—Ä–∏–º–µ—Ä {i+1}:\")\n",
    "    print(f\"  –¢–µ–∫—Å—Ç: \\\"{text}\\\"\")\n",
    "    print(f\"  –ò—Å—Ç–∏–Ω—Å–∫–∏: {true_label}\")\n",
    "    print(f\"  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω: {pred_label}\\n\")\n",
    "\n",
    "print(\"üí° Error analysis –ø–æ–º–∞–≥–∞ –¥–∞ —Ä–∞–∑–±–µ—Ä–µ–º —Å–ª–∞–±–æ—Å—Ç–∏—Ç–µ –Ω–∞ –º–æ–¥–µ–ª–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. –û–±–æ–±—â–µ–Ω–∏–µ –∏ –º–æ—Å—Ç –∫—ä–º –õ–µ–∫—Ü–∏—è 2\n",
    "\n",
    "### –ö–ª—é—á–æ–≤–∏ –∏–∑–≤–æ–¥–∏ –æ—Ç –¥–Ω–µ—Å\n",
    "\n",
    "**1. ML –µ —É—á–µ–Ω–µ –æ—Ç –¥–∞–Ω–Ω–∏**\n",
    "- Task, Experience, Performance metric\n",
    "- –§–æ—Ä–º–∞–ª–Ω–∞ –Ω–æ—Ç–∞—Ü–∏—è: $h: \\mathcal{X} \\to \\mathcal{Y}$\n",
    "\n",
    "**2. –¢–µ–∫—Å—Ç—ä—Ç –∏–∑–∏—Å–∫–≤–∞ —Å–ø–µ—Ü–∏–∞–ª–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ**\n",
    "- Bag-of-Words: –ø—Ä–æ—Å—Ç–æ, –Ω–æ –≥—É–±–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "- TF-IDF: –ø—Ä–µ—Ç–µ–≥–ª—è –≤–∞–∂–Ω–æ—Å—Ç—Ç–∞ –Ω–∞ –¥—É–º–∏—Ç–µ\n",
    "\n",
    "**3. –¢—Ä–∏ –ø–∞—Ä–∞–¥–∏–≥–º–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ**\n",
    "- –° —É—á–∏—Ç–µ–ª: –∏–º–∞–º–µ –µ—Ç–∏–∫–µ—Ç–∏\n",
    "- –ë–µ–∑ —É—á–∏—Ç–µ–ª: —Ç—ä—Ä—Å–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n",
    "- –° –ø–æ–¥—Å–∏–ª–≤–∞–Ω–µ: –Ω–∞–≥—Ä–∞–¥–∏ –∑–∞ –¥–µ–π—Å—Ç–≤–∏—è\n",
    "\n",
    "**4. –ì–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è—Ç–∞ –µ –∫–ª—é—á–æ–≤–∞**\n",
    "- –ü—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ vs —ä–Ω–¥—ä—Ä—Ñ–∏—Ç–≤–∞–Ω–µ\n",
    "- Train/val/test split\n",
    "\n",
    "**5. –ú–µ—Ç—Ä–∏–∫–∏—Ç–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –∑–∞–¥–∞—á–∞—Ç–∞**\n",
    "- Accuracy, Precision, Recall, F1\n",
    "- Confusion matrix –∑–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-lecture",
   "metadata": {},
   "source": [
    "### –°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è: –ï–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –Ω–∞ –¥—É–º–∏\n",
    "\n",
    "**–©–µ –æ—Ç–≥–æ–≤–æ—Ä–∏–º –Ω–∞:**\n",
    "\n",
    "- –ö–∞–∫ –¥–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º –¥—É–º–∏, —Ç–∞–∫–∞ —á–µ \"–∫—Ä–∞–ª\" –∏ \"–∫—Ä–∞–ª–∏—Ü–∞\" –¥–∞ —Å–∞ –±–ª–∏–∑–∫–∏?\n",
    "- –ö–∞–∫–≤–æ –µ –µ–∑–∏–∫–æ–≤ –º–æ–¥–µ–ª?\n",
    "- –ö–∞–∫ —Ä–∞–±–æ—Ç–∏ Word2Vec?\n",
    "- –ö–∞–∫–≤–æ –µ perplexity?\n",
    "\n",
    "**–ó–∞—â–æ –µ –≤–∞–∂–Ω–æ:**\n",
    "- Word embeddings —Å–∞ –æ—Å–Ω–æ–≤–∞ –Ω–∞ —Å—ä–≤—Ä–µ–º–µ–Ω–Ω–∏—Ç–µ LLM\n",
    "- –ï–∑–∏–∫–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª–∏ —Å–∞ –≤ —Å—ä—Ä—Ü–µ—Ç–æ –Ω–∞ ChatGPT, Claude, GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "## –†–µ—Å—É—Ä—Å–∏\n",
    "\n",
    "### –ü—Ä–µ–ø–æ—Ä—ä—á–∏—Ç–µ–ª–Ω–æ —á–µ—Ç–µ–Ω–µ\n",
    "\n",
    "**–°—Ç–∞—Ç–∏–∏:**\n",
    "1. \"A Few Useful Things to Know About Machine Learning\" - Pedro Domingos (2012)\n",
    "2. \"Natural Language Processing (almost) from Scratch\" - Collobert et al. (2011)\n",
    "\n",
    "**–£—á–µ–±–Ω–∏—Ü–∏:**\n",
    "1. \"Speech and Language Processing\" - Jurafsky & Martin (3rd ed.) - Chapter 4\n",
    "2. \"Introduction to Information Retrieval\" - Manning et al. - Chapter 13\n",
    "3. \"Pattern Recognition and Machine Learning\" - Bishop - Chapter 1\n",
    "\n",
    "**Online:**\n",
    "1. Stanford CS229 Lecture Notes\n",
    "2. Stanford CS224N Lecture 1\n",
    "3. Scikit-learn documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∑–∞ –≤–∫—ä—â–∏\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 1: Spam –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä**\n",
    "- –ó–∞—Ä–µ–¥–µ—Ç–µ SMS Spam Collection dataset\n",
    "- –†–∞–∑–¥–µ–ª–µ—Ç–µ –Ω–∞ train/val/test\n",
    "- –û–±—É—á–µ—Ç–µ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (BoW + Logistic Regression)\n",
    "- –û—Ü–µ–Ω–µ—Ç–µ —Å precision, recall, F1\n",
    "- –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–π—Ç–µ –≥—Ä–µ—à–∫–∏—Ç–µ\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞—Ü–∏**\n",
    "- –°—Ä–∞–≤–Ω–µ—Ç–µ unigrams vs bigrams vs trigrams\n",
    "- –ù–∞—á–µ—Ä—Ç–∞–π—Ç–µ learning curves –∑–∞ –≤—Å–µ–∫–∏\n",
    "- –ö–æ–π –ø—Ä–µ–¥—Å—Ç–∞–≤—è –¥–∞–Ω–Ω–∏—Ç–µ –Ω–∞–π-–¥–æ–±—Ä–µ?\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 3: Hyperparameter tuning**\n",
    "- –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–∞–π—Ç–µ —Å —Ä–∞–∑–ª–∏—á–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ –Ω–∞:\n",
    "  - `max_features` –≤ TfidfVectorizer\n",
    "  - `C` –≤ LogisticRegression\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞–π—Ç–µ validation set –∑–∞ –∏–∑–±–æ—Ä\n",
    "- –û—Ü–µ–Ω–µ—Ç–µ —Ñ–∏–Ω–∞–ª–Ω–æ –Ω–∞ test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "end",
   "metadata": {},
   "source": [
    "---\n",
    "## –ö—Ä–∞–π –Ω–∞ –õ–µ–∫—Ü–∏—è 1\n",
    "\n",
    "### –ë–ª–∞–≥–æ–¥–∞—Ä—è –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ—Ç–æ!\n",
    "\n",
    "**–í—ä–ø—Ä–æ—Å–∏?**\n",
    "\n",
    "---\n",
    "\n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –ï–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –Ω–∞ –¥—É–º–∏"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
