# Кратки упражнения: Лекция 7

Следните упражнения са за самостоятелна работа по време на лекцията или веднага след нея. Очаквано време: 2-3 минути за упражнение.

---

## Упражнение 1: Zero-Shot vs Few-Shot

Разгледайте следните prompt-ове за sentiment класификация:

**Zero-shot:**
```
Classify the sentiment of this review as positive or negative:
"This movie was absolutely terrible."
Sentiment:
```

**Few-shot (k=2):**
```
Review: "I loved this book!" → Sentiment: positive
Review: "Waste of money." → Sentiment: negative
Review: "This movie was absolutely terrible." → Sentiment:
```

**Въпроси:**
1. Кой подход изисква повече токени в prompt-а?
2. При какъв размер модел (~параметри) few-shot започва да работи надеждно?
3. Защо few-shot обикновено дава по-добри резултати?

---

## Упражнение 2: In-Context Learning изчисления

Few-shot prompt съдържа k примера, всеки с ~50 токена. Query-то е 30 токена.

**Задача:** Изчислете общия брой токени в prompt-а за:
1. k = 0 (zero-shot)
2. k = 3
3. k = 10

**Въпрос:** Ако context window е 4096 токена, какъв е максималният k?

---

## Упражнение 3: Emergence Threshold

От публикувани резултати на GSM8K (math word problems):

| Модел | Параметри | Accuracy |
|-------|-----------|----------|
| GPT-2 | 1.5B | ~0% |
| GPT-3 | 6.7B | ~6% |
| GPT-3 | 175B | ~35% |
| PaLM | 540B | ~56% |

**Въпроси:**
1. Между кои размери се наблюдава "sharp emergence"?
2. Колко пъти по-голям е 175B от 6.7B?
3. Подобрението от 6% на 35% пропорционално ли е на увеличението на параметрите?

---

## Упражнение 4: Few-Shot Scaling

Модел показва следните резултати на класификация с различен брой примери:

| k (примери) | Accuracy |
|-------------|----------|
| 0 | 45% |
| 1 | 62% |
| 3 | 71% |
| 5 | 74% |
| 10 | 76% |
| 20 | 77% |

**Въпроси:**
1. Кой скок е най-голям (абсолютно)?
2. След колко примера виждаме diminishing returns?
3. Струва ли си да използваме 20 примера вместо 5?

---

## Упражнение 5: Perplexity vs Task Performance

Два модела имат следните метрики:

| Модел | Perplexity | MMLU Accuracy |
|-------|------------|---------------|
| A | 15.2 | 52% |
| B | 12.8 | 68% |

**Въпроси:**
1. Кой модел е по-добър в language modeling?
2. По-ниска perplexity винаги ли означава по-добър reasoning?
3. Защо връзката не е линейна?

---

## Упражнение 6: Benchmark Contamination

Модел се тества на benchmark с 500 въпроса. Анализ показва:

- 25 въпроса са в training data (exact match)
- Accuracy на contaminated: 92%
- Accuracy на clean: 58%

**Задача:**
1. Какъв процент от benchmark-а е contaminated?
2. Изчислете "наивната" обща accuracy
3. Каква е истинската способност на модела?

**Формула:** Обща acc = (25 × 0.92 + 475 × 0.58) / 500

---

## Упражнение 7: Sharp vs Smooth Emergence

Разгледайте два типа emergence криви:

**Task A (smooth):** Accuracy нараства линейно с log(параметри)
**Task B (sharp):** Accuracy е ~0% до 10B параметъра, след това скача на 60%

**Въпроси:**
1. Кой тип е по-предсказуем?
2. За кой тип можем да екстраполираме от малки модели?
3. Какво значение има това за AI safety?

---

## Упражнение 8: Inverse Scaling

На задачата "Sycophancy" (съгласяване с грешни твърдения):

| Модел | Параметри | Sycophancy Rate |
|-------|-----------|-----------------|
| Small | 1B | 15% |
| Medium | 10B | 25% |
| Large | 100B | 40% |

**Въпроси:**
1. Какъв тип scaling е това?
2. Защо по-големите модели са по-лоши на тази задача?
3. Как това се отнася към alignment (следващата лекция)?

---

## Упражнение 9: ICL като Implicit Fine-Tuning

Теория: Attention механизмът в transformer имплицитно извършва gradient-like update.

**Въпроси:**
1. Променят ли се weights-ите на модела при ICL?
2. Ако не, къде се "съхранява" научената информация?
3. Защо това е различно от традиционно fine-tuning?

---

## Упражнение 10: Example Order Effect

Същите 3 few-shot примера, но в различен ред:

| Ред | Accuracy |
|-----|----------|
| ABC | 75% |
| ACB | 72% |
| BAC | 71% |
| BCA | 68% |
| CAB | 74% |
| CBA | 65% |

**Въпроси:**
1. Каква е variance в accuracy само от реда?
2. Кой пример вероятно е най-важен да е последен?
3. Какво означава това за reproducibility?

---

## Упражнение 11: Code Generation Emergence

HumanEval резултати (pass@1):

| Модел | Параметри | Pass@1 |
|-------|-----------|--------|
| Codex (initial) | 12B | 28.8% |
| GPT-3.5 | ~175B | 48.1% |
| GPT-4 | ~1T+ | 67.0% |

**Въпроси:**
1. Code generation показва smooth или sharp emergence?
2. Какво е подобрението от 12B до ~1T+ като множител?
3. Защо code generation е по-"smooth" от math reasoning?

---

## Упражнение 12: Emergence Prediction

Искате да предскажете дали модел от 50B параметъра ще може да решава задача X.

Данни от по-малки модели:
- 1B: 0% accuracy
- 5B: 2% accuracy
- 10B: 5% accuracy

**Въпроси:**
1. Какъв тип emergence виждате досега?
2. Можете ли уверено да предскажете performance на 50B?
3. Защо emergence прави AI development planning труден?

---

# Решения

## Решение 1
1. Few-shot изисква повече токени (примерите + query vs само query)
2. Few-shot работи надеждно при >10B параметъра, силно при >100B
3. Примерите показват формата на очаквания отговор и помагат на модела да "разпознае" задачата

## Решение 2
1. k=0: 30 токена (само query)
2. k=3: 3 × 50 + 30 = 180 токена
3. k=10: 10 × 50 + 30 = 530 токена

Максимален k: (4096 - 30) / 50 ≈ 81 примера

## Решение 3
1. Sharp emergence между 6.7B и 175B
2. 175B / 6.7B ≈ 26× по-голям
3. Не, 6× подобрение в accuracy (6%→35%) при 26× параметри показва нелинейна, "emergent" връзка

## Решение 4
1. Най-голям скок: 0→1 пример (17 процентни точки)
2. Diminishing returns след k=5 (от 74% на 76% = само 2pp за 5 примера)
3. Не особено - 3 процентни точки за 15 допълнителни примера не е ефективно

## Решение 5
1. Модел B (по-ниска perplexity = по-добро language modeling)
2. Не винаги - perplexity измерва предсказване на токени, не reasoning
3. Task performance изисква специфични способности (reasoning, knowledge), които не се измерват директно от perplexity

## Решение 6
1. 25/500 = 5% contaminated
2. Наивна accuracy: (25 × 0.92 + 475 × 0.58) / 500 = (23 + 275.5) / 500 = 59.7%
3. Истинската способност е ~58% (clean accuracy), наивната е надценена

## Решение 7
1. Smooth е по-предсказуем
2. Smooth - можем да екстраполираме тренда; sharp - не можем да предвидим "прага"
3. Sharp emergence означава, че нови способности могат да се появят неочаквано при scale-up, което прави safety evaluation труден

## Решение 8
1. Inverse scaling (по-лошо с повече параметри)
2. По-големите модели са по-добри в pattern matching и са научили, че хората харесват съгласие - дори когато е грешно
3. Alignment трябва да коригира тези проблемни поведения, които се усилват с мащаба

## Решение 9
1. Не, weights остават замразени (frozen)
2. В activations/representations по време на forward pass - attention механизмът комбинира информация от примерите
3. Fine-tuning променя weights перманентно; ICL използва context временно

## Решение 10
1. Variance: 75% - 65% = 10 процентни точки
2. Пример A (най-високо когато е последен: ABC=75%, CAB=74%)
3. Резултатите са нестабилни - същият модел със същите примери дава различни резултати. Трябва да се тества с multiple orderings

## Решение 11
1. По-скоро smooth emergence (постепенно подобрение)
2. От 28.8% до 67.0% = 2.3× подобрение
3. Code има ясни правила (syntax, logic), докато math word problems изискват multi-step reasoning и "разбиране" - различни типове emergence

## Решение 12
1. Много бавно (smooth) нарастване досега
2. Не - 5% при 10B не гарантира какво ще е при 50B. Може да остане ниско или да има sharp jump
3. Не можем да знаем предварително кои способности ще се появят и при какъв мащаб - прави planning и safety evaluation трудни
