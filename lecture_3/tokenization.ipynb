{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Лекция 3: Токенизация\n",
    "\n",
    "**Продължителност:** 2-2.5 часа  \n",
    "**Предпоставки:** Лекция 2 (Невронни мрежи, embeddings)  \n",
    "**Следваща лекция:** Механизми на внимание (Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives",
   "metadata": {},
   "source": [
    "---\n",
    "## Цели на лекцията\n",
    "\n",
    "След тази лекция ще можете:\n",
    "\n",
    "- Обяснявате защо токенизацията е критична за езиковите модели\n",
    "- Сравнявате word-level, character-level и subword токенизация\n",
    "- Имплементирате BPE алгоритъм от нулата\n",
    "- Разбирате Unigram и WordPiece алгоритмите\n",
    "- Работите с HuggingFace tokenizers и tiktoken в практически задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roadmap",
   "metadata": {},
   "source": [
    "### Пътна карта\n",
    "\n",
    "```\n",
    "Мотивация → Нормализация → Word/Char → BPE (deep dive) → Unigram → Практика → LLM токенизатори\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основни библиотеки\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Настройки\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Библиотеките са заредени успешно.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Мотивация: Защо токенизацията е важна\n",
    "\n",
    "### Припомняне от Лекция 2\n",
    "\n",
    "В Лекция 2 изградихме текстов класификатор:\n",
    "\n",
    "```\n",
    "Текст → Токени → Token IDs → Embeddings → Neural Network → Output\n",
    "```\n",
    "\n",
    "**Използвахме проста токенизация:** разделяне по whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding-problem",
   "metadata": {},
   "source": [
    "### Проблемът с embedding слоя\n",
    "\n",
    "```python\n",
    "nn.Embedding(vocab_size, embed_dim)\n",
    "```\n",
    "\n",
    "**Какво е `vocab_size`?**\n",
    "\n",
    "- Всеки уникален токен има собствен embedding вектор\n",
    "- Размерът на речника директно влияе на размера на модела\n",
    "- **Въпрос:** Как да определим какво е \"токен\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "what-is-word",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Какво е \"дума\"?\n",
    "challenges = [\n",
    "    (\"New York\", \"Един град или две думи?\"),\n",
    "    (\"don't\", \"Една дума или 'do' + 'not'?\"),\n",
    "    (\"COVID-19\", \"Как да разделим?\"),\n",
    "    (\"state-of-the-art\", \"Едно понятие, много тирета\"),\n",
    "    (\"Bundesausbildungsförderungsgesetz\", \"Немски съставни думи\"),\n",
    "    (\"中华人民共和国\", \"Китайски: няма интервали\"),\n",
    "    (\"goooood\", \"Удължена дума в социални мрежи\"),\n",
    "]\n",
    "\n",
    "print(\"Какво е 'дума'?\\n\")\n",
    "for text, question in challenges:\n",
    "    print(f\"  {text:40s} → {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-matters",
   "metadata": {},
   "source": [
    "### Защо токенизацията определя качеството на модела\n",
    "\n",
    "| Проблем | Последица |\n",
    "|---------|----------|\n",
    "| **OOV думи** | Модел не може да обработи нови/непознати думи |\n",
    "| **Огромен речник** | Милиони думи = милиони embedding вектори = много памет |\n",
    "| **Липса на генерализация** | \"walk\", \"walks\", \"walked\" са напълно различни вектори |\n",
    "| **Дълги последователности** | Повече токени = повече compute и памет |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oov-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Демонстрация: OOV (Out-of-Vocabulary) проблем\n",
    "vocabulary = {'the', 'cat', 'sat', 'on', 'mat', 'dog', 'runs'}\n",
    "\n",
    "test_sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog runs\",\n",
    "    \"the kitten sat quietly\",\n",
    "    \"COVID-19 affects everyone\",\n",
    "]\n",
    "\n",
    "print(f\"Речник: {vocabulary}\\n\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    words = sent.lower().split()\n",
    "    oov = [w for w in words if w not in vocabulary]\n",
    "    status = \"OK\" if not oov else f\"OOV: {oov}\"\n",
    "    print(f\"  '{sent}'\")\n",
    "    print(f\"  → {status}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization-def",
   "metadata": {},
   "source": [
    "### Три подхода за токенизация\n",
    "\n",
    "| Подход | Токени | Пример за \"playing\" | Vocab size |\n",
    "|--------|--------|---------------------|------------|\n",
    "| **Word-level** | Цели думи | `[\"playing\"]` | ~100K-1M |\n",
    "| **Character-level** | Символи | `[\"p\",\"l\",\"a\",\"y\",\"i\",\"n\",\"g\"]` | ~100-300 |\n",
    "| **Subword** | Части от думи | `[\"play\", \"ing\"]` | ~30K-100K |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Нормализация и предобработка\n",
    "\n",
    "Преди токенизация обикновено прилагаме **нормализация** — стандартизиране на текста."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "case-norm",
   "metadata": {},
   "source": [
    "### 2.1 Case нормализация\n",
    "\n",
    "**Въпрос:** \"The\" и \"the\" една дума ли са? А \"Apple\" (компания) и \"apple\" (плод)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "case-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case sensitivity\n",
    "text = \"Apple released the new iPhone. I ate an apple.\"\n",
    "\n",
    "words_original = text.split()\n",
    "words_lower = text.lower().split()\n",
    "\n",
    "print(f\"Текст: '{text}'\\n\")\n",
    "print(f\"Case-sensitive:   {len(set(words_original))} уникални думи\")\n",
    "print(f\"Case-insensitive: {len(set(words_lower))} уникални думи\")\n",
    "\n",
    "print(\"\\nLowercase намалява речника, но губим информация:\")\n",
    "print(\"  'Apple' (компания) vs 'apple' (плод)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unicode-norm",
   "metadata": {},
   "source": [
    "### 2.2 Unicode нормализация\n",
    "\n",
    "**Проблем:** Един и същ символ може да има различни Unicode представяния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unicode-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode: \"é\" може да бъде представено по различни начини\n",
    "e_composed = 'é'           # NFC: един codepoint (U+00E9)\n",
    "e_decomposed = 'e\\u0301'   # NFD: 'e' + combining acute accent\n",
    "\n",
    "print(\"Unicode представяния на 'é':\\n\")\n",
    "print(f\"  Composed (NFC):   '{e_composed}' → bytes: {e_composed.encode('utf-8')}\")\n",
    "print(f\"  Decomposed (NFD): '{e_decomposed}' → bytes: {e_decomposed.encode('utf-8')}\")\n",
    "\n",
    "print(f\"\\n  Визуално еднакви? {e_composed} == {e_decomposed} визуално\")\n",
    "print(f\"  Еднакви в Python? {e_composed == e_decomposed}\")\n",
    "\n",
    "# Нормализация\n",
    "nfc = unicodedata.normalize('NFC', e_decomposed)\n",
    "print(f\"\\n  След NFC нормализация: '{nfc}' == '{e_composed}' → {nfc == e_composed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "punct-handling",
   "metadata": {},
   "source": [
    "### 2.3 Третиране на пунктуация\n",
    "\n",
    "**Въпрос:** \"Hello!\" и \"Hello\" еднакви ли са? Какво правим с \"don't\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "punct-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Различни стратегии за пунктуация\n",
    "text = \"Hello, world! I'm learning NLP. It's state-of-the-art.\"\n",
    "\n",
    "# Стратегия 1: Запазваме пунктуацията прикачена\n",
    "tokens_attached = text.split()\n",
    "\n",
    "# Стратегия 2: Разделяме пунктуацията\n",
    "tokens_separated = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "# Стратегия 3: Премахваме пунктуацията\n",
    "tokens_no_punct = re.findall(r\"\\w+\", text)\n",
    "\n",
    "print(f\"Текст: '{text}'\\n\")\n",
    "print(f\"Прикачена:  {tokens_attached}\")\n",
    "print(f\"Разделена:  {tokens_separated}\")\n",
    "print(f\"Без punct:  {tokens_no_punct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stemming-lemma",
   "metadata": {},
   "source": [
    "### 2.4 Стемиране vs Лематизация\n",
    "\n",
    "| Метод | Подход | \"running\" → | \"better\" → |\n",
    "|-------|--------|-------------|------------|\n",
    "| **Stemming** | Правила (отрязване) | \"run\" | \"better\" |\n",
    "| **Lemmatization** | Речник | \"run\" | \"good\" |\n",
    "\n",
    "**Забележка:** Съвременните LLM рядко използват stemming/lemmatization — оставят модела да научи морфологията."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stemming-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming демонстрация (Porter Stemmer)\n",
    "# Прост алгоритъм без NLTK\n",
    "\n",
    "def simple_stem(word):\n",
    "    \"\"\"Опростен stemmer за демонстрация.\"\"\"\n",
    "    suffixes = ['ing', 'ed', 'es', 's', 'ly', 'er', 'est']\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "words = ['running', 'runs', 'runner', 'walked', 'walking', 'happily', 'studies']\n",
    "\n",
    "print(\"Прост stemming:\\n\")\n",
    "for word in words:\n",
    "    stem = simple_stem(word)\n",
    "    print(f\"  {word:12s} → {stem}\")\n",
    "\n",
    "print(\"\\nStemming е бърз, но груб. Lemmatization е по-точна, но изисква речник.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Прости подходи за токенизация\n",
    "\n",
    "### 3.1 Word-level токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "word-tokenization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Различни методи за word tokenization\n",
    "text = \"Hello, world! I'm learning NLP. It's state-of-the-art.\"\n",
    "\n",
    "# Метод 1: Whitespace split\n",
    "whitespace = text.split()\n",
    "\n",
    "# Метод 2: Regex (само думи)\n",
    "regex_words = re.findall(r'\\w+', text)\n",
    "\n",
    "# Метод 3: Regex (думи + пунктуация)\n",
    "regex_all = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "print(f\"Текст: '{text}'\\n\")\n",
    "print(f\"Whitespace:     {whitespace}\")\n",
    "print(f\"Regex (\\\\w+):   {regex_words}\")\n",
    "print(f\"Regex (+ punct): {regex_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "word-problems",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проблеми с word-level токенизация\n",
    "print(\"Проблеми с word-level токенизация:\\n\")\n",
    "\n",
    "print(\"1. ОГРОМЕН РЕЧНИК:\")\n",
    "print(\"   English Wikipedia: ~5M уникални думи\")\n",
    "print(\"   Google Books: ~100M уникални думи\")\n",
    "print(\"   → Embedding layer: 100M × 768 = 76.8 GB само за embeddings!\")\n",
    "\n",
    "print(\"\\n2. OOV ПРОБЛЕМ:\")\n",
    "print(\"   Нови думи: 'chatgpt', 'cryptocurrency', 'covid'\")\n",
    "print(\"   Грешки: 'teh', 'recieve', 'definately'\")\n",
    "print(\"   → [UNK] токен губи цялата информация\")\n",
    "\n",
    "print(\"\\n3. НЯМА СПОДЕЛЯНЕ:\")\n",
    "print(\"   'walk', 'walks', 'walked', 'walking' са 4 различни вектора\")\n",
    "print(\"   → Моделът не знае, че са свързани\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "char-level-header",
   "metadata": {},
   "source": [
    "### 3.2 Character-level токенизация\n",
    "\n",
    "**Идея:** Всеки символ е токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "char-tokenization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-level токенизация\n",
    "text = \"Hello NLP!\"\n",
    "\n",
    "char_tokens = list(text)\n",
    "\n",
    "print(f\"Текст: '{text}'\")\n",
    "print(f\"Токени: {char_tokens}\")\n",
    "print(f\"Брой токени: {len(char_tokens)}\")\n",
    "\n",
    "print(\"\\nРазмер на речника:\")\n",
    "print(\"  ASCII букви + цифри + punct: ~100 символа\")\n",
    "print(\"  Пълен Unicode: ~150,000 символа\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "char-problems",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проблеми с character-level\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "word_tokens = sentence.split()\n",
    "char_tokens = list(sentence)\n",
    "\n",
    "print(f\"Текст: '{sentence}'\\n\")\n",
    "print(f\"Word tokens: {len(word_tokens)} токена\")\n",
    "print(f\"Char tokens: {len(char_tokens)} токена\")\n",
    "print(f\"\\nСъотношение: {len(char_tokens)/len(word_tokens):.1f}x по-дълга последователност\")\n",
    "\n",
    "print(\"\\nПроблеми:\")\n",
    "print(\"  1. Много дълги последователности → повече compute\")\n",
    "print(\"  2. Символите имат малко семантично значение\")\n",
    "print(\"  3. Труднo за модела да научи дълги зависимости\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tradeoff-header",
   "metadata": {},
   "source": [
    "### 3.3 Trade-off: Vocabulary size vs Sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tradeoff-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация на trade-off\n",
    "approaches = ['Character', 'Subword\\n(BPE)', 'Word']\n",
    "vocab_sizes = [100, 50000, 1000000]\n",
    "seq_lengths = [50, 15, 10]  # за същия текст\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "colors = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "\n",
    "# Vocabulary size\n",
    "axes[0].bar(approaches, vocab_sizes, color=colors)\n",
    "axes[0].set_ylabel('Размер на речника')\n",
    "axes[0].set_title('Vocabulary Size')\n",
    "axes[0].set_yscale('log')\n",
    "for i, v in enumerate(vocab_sizes):\n",
    "    axes[0].text(i, v*1.5, f'{v:,}', ha='center', fontsize=11)\n",
    "\n",
    "# Sequence length\n",
    "axes[1].bar(approaches, seq_lengths, color=colors)\n",
    "axes[1].set_ylabel('Брой токени (за ~10 думи)')\n",
    "axes[1].set_title('Дължина на последователността')\n",
    "for i, v in enumerate(seq_lengths):\n",
    "    axes[1].text(i, v+1, str(v), ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Subword токенизация е 'златната среда':\")\n",
    "print(\"  - Умерен размер на речника (~30K-100K)\")\n",
    "print(\"  - Разумна дължина на последователностите\")\n",
    "print(\"  - Няма OOV (всичко може да се разбие до символи/bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Subword токенизация: BPE и Unigram\n",
    "\n",
    "### Ключова идея\n",
    "\n",
    "**Subword tokenization** разбива думите на смислени части:\n",
    "- Чести думи → един токен: \"the\", \"and\", \"is\"\n",
    "- Редки думи → няколко токена: \"un\" + \"happi\" + \"ness\"\n",
    "\n",
    "**Предимства:**\n",
    "- Фиксиран речник (30K-100K)\n",
    "- Няма OOV — всичко се разбива до символи/bytes\n",
    "- Споделяне: \"play\", \"playing\", \"replay\" споделят \"play\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpe-header",
   "metadata": {},
   "source": [
    "### 4.1 Byte-Pair Encoding (BPE) — Deep Dive\n",
    "\n",
    "**Философия:** Bottom-up, greedy, frequency-based\n",
    "\n",
    "**Алгоритъм за обучение:**\n",
    "1. Започни с речник от всички символи\n",
    "2. Намери най-честата двойка съседни символи\n",
    "3. Обедини ги в нов символ\n",
    "4. Повтаряй докато речникът достигне желания размер\n",
    "\n",
    "**Използва се от:** GPT-2, GPT-3, GPT-4, LLaMA, RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примерен корпус за BPE демонстрация\n",
    "corpus = [\n",
    "    \"low\", \"low\", \"low\", \"low\", \"low\",\n",
    "    \"lower\", \"lower\",\n",
    "    \"newest\", \"newest\", \"newest\", \"newest\", \"newest\", \"newest\",\n",
    "    \"widest\", \"widest\", \"widest\"\n",
    "]\n",
    "\n",
    "word_freq = Counter(corpus)\n",
    "\n",
    "print(\"Корпус (честота на думите):\\n\")\n",
    "for word, count in word_freq.items():\n",
    "    print(f\"  '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE Стъпка 1: Инициализация — разбиваме на символи\n",
    "def initialize_vocab(word_freq):\n",
    "    \"\"\"Разбива всяка дума на символи с маркер за край на дума.\"\"\"\n",
    "    vocab = {}\n",
    "    for word, freq in word_freq.items():\n",
    "        # Добавяме </w> като маркер за край на дума\n",
    "        chars = list(word) + ['</w>']\n",
    "        vocab[' '.join(chars)] = freq\n",
    "    return vocab\n",
    "\n",
    "vocab = initialize_vocab(word_freq)\n",
    "\n",
    "print(\"Начален речник (символи):\\n\")\n",
    "for word, freq in vocab.items():\n",
    "    print(f\"  [{word}] : {freq}\")\n",
    "\n",
    "# Извличаме уникалните символи\n",
    "all_symbols = set()\n",
    "for word in vocab:\n",
    "    all_symbols.update(word.split())\n",
    "\n",
    "print(f\"\\nУникални символи: {sorted(all_symbols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-pairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE Стъпка 2: Намираме най-честата двойка\n",
    "def get_pair_counts(vocab):\n",
    "    \"\"\"Брои всички съседни двойки символи.\"\"\"\n",
    "    pairs = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            pairs[pair] += freq\n",
    "    return pairs\n",
    "\n",
    "pairs = get_pair_counts(vocab)\n",
    "\n",
    "print(\"Честота на двойките:\\n\")\n",
    "for pair, count in pairs.most_common(10):\n",
    "    print(f\"  {pair}: {count}\")\n",
    "\n",
    "best_pair = pairs.most_common(1)[0]\n",
    "print(f\"\\nНай-честа двойка: {best_pair[0]} (честота: {best_pair[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-merge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE Стъпка 3: Обединяваме най-честата двойка\n",
    "def merge_pair(pair, vocab):\n",
    "    \"\"\"Обединява двойка символи навсякъде в речника.\"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = freq\n",
    "    \n",
    "    return new_vocab\n",
    "\n",
    "# Извършваме първото сливане\n",
    "best = pairs.most_common(1)[0][0]\n",
    "vocab = merge_pair(best, vocab)\n",
    "\n",
    "print(f\"Merge #1: {best[0]} + {best[1]} → {''.join(best)}\\n\")\n",
    "print(\"Нов речник:\")\n",
    "for word, freq in vocab.items():\n",
    "    print(f\"  [{word}] : {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-full",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пълна BPE имплементация\n",
    "def train_bpe(word_freq, num_merges):\n",
    "    \"\"\"Обучава BPE tokenizer.\"\"\"\n",
    "    vocab = initialize_vocab(word_freq)\n",
    "    merges = []  # Записваме реда на сливанията\n",
    "    \n",
    "    print(f\"BPE обучение ({num_merges} merge операции):\\n\")\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_pair_counts(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        best_pair = pairs.most_common(1)[0][0]\n",
    "        vocab = merge_pair(best_pair, vocab)\n",
    "        merges.append(best_pair)\n",
    "        \n",
    "        merged = ''.join(best_pair)\n",
    "        print(f\"  Merge #{i+1}: {best_pair[0]:8s} + {best_pair[1]:8s} → {merged}\")\n",
    "    \n",
    "    return vocab, merges\n",
    "\n",
    "# Обучаваме с 10 сливания\n",
    "final_vocab, merge_rules = train_bpe(word_freq, num_merges=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Финален BPE речник\n",
    "print(\"Финален BPE речник:\\n\")\n",
    "for word, freq in final_vocab.items():\n",
    "    print(f\"  [{word}] : {freq}\")\n",
    "\n",
    "# Извличаме всички токени\n",
    "all_tokens = set()\n",
    "for word in final_vocab:\n",
    "    all_tokens.update(word.split())\n",
    "\n",
    "print(f\"\\nТокени в речника ({len(all_tokens)}):\")\n",
    "print(f\"  {sorted(all_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpe-encoding-header",
   "metadata": {},
   "source": [
    "### BPE Encoding: Токенизация на нов текст\n",
    "\n",
    "**Алгоритъм:**\n",
    "1. Разбий думата на символи\n",
    "2. Приложи merge правилата в реда, в който са научени\n",
    "3. Резултатът е токенизираната дума"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-encode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE Encoding\n",
    "def bpe_encode(word, merges):\n",
    "    \"\"\"Токенизира дума с BPE merge rules.\"\"\"\n",
    "    # Започваме със символи\n",
    "    tokens = list(word) + ['</w>']\n",
    "    \n",
    "    # Прилагаме merge правилата в реда, в който са научени\n",
    "    for pair in merges:\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            if tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                tokens = tokens[:i] + [''.join(pair)] + tokens[i+2:]\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Тестваме\n",
    "test_words = ['low', 'lower', 'lowest', 'newest', 'wider', 'unknown']\n",
    "\n",
    "print(\"BPE Encoding:\\n\")\n",
    "for word in test_words:\n",
    "    tokens = bpe_encode(word, merge_rules)\n",
    "    print(f\"  {word:12s} → {tokens}\")\n",
    "\n",
    "print(\"\\n'lowest' се токенизира въпреки че не е в корпуса!\")\n",
    "print(\"BPE може да обработи всяка дума чрез subword разбивка.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-steps-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация на BPE encoding стъпки\n",
    "def visualize_bpe_encoding(word, merges):\n",
    "    \"\"\"Показва стъпките на BPE encoding.\"\"\"\n",
    "    tokens = list(word) + ['</w>']\n",
    "    steps = [tokens.copy()]\n",
    "    \n",
    "    for pair in merges:\n",
    "        changed = False\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            if tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                tokens = tokens[:i] + [''.join(pair)] + tokens[i+2:]\n",
    "                changed = True\n",
    "            else:\n",
    "                i += 1\n",
    "        if changed:\n",
    "            steps.append(tokens.copy())\n",
    "    \n",
    "    return steps\n",
    "\n",
    "word = \"newest\"\n",
    "steps = visualize_bpe_encoding(word, merge_rules)\n",
    "\n",
    "print(f\"BPE encoding стъпки за '{word}':\\n\")\n",
    "for i, step in enumerate(steps):\n",
    "    prefix = \"Start:\" if i == 0 else f\"Step {i}:\"\n",
    "    print(f\"  {prefix:8s} {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-vocab-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация: Как расте речникът\n",
    "def track_vocab_growth(word_freq, num_merges):\n",
    "    \"\"\"Следи растежа на речника.\"\"\"\n",
    "    vocab = initialize_vocab(word_freq)\n",
    "    \n",
    "    # Начален брой символи\n",
    "    initial_symbols = set()\n",
    "    for word in vocab:\n",
    "        initial_symbols.update(word.split())\n",
    "    \n",
    "    sizes = [len(initial_symbols)]\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_pair_counts(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = pairs.most_common(1)[0][0]\n",
    "        vocab = merge_pair(best_pair, vocab)\n",
    "        sizes.append(sizes[-1] + 1)  # Всеки merge добавя 1 нов токен\n",
    "    \n",
    "    return sizes\n",
    "\n",
    "sizes = track_vocab_growth(word_freq, 10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(sizes)), sizes, 'b-o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Брой merge операции')\n",
    "plt.ylabel('Размер на речника')\n",
    "plt.title('BPE: Растеж на речника')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"BPE vocabulary = base_chars + num_merges\")\n",
    "print(\"GPT-2: 256 bytes + 50,000 merges = 50,256 токена\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unigram-header",
   "metadata": {},
   "source": [
    "### 4.2 Unigram Language Model Tokenization — Deep Dive\n",
    "\n",
    "**Философия:** Top-down, probabilistic, likelihood-based\n",
    "\n",
    "**Разлика от BPE:**\n",
    "- BPE: greedy, добавя токени (bottom-up)\n",
    "- Unigram: вероятностен, премахва токени (top-down)\n",
    "\n",
    "**Алгоритъм:**\n",
    "1. Започни с голям речник (всички символи + чести substrings)\n",
    "2. За всеки токен изчисли как се променя likelihood ако го премахнем\n",
    "3. Премахни токените с най-малък impact\n",
    "4. Повтаряй докато речникът достигне желания размер\n",
    "\n",
    "**Използва се от:** T5, ALBERT, mBART, XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unigram-multiple-seg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram: Множество възможни сегментации\n",
    "word = \"unhappiness\"\n",
    "\n",
    "segmentations = [\n",
    "    [\"un\", \"happiness\"],\n",
    "    [\"un\", \"happy\", \"ness\"],\n",
    "    [\"un\", \"happ\", \"iness\"],\n",
    "    [\"unhapp\", \"iness\"],\n",
    "    [\"u\", \"n\", \"h\", \"a\", \"p\", \"p\", \"i\", \"n\", \"e\", \"s\", \"s\"],\n",
    "]\n",
    "\n",
    "print(f\"Дума: '{word}'\\n\")\n",
    "print(\"Възможни сегментации:\")\n",
    "for seg in segmentations:\n",
    "    print(f\"  {seg}\")\n",
    "\n",
    "print(\"\\nUnigram избира сегментацията с най-висока вероятност!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unigram-math",
   "metadata": {},
   "source": [
    "### Unigram математика\n",
    "\n",
    "**Вероятност на сегментация:**\n",
    "$$P(x_1, x_2, ..., x_n) = \\prod_{i=1}^{n} P(x_i)$$\n",
    "\n",
    "**Най-вероятна сегментация:** (използва се Viterbi алгоритъм)\n",
    "$$x^* = \\arg\\max_x P(x)$$\n",
    "\n",
    "**Пример:**\n",
    "- P(\"un\") = 0.01, P(\"happiness\") = 0.001\n",
    "- P([\"un\", \"happiness\"]) = 0.01 × 0.001 = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unigram-probs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Симулация на Unigram вероятности\n",
    "# (В реалност се учат от корпус)\n",
    "\n",
    "token_probs = {\n",
    "    'un': 0.01,\n",
    "    'happiness': 0.001,\n",
    "    'happy': 0.005,\n",
    "    'ness': 0.008,\n",
    "    'happ': 0.0001,\n",
    "    'iness': 0.0002,\n",
    "    'unhapp': 0.00001,\n",
    "}\n",
    "\n",
    "def segmentation_prob(tokens, probs):\n",
    "    \"\"\"Изчислява вероятността на сегментация.\"\"\"\n",
    "    prob = 1.0\n",
    "    for t in tokens:\n",
    "        prob *= probs.get(t, 0.0001)  # fallback за непознати\n",
    "    return prob\n",
    "\n",
    "segmentations = [\n",
    "    [\"un\", \"happiness\"],\n",
    "    [\"un\", \"happy\", \"ness\"],\n",
    "    [\"un\", \"happ\", \"iness\"],\n",
    "    [\"unhapp\", \"iness\"],\n",
    "]\n",
    "\n",
    "print(\"Unigram вероятности на сегментации:\\n\")\n",
    "print(f\"{'Сегментация':<30} {'Вероятност':>15} {'Log-prob':>12}\")\n",
    "print(\"-\" * 58)\n",
    "\n",
    "best_prob = 0\n",
    "best_seg = None\n",
    "\n",
    "for seg in segmentations:\n",
    "    prob = segmentation_prob(seg, token_probs)\n",
    "    log_prob = np.log(prob) if prob > 0 else float('-inf')\n",
    "    print(f\"{str(seg):<30} {prob:>15.2e} {log_prob:>12.2f}\")\n",
    "    \n",
    "    if prob > best_prob:\n",
    "        best_prob = prob\n",
    "        best_seg = seg\n",
    "\n",
    "print(f\"\\nНай-вероятна сегментация: {best_seg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpe-vs-unigram",
   "metadata": {},
   "source": [
    "### 4.3 BPE vs Unigram: Сравнение\n",
    "\n",
    "| Характеристика | BPE | Unigram |\n",
    "|----------------|-----|--------|\n",
    "| **Подход** | Bottom-up (добавя) | Top-down (премахва) |\n",
    "| **Критерий** | Честота | Likelihood |\n",
    "| **Детерминистичен** | Да | Може да семплира |\n",
    "| **Множество токенизации** | Не | Да |\n",
    "| **Модели** | GPT, LLaMA | T5, ALBERT |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-vs-unigram-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуално сравнение на подходите\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# BPE: Bottom-up\n",
    "ax1 = axes[0]\n",
    "levels_bpe = ['a b c d e', 'a b c de', 'a bc de', 'abc de', 'abcde']\n",
    "\n",
    "for i, level in enumerate(levels_bpe):\n",
    "    ax1.text(0.5, i, level, ha='center', va='center', fontsize=14,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    if i < len(levels_bpe) - 1:\n",
    "        ax1.annotate('', xy=(0.5, i+0.7), xytext=(0.5, i+0.3),\n",
    "                    arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(-0.5, len(levels_bpe)-0.5)\n",
    "ax1.set_title('BPE: Bottom-up (merge)', fontsize=14)\n",
    "ax1.axis('off')\n",
    "ax1.text(0.5, -0.3, 'Merge най-честата двойка', ha='center', fontsize=11)\n",
    "\n",
    "# Unigram: Top-down\n",
    "ax2 = axes[1]\n",
    "levels_uni = ['abcde + abc + ab + a + ...', 'abcde + abc + ab + ...', \n",
    "              'abcde + ab + ...', 'abcde + ...', 'abcde']\n",
    "\n",
    "for i, level in enumerate(levels_uni):\n",
    "    color = 'lightgreen' if i == len(levels_uni)-1 else 'lightyellow'\n",
    "    ax2.text(0.5, i, level, ha='center', va='center', fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor=color, alpha=0.8))\n",
    "    if i < len(levels_uni) - 1:\n",
    "        ax2.annotate('', xy=(0.5, i+0.7), xytext=(0.5, i+0.3),\n",
    "                    arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(-0.5, len(levels_uni)-0.5)\n",
    "ax2.set_title('Unigram: Top-down (prune)', fontsize=14)\n",
    "ax2.axis('off')\n",
    "ax2.text(0.5, -0.3, 'Премахни токен с най-малък likelihood impact', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wordpiece-header",
   "metadata": {},
   "source": [
    "### 4.4 WordPiece (кратко)\n",
    "\n",
    "**Използва се от:** BERT, DistilBERT, ELECTRA\n",
    "\n",
    "**Подобен на BPE, но:**\n",
    "- Избира merge въз основа на likelihood gain, не честота\n",
    "- Използва `##` префикс за continuation tokens\n",
    "\n",
    "**Пример:** \"playing\" → [\"play\", \"##ing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wordpiece-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordPiece пример (симулация)\n",
    "print(\"WordPiece токенизация (BERT стил):\\n\")\n",
    "\n",
    "examples = [\n",
    "    (\"playing\", [\"play\", \"##ing\"]),\n",
    "    (\"unhappiness\", [\"un\", \"##hap\", \"##pi\", \"##ness\"]),\n",
    "    (\"tokenization\", [\"token\", \"##ization\"]),\n",
    "    (\"embeddings\", [\"em\", \"##bed\", \"##ding\", \"##s\"]),\n",
    "]\n",
    "\n",
    "for word, tokens in examples:\n",
    "    print(f\"  {word:15s} → {tokens}\")\n",
    "\n",
    "print(\"\\n'##' означава, че токенът продължава предишен (не е начало на дума)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Практическа токенизация\n",
    "\n",
    "### 5.1 Специални токени\n",
    "\n",
    "Всеки tokenizer има специални токени за различни цели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-tokens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Специални токени в различни модели\n",
    "special_tokens = {\n",
    "    'BERT': {\n",
    "        '[CLS]': 'Начало на input (за classification)',\n",
    "        '[SEP]': 'Разделител между изречения',\n",
    "        '[PAD]': 'Padding до еднаква дължина',\n",
    "        '[MASK]': 'Маскиран токен (за MLM)',\n",
    "        '[UNK]': 'Непознат токен',\n",
    "    },\n",
    "    'GPT': {\n",
    "        '<|endoftext|>': 'Край на текст/документ',\n",
    "        '<|startoftext|>': 'Начало на текст',\n",
    "    },\n",
    "    'LLaMA': {\n",
    "        '<s>': 'Начало на sequence (BOS)',\n",
    "        '</s>': 'Край на sequence (EOS)',\n",
    "        '<unk>': 'Непознат токен',\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Специални токени по модел:\\n\")\n",
    "for model, tokens in special_tokens.items():\n",
    "    print(f\"  {model}:\")\n",
    "    for token, desc in tokens.items():\n",
    "        print(f\"    {token:20s} → {desc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf-tokenizers-header",
   "metadata": {},
   "source": [
    "### 5.2 HuggingFace Tokenizers\n",
    "\n",
    "Най-популярната библиотека за работа с токенизатори."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка за инсталирани библиотеки\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    HF_AVAILABLE = True\n",
    "    print(\"HuggingFace transformers е наличен!\")\n",
    "except ImportError:\n",
    "    HF_AVAILABLE = False\n",
    "    print(\"HuggingFace transformers не е инсталиран.\")\n",
    "    print(\"Инсталирайте с: pip install transformers\")\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    TIKTOKEN_AVAILABLE = True\n",
    "    print(\"tiktoken е наличен!\")\n",
    "except ImportError:\n",
    "    TIKTOKEN_AVAILABLE = False\n",
    "    print(\"tiktoken не е инсталиран.\")\n",
    "    print(\"Инсталирайте с: pip install tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-bert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зареждаме BERT tokenizer\n",
    "if HF_AVAILABLE:\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    print(f\"BERT tokenizer:\")\n",
    "    print(f\"  Тип: WordPiece\")\n",
    "    print(f\"  Размер на речника: {bert_tokenizer.vocab_size:,}\")\n",
    "else:\n",
    "    print(\"BERT tokenizer не е наличен (transformers не е инсталиран)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert-tokenize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT токенизация\n",
    "if HF_AVAILABLE:\n",
    "    text = \"I love machine learning and natural language processing!\"\n",
    "    \n",
    "    tokens = bert_tokenizer.tokenize(text)\n",
    "    token_ids = bert_tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"Текст: '{text}'\\n\")\n",
    "    print(f\"Токени: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Брой токени: {len(tokens)}\")\n",
    "    \n",
    "    # Decode обратно\n",
    "    decoded = bert_tokenizer.decode(token_ids)\n",
    "    print(f\"\\nDecoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-gpt2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зареждаме GPT-2 tokenizer\n",
    "if HF_AVAILABLE:\n",
    "    gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    print(f\"GPT-2 tokenizer:\")\n",
    "    print(f\"  Тип: BPE (byte-level)\")\n",
    "    print(f\"  Размер на речника: {gpt2_tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-tokenizers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение: BERT vs GPT-2\n",
    "if HF_AVAILABLE:\n",
    "    text = \"Tokenization is fundamental to NLP.\"\n",
    "    \n",
    "    bert_tokens = bert_tokenizer.tokenize(text)\n",
    "    gpt2_tokens = gpt2_tokenizer.tokenize(text)\n",
    "    \n",
    "    print(f\"Текст: '{text}'\\n\")\n",
    "    print(f\"BERT (WordPiece):  {bert_tokens}\")\n",
    "    print(f\"GPT-2 (BPE):       {gpt2_tokens}\")\n",
    "    \n",
    "    print(f\"\\nБрой токени:\")\n",
    "    print(f\"  BERT:  {len(bert_tokens)}\")\n",
    "    print(f\"  GPT-2: {len(gpt2_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение с повече примери\n",
    "if HF_AVAILABLE:\n",
    "    examples = [\n",
    "        \"Hello world!\",\n",
    "        \"Machine learning is amazing.\",\n",
    "        \"Unhappiness leads to introspection.\",\n",
    "        \"COVID-19 pandemic affected everyone.\",\n",
    "        \"Supercalifragilisticexpialidocious\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Сравнение BERT vs GPT-2:\\n\")\n",
    "    print(f\"{'Текст':<45} {'BERT':>6} {'GPT-2':>6}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for text in examples:\n",
    "        bert_len = len(bert_tokenizer.tokenize(text))\n",
    "        gpt2_len = len(gpt2_tokenizer.tokenize(text))\n",
    "        display = text[:42] + \"...\" if len(text) > 45 else text\n",
    "        print(f\"{display:<45} {bert_len:>6} {gpt2_len:>6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tiktoken-header",
   "metadata": {},
   "source": [
    "### 5.3 tiktoken (OpenAI)\n",
    "\n",
    "Бърз BPE tokenizer, използван от GPT-3.5/4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tiktoken-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken демонстрация\n",
    "if TIKTOKEN_AVAILABLE:\n",
    "    import tiktoken\n",
    "    \n",
    "    # Encoding за GPT-4\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    text = \"Hello, how are you doing today?\"\n",
    "    tokens = enc.encode(text)\n",
    "    \n",
    "    print(f\"tiktoken (GPT-4 encoding):\\n\")\n",
    "    print(f\"  Текст: '{text}'\")\n",
    "    print(f\"  Token IDs: {tokens}\")\n",
    "    print(f\"  Брой токени: {len(tokens)}\")\n",
    "    \n",
    "    # Показваме какво е всеки токен\n",
    "    print(f\"\\n  Токени:\")\n",
    "    for tid in tokens:\n",
    "        token_bytes = enc.decode_single_token_bytes(tid)\n",
    "        print(f\"    {tid:6d} → {token_bytes}\")\n",
    "else:\n",
    "    print(\"tiktoken не е инсталиран.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocab-size-header",
   "metadata": {},
   "source": [
    "### 5.4 Training Custom Tokenizer\n",
    "\n",
    "**Кога да обучавате собствен tokenizer?**\n",
    "- Специализиран домейн (код, медицински, правен текст)\n",
    "- Език, който не е добре представен в pre-trained tokenizers\n",
    "- Много специфичен vocabulary\n",
    "\n",
    "**Обичайно:** Използвайте готов tokenizer от модела, който ще използвате!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Токенизация в съвременни LLM\n",
    "\n",
    "### 6.1 Сравнение на LLM токенизатори"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-tokenizers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Таблица с токенизатори на популярни модели\n",
    "llm_tokenizers = {\n",
    "    'GPT-2': {'type': 'BPE', 'vocab': 50257, 'note': 'Byte-level'},\n",
    "    'GPT-3.5/4': {'type': 'BPE', 'vocab': 100000, 'note': 'cl100k_base'},\n",
    "    'BERT': {'type': 'WordPiece', 'vocab': 30522, 'note': 'Uncased'},\n",
    "    'T5': {'type': 'Unigram', 'vocab': 32000, 'note': 'SentencePiece'},\n",
    "    'LLaMA': {'type': 'BPE', 'vocab': 32000, 'note': 'SentencePiece'},\n",
    "    'LLaMA 2/3': {'type': 'BPE', 'vocab': 32000, 'note': 'SentencePiece'},\n",
    "    'Mistral': {'type': 'BPE', 'vocab': 32000, 'note': 'SentencePiece'},\n",
    "    'Claude': {'type': 'BPE', 'vocab': '~100k', 'note': 'Byte-level'},\n",
    "}\n",
    "\n",
    "print(\"Токенизатори в популярни LLM:\\n\")\n",
    "print(f\"{'Модел':<15} {'Тип':<12} {'Vocab Size':<12} {'Бележка'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for model, info in llm_tokenizers.items():\n",
    "    vocab = str(info['vocab'])\n",
    "    print(f\"{model:<15} {info['type']:<12} {vocab:<12} {info['note']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocab-tradeoff",
   "metadata": {},
   "source": [
    "### 6.2 Vocabulary size trade-off\n",
    "\n",
    "| По-малък речник (30K) | По-голям речник (100K) |\n",
    "|----------------------|------------------------|\n",
    "| По-дълги последователности | По-кратки последователности |\n",
    "| По-малко параметри в embedding | Повече параметри |\n",
    "| По-добра генерализация | По-добро покритие |\n",
    "\n",
    "**GPT-2:** vocab = 50,257 = 256 bytes + 50,000 merges + 1 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocab-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация: Impact на vocabulary size\n",
    "vocab_sizes = [1000, 10000, 30000, 50000, 100000]\n",
    "embedding_dim = 768  # BERT-base\n",
    "\n",
    "# Параметри в embedding layer = vocab_size × embedding_dim\n",
    "params = [v * embedding_dim / 1e6 for v in vocab_sizes]  # в милиони\n",
    "\n",
    "# Примерна дължина на последователност (обратно пропорционална)\n",
    "seq_lengths = [100, 40, 25, 20, 15]  # примерни стойности\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Embedding parameters\n",
    "axes[0].bar([f'{v//1000}K' for v in vocab_sizes], params, color='steelblue')\n",
    "axes[0].set_xlabel('Vocabulary Size')\n",
    "axes[0].set_ylabel('Embedding параметри (милиони)')\n",
    "axes[0].set_title('Параметри в Embedding Layer')\n",
    "\n",
    "# Sequence length\n",
    "axes[1].bar([f'{v//1000}K' for v in vocab_sizes], seq_lengths, color='coral')\n",
    "axes[1].set_xlabel('Vocabulary Size')\n",
    "axes[1].set_ylabel('Средна дължина на sequence')\n",
    "axes[1].set_title('Дължина на токенизиран текст')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"По-голям речник = повече памет за embeddings, но по-кратки sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token-limits",
   "metadata": {},
   "source": [
    "### 6.3 Token limits и context windows\n",
    "\n",
    "**Важно:** Когато API казва \"128K tokens\", това са **токени**, не думи!\n",
    "\n",
    "- Средно 1 токен ≈ 4 символа (английски)\n",
    "- ~75 думи ≈ 100 токена\n",
    "- За други езици съотношението е различно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "token-cost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Демонстрация: Токени vs думи\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming industries.\",\n",
    "    \"def fibonacci(n): return n if n < 2 else fibonacci(n-1) + fibonacci(n-2)\",\n",
    "]\n",
    "\n",
    "print(\"Токени vs символи vs думи:\\n\")\n",
    "print(f\"{'Текст':<55} {'Chars':>6} {'Words':>6} {'Tokens':>7}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "if HF_AVAILABLE:\n",
    "    for text in sample_texts:\n",
    "        chars = len(text)\n",
    "        words = len(text.split())\n",
    "        tokens = len(gpt2_tokenizer.tokenize(text))\n",
    "        \n",
    "        display = text[:52] + \"...\" if len(text) > 55 else text\n",
    "        print(f\"{display:<55} {chars:>6} {words:>6} {tokens:>7}\")\n",
    "    \n",
    "    print(f\"\\nСредно chars/token: ~4 за английски текст\")\n",
    "else:\n",
    "    print(\"(GPT-2 tokenizer не е наличен)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Проблеми и Pitfalls\n",
    "\n",
    "### 7.1 Multilingual bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multilingual-bias",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilingual tokenization bias\n",
    "sentences = {\n",
    "    'English': \"Hello, how are you today?\",\n",
    "    'Bulgarian': \"Здравейте, как сте днес?\",\n",
    "    'German': \"Hallo, wie geht es Ihnen heute?\",\n",
    "    'Chinese': \"你好，今天怎么样？\",\n",
    "}\n",
    "\n",
    "if HF_AVAILABLE:\n",
    "    print(\"Multilingual Tokenization Bias:\\n\")\n",
    "    print(f\"{'Език':<12} {'Символи':>10} {'GPT-2 токени':>15} {'Chars/Token':>12}\")\n",
    "    print(\"-\" * 52)\n",
    "    \n",
    "    for lang, text in sentences.items():\n",
    "        chars = len(text)\n",
    "        tokens = len(gpt2_tokenizer.tokenize(text))\n",
    "        ratio = chars / tokens if tokens > 0 else 0\n",
    "        print(f\"{lang:<12} {chars:>10} {tokens:>15} {ratio:>12.1f}\")\n",
    "    \n",
    "    print(\"\\nНе-латински езици се токенизират по-'скъпо':\")\n",
    "    print(\"  - Повече токени за същото съдържание\")\n",
    "    print(\"  - По-бързо изчерпване на context window\")\n",
    "    print(\"  - По-висока цена при API calls\")\n",
    "else:\n",
    "    print(\"(Демонстрация изисква HuggingFace transformers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "byte-level-bpe",
   "metadata": {},
   "source": [
    "### 7.2 Byte-level BPE\n",
    "\n",
    "**Проблем:** Как да обработим всеки Unicode символ без [UNK]?\n",
    "\n",
    "**Решение (GPT-2):** Byte-level BPE\n",
    "- Работи върху bytes (256 възможни стойности)\n",
    "- Може да токенизира всичко\n",
    "- Никога не връща [UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "byte-level-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte-level BPE: може да обработи всичко\n",
    "weird_texts = [\n",
    "    \"Hello World! \",\n",
    "    \"Math: x + y = z\",\n",
    "    \"Mixed text with Chinese\",\n",
    "    \"<script>alert('xss')</script>\",\n",
    "]\n",
    "\n",
    "if HF_AVAILABLE:\n",
    "    print(\"Byte-level BPE: обработва всичко!\\n\")\n",
    "    for text in weird_texts:\n",
    "        tokens = gpt2_tokenizer.tokenize(text)\n",
    "        print(f\"  '{text}'\")\n",
    "        print(f\"  → {tokens}\\n\")\n",
    "    \n",
    "    print(\"GPT-2 никога не връща [UNK] — всичко се разбива до bytes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pitfalls-header",
   "metadata": {},
   "source": [
    "### 7.3 Чести грешки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pitfall-spaces",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 1: Trailing spaces matter!\n",
    "if HF_AVAILABLE:\n",
    "    texts = [\"Hello\", \"Hello \", \" Hello\"]\n",
    "    \n",
    "    print(\"Pitfall: Spaces matter!\\n\")\n",
    "    for text in texts:\n",
    "        tokens = gpt2_tokenizer.tokenize(text)\n",
    "        print(f\"  '{text}' ({len(text)} chars) → {tokens}\")\n",
    "    \n",
    "    print(\"\\nВодещ/следващ space може да промени токенизацията!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pitfall-case",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 2: Case sensitivity\n",
    "if HF_AVAILABLE:\n",
    "    texts = [\"Hello\", \"hello\", \"HELLO\"]\n",
    "    \n",
    "    print(\"Pitfall: Case sensitivity\\n\")\n",
    "    print(\"GPT-2 (case-sensitive):\")\n",
    "    for text in texts:\n",
    "        tokens = gpt2_tokenizer.tokenize(text)\n",
    "        print(f\"  '{text}' → {tokens}\")\n",
    "    \n",
    "    print(\"\\nBERT uncased (case-insensitive):\")\n",
    "    for text in texts:\n",
    "        tokens = bert_tokenizer.tokenize(text)\n",
    "        print(f\"  '{text}' → {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pitfall-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 3: Numbers are tricky\n",
    "if HF_AVAILABLE:\n",
    "    numbers = [\"123\", \"1234\", \"12345\", \"123456\", \"1000000\"]\n",
    "    \n",
    "    print(\"Pitfall: Числата се токенизират непредвидимо\\n\")\n",
    "    for num in numbers:\n",
    "        tokens = gpt2_tokenizer.tokenize(num)\n",
    "        print(f\"  '{num}' → {tokens} ({len(tokens)} токена)\")\n",
    "    \n",
    "    print(\"\\nLLM не 'виждат' числа като математически стойности,\")\n",
    "    print(\"а като последователност от токени!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Обобщение и мост към следващата лекция\n",
    "\n",
    "### Ключови изводи\n",
    "\n",
    "**1. Токенизацията е нетривиален проблем**\n",
    "- Word-level: огромен речник, OOV проблем\n",
    "- Character-level: дълги последователности\n",
    "- Subword: балансирано решение\n",
    "\n",
    "**2. BPE и Unigram са основните алгоритми**\n",
    "- BPE: greedy, bottom-up, най-популярен (GPT, LLaMA)\n",
    "- Unigram: probabilistic, top-down (T5, mBART)\n",
    "- WordPiece: среден вариант (BERT)\n",
    "\n",
    "**3. Практически съображения**\n",
    "- Токени ≠ думи (важно за API costs!)\n",
    "- Multilingual bias е реален проблем\n",
    "- Spaces и case могат да объркат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-lecture",
   "metadata": {},
   "source": [
    "### Следваща лекция: Механизми на внимание (Attention)\n",
    "\n",
    "**Въпроси, на които ще отговорим:**\n",
    "\n",
    "- Какви са ограниченията на mean pooling?\n",
    "- Как self-attention позволява на токените да \"комуникират\"?\n",
    "- Какво са Query, Key, Value?\n",
    "- Как работи Multi-Head Attention?\n",
    "\n",
    "**Защо е важно:**\n",
    "- Attention е основата на Transformer архитектурата\n",
    "- Всички съвременни LLM (GPT, BERT, LLaMA) използват attention\n",
    "\n",
    "```\n",
    "Текст → Токени → Embeddings → ??? → Предсказания\n",
    "       (Лекция 3)                 (Лекция 4: Attention!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "## Ресурси\n",
    "\n",
    "### Статии\n",
    "1. Sennrich et al. (2016) — \"Neural Machine Translation of Rare Words with Subword Units\" (BPE)\n",
    "2. Kudo (2018) — \"Subword Regularization: Improving Neural Network Translation Models\" (Unigram)\n",
    "3. Radford et al. (2019) — \"Language Models are Unsupervised Multitask Learners\" (GPT-2 byte-level BPE)\n",
    "\n",
    "### Документация\n",
    "1. HuggingFace Tokenizers: https://huggingface.co/docs/tokenizers/\n",
    "2. tiktoken: https://github.com/openai/tiktoken\n",
    "3. SentencePiece: https://github.com/google/sentencepiece\n",
    "\n",
    "### Интерактивни инструменти\n",
    "1. OpenAI Tokenizer: https://platform.openai.com/tokenizer\n",
    "2. HuggingFace tokenizer playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "---\n",
    "## Упражнения\n",
    "\n",
    "### Упражнение 1: Имплементирайте BPE от нулата\n",
    "- Използвайте по-голям корпус\n",
    "- Обучете BPE с 1000 merge операции\n",
    "- Токенизирайте нов текст и анализирайте резултата\n",
    "\n",
    "### Упражнение 2: Сравнете токенизатори\n",
    "- Токенизирайте един и същ текст с BERT, GPT-2, T5\n",
    "- Сравнете броя токени и какви са те\n",
    "- Кой работи най-добре за български текст?\n",
    "\n",
    "### Упражнение 3: Анализ на multilingual bias\n",
    "- Токенизирайте еднакво дълги текстове на 5 езика\n",
    "- Изчислете средния брой токени на символ\n",
    "- Какви са последиците за API costs?\n",
    "\n",
    "### Упражнение 4: Debug tokenization issues\n",
    "- Намерете примери с проблемна токенизация\n",
    "- Trailing spaces, special characters, numbers\n",
    "- Обяснете защо се случва и как да се избегне"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "end",
   "metadata": {},
   "source": [
    "---\n",
    "## Край на Лекция 3\n",
    "\n",
    "**Въпроси?**\n",
    "\n",
    "---\n",
    "\n",
    "**Следваща лекция:** Механизми на внимание (Attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
