{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# –õ–µ–∫—Ü–∏—è 4: –ù–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏\n",
    "\n",
    "## –û—Ç –ª–∏–Ω–µ–π–Ω–∏ –º–æ–¥–µ–ª–∏ –¥–æ –¥—ä–ª–±–æ–∫–æ –æ–±—É—á–µ–Ω–∏–µ: Neurons, Layers, Backpropagation\n",
    "\n",
    "**–ü—Ä–æ–¥—ä–ª–∂–∏—Ç–µ–ª–Ω–æ—Å—Ç:** 2-2.5 —á–∞—Å–∞  \n",
    "**–ü—Ä–µ–¥–ø–æ—Å—Ç–∞–≤–∫–∏:** –õ–µ–∫—Ü–∏—è 3 (–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è), –æ—Å–Ω–æ–≤–∏ –Ω–∞ –∫–∞–ª–∫—É–ª—É—Å –∏ –ª–∏–Ω–µ–π–Ω–∞ –∞–ª–≥–µ–±—Ä–∞  \n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–∏ (Attention, BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives",
   "metadata": {},
   "source": [
    "---\n",
    "## –¶–µ–ª–∏ –Ω–∞ –ª–µ–∫—Ü–∏—è—Ç–∞\n",
    "\n",
    "–°–ª–µ–¥ —Ç–∞–∑–∏ –ª–µ–∫—Ü–∏—è —â–µ –º–æ–∂–µ—Ç–µ:\n",
    "\n",
    "- –û–±—è—Å–Ω—è–≤–∞—Ç–µ –∑–∞—â–æ –Ω–µ–≤—Ä–æ–Ω–Ω–∏—Ç–µ –º—Ä–µ–∂–∏ –Ω–∞–¥–≥—Ä–∞–∂–¥–∞—Ç –ª–∏–Ω–µ–π–Ω–∏—Ç–µ –º–æ–¥–µ–ª–∏\n",
    "- –ò–∑–≥—Ä–∞–∂–¥–∞—Ç–µ feedforward –º—Ä–µ–∂–∞ –æ—Ç –Ω—É–ª–∞—Ç–∞ —Å NumPy\n",
    "- –ü—Ä–∏–ª–∞–≥–∞—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç—ä–º–∞ –∑–∞ –æ–±—Ä–∞—Ç–Ω–æ —Ä–∞–∑–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ —Å–∏–≥–Ω–∞–ª–∞ (backpropagation)\n",
    "- –°—Ä–∞–≤–Ω—è–≤–∞—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∏: SGD, Momentum, Adam\n",
    "- –†–∞–∑—á–∏—Ç–∞—Ç–µ training curves –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–∞—Ç–µ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ\n",
    "- –ü—Ä–∏–ª–∞–≥–∞—Ç–µ –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞ –∑–∞ —Ç–µ–∫—Å—Ç–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roadmap",
   "metadata": {},
   "source": [
    "### –ü—ä—Ç–Ω–∞ –∫–∞—Ä—Ç–∞ –∑–∞ –¥–Ω–µ—Å\n",
    "\n",
    "```\n",
    "–ú–æ—Ç–∏–≤–∞—Ü–∏—è ‚Üí –ù–µ–≤—Ä–æ–Ω–∏ ‚Üí Forward Pass ‚Üí Loss ‚Üí Backprop ‚Üí –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∏ ‚Üí Training ‚Üí NLP\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn –∑–∞ –¥–∞–Ω–Ω–∏ –∏ baseline –º–æ–¥–µ–ª–∏\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# –§–∏–∫—Å–∏—Ä–∞–º–µ random seed –∑–∞ –≤—ä–∑–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì –í—Å–∏—á–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏ —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. –ú–æ—Ç–∏–≤–∞—Ü–∏—è: –û—Ç –ª–∏–Ω–µ–π–Ω–∏ –º–æ–¥–µ–ª–∏ –∫—ä–º –Ω–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏\n",
    "\n",
    "### –ö–∞–∫–≤–æ –Ω–∞—É—á–∏—Ö–º–µ –¥–æ—Å–µ–≥–∞?\n",
    "\n",
    "| –õ–µ–∫—Ü–∏—è | –ö–æ–Ω—Ü–µ–ø—Ü–∏—è | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ |\n",
    "|--------|-----------|-------------|\n",
    "| **1** | Logistic Regression + BoW | –õ–∏–Ω–µ–µ–Ω –º–æ–¥–µ–ª, –Ω—è–º–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è |\n",
    "| **2** | Word2Vec embeddings | –°—Ç–∞—Ç–∏—á–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏, –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç |\n",
    "| **3** | Tokenization (BPE) | –ü–æ–¥–≥–æ—Ç–≤—è –≤—Ö–æ–¥–∞, –Ω–æ –Ω–µ –º–æ–¥–µ–ª–∏—Ä–∞ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-limitation",
   "metadata": {},
   "source": [
    "### –ó–∞—â–æ –ª–∏–Ω–µ–π–Ω–∏—Ç–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å–∞ –¥–æ—Å—Ç–∞—Ç—ä—á–Ω–∏?\n",
    "\n",
    "**–õ–∏–Ω–µ–µ–Ω –º–æ–¥–µ–ª:** $\\hat{y} = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º:** –ú–æ–∂–µ —Å–∞–º–æ –¥–∞ —Ä–∞–∑–¥–µ–ª–∏ –¥–∞–Ω–Ω–∏—Ç–µ —Å —Ö–∏–ø–µ—Ä—Ä–∞–≤–Ω–∏–Ω–∞.\n",
    "\n",
    "–ù–µ –º–æ–∂–µ –¥–∞ —É–ª–æ–≤–∏:\n",
    "- **–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è:** \"not good\" ‚â† \"not\" + \"good\"\n",
    "- **–ô–µ—Ä–∞—Ä—Ö–∏—è:** –Ω–∏—Å–∫–æ –Ω–∏–≤–æ (–±—É–∫–≤–∏) ‚Üí —Å—Ä–µ–¥–Ω–æ (–¥—É–º–∏) ‚Üí –≤–∏—Å–æ–∫–æ (—Ç–µ–º–∏)\n",
    "- **–ù–µ–ª–∏–Ω–µ–π–Ω–∏ –≥—Ä–∞–Ω–∏—Ü–∏:** XOR –ø—Ä–æ–±–ª–µ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xor-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: XOR –ø—Ä–æ–±–ª–µ–º - –ª–∏–Ω–µ–π–Ω–æ –Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])  # XOR: —Ä–∞–∑–ª–∏—á–Ω–∏ = 1, –µ–¥–Ω–∞–∫–≤–∏ = 0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# –õ—è–≤–æ: XOR –¥–∞–Ω–Ω–∏\n",
    "colors = ['red' if y == 0 else 'blue' for y in y_xor]\n",
    "axes[0].scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
    "axes[0].set_xlabel('$x_1$')\n",
    "axes[0].set_ylabel('$x_2$')\n",
    "axes[0].set_title('XOR: –ú–æ–∂–µ –ª–∏ –µ–¥–Ω–∞ –ø—Ä–∞–≤–∞ –¥–∞ —Ä–∞–∑–¥–µ–ª–∏ —Ü–≤–µ—Ç–æ–≤–µ—Ç–µ?')\n",
    "axes[0].set_xlim(-0.5, 1.5)\n",
    "axes[0].set_ylim(-0.5, 1.5)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä–Ω–∏ (–≥—Ä–µ—à–Ω–∏) –ª–∏–Ω–µ–π–Ω–∏ –≥—Ä–∞–Ω–∏—Ü–∏\n",
    "x_line = np.linspace(-0.5, 1.5, 100)\n",
    "for slope, intercept, ls in [(-1, 0.5, '--'), (1, -0.5, ':'), (-1, 1.5, '-.')]:\n",
    "    axes[0].plot(x_line, slope * x_line + intercept, ls, alpha=0.5, color='gray')\n",
    "\n",
    "axes[0].legend(['–û–ø–∏—Ç 1', '–û–ø–∏—Ç 2', '–û–ø–∏—Ç 3'], loc='upper right')\n",
    "\n",
    "# –î—è—Å–Ω–æ: –†–µ—à–µ–Ω–∏–µ —Å –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞\n",
    "axes[1].scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
    "axes[1].set_xlabel('$x_1$')\n",
    "axes[1].set_ylabel('$x_2$')\n",
    "axes[1].set_title('–ù–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞: –Ω–µ–ª–∏–Ω–µ–π–Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞')\n",
    "axes[1].set_xlim(-0.5, 1.5)\n",
    "axes[1].set_ylim(-0.5, 1.5)\n",
    "\n",
    "# –†–∏—Å—É–≤–∞–º–µ –Ω–µ–ª–∏–Ω–µ–π–Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞ (—Ä—ä—á–Ω–æ)\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
    "Z = np.logical_xor(xx > 0.5, yy > 0.5).astype(float)\n",
    "axes[1].contourf(xx, yy, Z, levels=[0, 0.5, 1], colors=['#ffcccc', '#ccccff'], alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° XOR –µ –ª–∏–Ω–µ–π–Ω–æ –Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º. –ù–µ–≤—Ä–æ–Ω–Ω–∞—Ç–∞ –º—Ä–µ–∂–∞ –º–æ–∂–µ –¥–∞ —Å—ä–∑–¥–∞–¥–µ –Ω–µ–ª–∏–Ω–µ–π–Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn-offer",
   "metadata": {},
   "source": [
    "### –ö–∞–∫–≤–æ –ø—Ä–µ–¥–ª–∞–≥–∞—Ç –Ω–µ–≤—Ä–æ–Ω–Ω–∏—Ç–µ –º—Ä–µ–∂–∏?\n",
    "\n",
    "| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | –õ–∏–Ω–µ–µ–Ω –º–æ–¥–µ–ª | –ù–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞ |\n",
    "|----------------|--------------|----------------|\n",
    "| **–ì—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ** | –•–∏–ø–µ—Ä—Ä–∞–≤–Ω–∏–Ω–∞ | –ü—Ä–æ–∏–∑–≤–æ–ª–Ω–∞ —Ñ–æ—Ä–º–∞ |\n",
    "| **–ü—Ä–∏–∑–Ω–∞—Ü–∏** | –†—ä—á–Ω–æ —Å—ä–∑–¥–∞–¥–µ–Ω–∏ | –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –Ω–∞—É—á–µ–Ω–∏ |\n",
    "| **–ö–æ–º–ø–æ–∑–∏—Ü–∏—è** | $f(x) = Wx$ | $f(x) = f_n(f_{n-1}(...f_1(x)))$ |\n",
    "| **–ò–∑—Ä–∞–∑–∏—Ç–µ–ª–Ω–æ—Å—Ç** | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ | –£–Ω–∏–≤–µ—Ä—Å–∞–ª–µ–Ω –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ç–æ—Ä |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "path-to-llm",
   "metadata": {},
   "source": [
    "### –ü—ä—Ç—è—Ç –∫—ä–º LLM\n",
    "\n",
    "```\n",
    "–ù–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏ (–¥–Ω–µ—Å)\n",
    "    ‚Üì\n",
    "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–∏ (–õ–µ–∫—Ü–∏—è 5)\n",
    "    ‚Üì\n",
    "–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª–Ω–æ –æ–±—É—á–µ–Ω–∏–µ (–õ–µ–∫—Ü–∏—è 6)\n",
    "    ‚Üì\n",
    "GPT, Claude, LLaMA (–õ–µ–∫—Ü–∏–∏ 7-12)\n",
    "```\n",
    "\n",
    "**–í–∞–∂–Ω–æ:** –í—Å–∏—á–∫–æ, –∫–æ–µ—Ç–æ –Ω–∞—É—á–∏–º –¥–Ω–µ—Å (forward pass, backprop, Adam), —Å–µ –∏–∑–ø–æ–ª–∑–≤–∞ –∏ –≤ –Ω–∞–π-–≥–æ–ª–µ–º–∏—Ç–µ LLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. –ì—Ä–∞–¥–∏–≤–Ω–∏ –µ–ª–µ–º–µ–Ω—Ç–∏: –ù–µ–≤—Ä–æ–Ω–∏ –∏ —Å–ª–æ–µ–≤–µ\n",
    "\n",
    "### –ö–∞–∫–≤–æ –µ –Ω–µ–≤—Ä–æ–Ω?\n",
    "\n",
    "**–ù–µ–≤—Ä–æ–Ω** = –ø—Ä–µ—Ç–µ–≥–ª–µ–Ω–∞ —Å—É–º–∞ + –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç\n",
    "\n",
    "$$z = \\sum_{i=1}^{n} w_i x_i + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "$$a = f(z)$$\n",
    "\n",
    "–ö—ä–¥–µ—Ç–æ:\n",
    "- $\\mathbf{x}$ ‚Äî –≤—Ö–æ–¥ (–≤–µ–∫—Ç–æ—Ä)\n",
    "- $\\mathbf{w}$ ‚Äî —Ç–µ–≥–ª–∞ (weights)\n",
    "- $b$ ‚Äî bias\n",
    "- $f$ ‚Äî –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-neuron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ—Å—Ç –Ω–µ–≤—Ä–æ–Ω\n",
    "def neuron(x, w, b, activation='relu'):\n",
    "    \"\"\"–ï–¥–∏–Ω –Ω–µ–≤—Ä–æ–Ω: z = w¬∑x + b, a = f(z)\"\"\"\n",
    "    z = np.dot(w, x) + b\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        a = np.maximum(0, z)\n",
    "    elif activation == 'sigmoid':\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "    else:\n",
    "        a = z  # linear\n",
    "    \n",
    "    return z, a\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä\n",
    "x = np.array([1.0, 2.0, 3.0])  # 3 –≤—Ö–æ–¥–∞\n",
    "w = np.array([0.5, -0.3, 0.8])  # 3 —Ç–µ–≥–ª–∞\n",
    "b = 0.1  # bias\n",
    "\n",
    "z, a = neuron(x, w, b, activation='relu')\n",
    "\n",
    "print(\"üß† –ï–¥–∏–Ω –Ω–µ–≤—Ä–æ–Ω:\\n\")\n",
    "print(f\"   –í—Ö–æ–¥ x = {x}\")\n",
    "print(f\"   –¢–µ–≥–ª–∞ w = {w}\")\n",
    "print(f\"   Bias b = {b}\")\n",
    "print(f\"\\n   z = w¬∑x + b = {w[0]}√ó{x[0]} + {w[1]}√ó{x[1]} + {w[2]}√ó{x[2]} + {b}\")\n",
    "print(f\"   z = {z:.2f}\")\n",
    "print(f\"   a = ReLU(z) = max(0, {z:.2f}) = {a:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activation-functions",
   "metadata": {},
   "source": [
    "### –ê–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∏ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "\n",
    "**–ó–∞—â–æ —Å–∞ –Ω—É–∂–Ω–∏?** –ë–µ–∑ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç, –º–Ω–æ–≥–æ —Å–ª–æ–µ–≤–µ = –µ–¥–∏–Ω –ª–∏–Ω–µ–µ–Ω —Å–ª–æ–π.\n",
    "\n",
    "$$f(W_2 \\cdot f(W_1 \\cdot x)) = f(W_2 W_1 x) = f(W x)$$\n",
    "\n",
    "| –§—É–Ω–∫—Ü–∏—è | –§–æ—Ä–º—É–ª–∞ | –î–∏–∞–ø–∞–∑–æ–Ω | –ò–∑–ø–æ–ª–∑–≤–∞ —Å–µ |\n",
    "|---------|---------|----------|-------------|\n",
    "| **Sigmoid** | $\\frac{1}{1+e^{-z}}$ | (0, 1) | –ò–∑—Ö–æ–¥ –∑–∞ binary classification |\n",
    "| **Tanh** | $\\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | (-1, 1) | –ü–æ-–¥–æ–±—Ä–∞ –æ—Ç sigmoid (—Ü–µ–Ω—Ç—Ä–∏—Ä–∞–Ω–∞) |\n",
    "| **ReLU** | $\\max(0, z)$ | [0, ‚àû) | **–°—Ç–∞–Ω–¥–∞—Ä—Ç** –≤ —Å–∫—Ä–∏—Ç–∏—Ç–µ —Å–ª–æ–µ–≤–µ |\n",
    "| **GELU** | $z \\cdot \\Phi(z)$ | (-0.17, ‚àû) | GPT, BERT |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∏ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "# –î–µ—Ñ–∏–Ω–∏—Ü–∏–∏\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "tanh = np.tanh(z)\n",
    "relu = np.maximum(0, z)\n",
    "leaky_relu = np.where(z > 0, z, 0.01 * z)\n",
    "\n",
    "# GELU –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è\n",
    "gelu = 0.5 * z * (1 + np.tanh(np.sqrt(2/np.pi) * (z + 0.044715 * z**3)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏–∏\n",
    "axes[0].plot(z, sigmoid, label='Sigmoid', linewidth=2)\n",
    "axes[0].plot(z, tanh, label='Tanh', linewidth=2)\n",
    "axes[0].plot(z, relu, label='ReLU', linewidth=2)\n",
    "axes[0].plot(z, gelu, label='GELU (GPT)', linewidth=2, linestyle='--')\n",
    "axes[0].axhline(y=0, color='gray', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='gray', linewidth=0.5)\n",
    "axes[0].set_xlabel('z')\n",
    "axes[0].set_ylabel('f(z)')\n",
    "axes[0].set_title('–ê–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∏ —Ñ—É–Ω–∫—Ü–∏–∏')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(-1.5, 3)\n",
    "\n",
    "# –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∏ (–≤–∞–∂–Ω–∏ –∑–∞ backprop)\n",
    "sigmoid_deriv = sigmoid * (1 - sigmoid)\n",
    "tanh_deriv = 1 - tanh**2\n",
    "relu_deriv = (z > 0).astype(float)\n",
    "\n",
    "axes[1].plot(z, sigmoid_deriv, label=\"Sigmoid'\", linewidth=2)\n",
    "axes[1].plot(z, tanh_deriv, label=\"Tanh'\", linewidth=2)\n",
    "axes[1].plot(z, relu_deriv, label=\"ReLU'\", linewidth=2)\n",
    "axes[1].axhline(y=0, color='gray', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='gray', linewidth=0.5)\n",
    "axes[1].set_xlabel('z')\n",
    "axes[1].set_ylabel(\"f'(z)\")\n",
    "axes[1].set_title('–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∏ (–≤–∞–∂–Ω–∏ –∑–∞ backpropagation)')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
    "print(\"   ‚Ä¢ Sigmoid/Tanh: –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—Ç–∞ ‚Üí 0 –∑–∞ |z| > 3 (vanishing gradient!)\")\n",
    "print(\"   ‚Ä¢ ReLU: –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—Ç–∞ = 1 –∑–∞ z > 0 (–≥—Ä–∞–¥–∏–µ–Ω—Ç—ä—Ç —Å–µ –∑–∞–ø–∞–∑–≤–∞)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanishing-gradient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: Vanishing gradient problem\n",
    "print(\"‚ö†Ô∏è Vanishing Gradient Problem:\\n\")\n",
    "\n",
    "# Sigmoid –ø—Ä–∏ –≥–æ–ª–µ–º–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏\n",
    "z_values = [0, 2, 5, 10]\n",
    "\n",
    "print(f\"{'z':>6} | {'sigmoid(z)':>12} | {'sigmoid\\'(z)':>12}\")\n",
    "print(\"-\" * 36)\n",
    "\n",
    "for z_val in z_values:\n",
    "    sig = 1 / (1 + np.exp(-z_val))\n",
    "    sig_deriv = sig * (1 - sig)\n",
    "    print(f\"{z_val:>6} | {sig:>12.6f} | {sig_deriv:>12.6f}\")\n",
    "\n",
    "print(\"\\nüí° –ü—Ä–∏ z=10, –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—Ç–∞ –µ ~0.00005. –ì—Ä–∞–¥–∏–µ–Ω—Ç—ä—Ç –∏–∑—á–µ–∑–≤–∞!\")\n",
    "print(\"   ReLU —Ä–µ—à–∞–≤–∞ —Ç–æ–∑–∏ –ø—Ä–æ–±–ª–µ–º: f'(z) = 1 –∑–∞ z > 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "layers-section",
   "metadata": {},
   "source": [
    "### –°–ª–æ–µ–≤–µ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "**Feedforward –º—Ä–µ–∂–∞** (–º–Ω–æ–≥–æ—Å–ª–æ–µ–Ω –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω, MLP):\n",
    "\n",
    "```\n",
    "–í—Ö–æ–¥ x       –°–∫—Ä–∏—Ç —Å–ª–æ–π 1      –°–∫—Ä–∏—Ç —Å–ª–æ–π 2      –ò–∑—Ö–æ–¥\n",
    "[x‚ÇÅ]           [h‚ÇÅ]              [h‚ÇÅ']            [≈∑‚ÇÅ]\n",
    "[x‚ÇÇ]    ‚Üí      [h‚ÇÇ]      ‚Üí       [h‚ÇÇ']     ‚Üí     [≈∑‚ÇÇ]\n",
    "[x‚ÇÉ]           [h‚ÇÉ]              [h‚ÇÉ']\n",
    " ‚ãÆ              ‚ãÆ                 ‚ãÆ\n",
    "```\n",
    "\n",
    "**–ù–æ—Ç–∞—Ü–∏—è:**\n",
    "- $W^{[l]}$ ‚Äî —Ç–µ–≥–ª–∞ –Ω–∞ —Å–ª–æ–π $l$, —Ä–∞–∑–º–µ—Ä $(n_l, n_{l-1})$\n",
    "- $b^{[l]}$ ‚Äî bias –Ω–∞ —Å–ª–æ–π $l$, —Ä–∞–∑–º–µ—Ä $(n_l,)$\n",
    "- $a^{[l]}$ ‚Äî –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –Ω–∞ —Å–ª–æ–π $l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞\n",
    "def draw_neural_network(ax, layer_sizes, layer_names=None):\n",
    "    \"\"\"–†–∏—Å—É–≤–∞ –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞.\"\"\"\n",
    "    n_layers = len(layer_sizes)\n",
    "    max_neurons = max(layer_sizes)\n",
    "    \n",
    "    v_spacing = 1.0\n",
    "    h_spacing = 2.0\n",
    "    \n",
    "    # –ü–æ–∑–∏—Ü–∏–∏ –Ω–∞ –Ω–µ–≤—Ä–æ–Ω–∏—Ç–µ\n",
    "    positions = []\n",
    "    for i, n_neurons in enumerate(layer_sizes):\n",
    "        layer_pos = []\n",
    "        start_y = (max_neurons - n_neurons) / 2 * v_spacing\n",
    "        for j in range(n_neurons):\n",
    "            x = i * h_spacing\n",
    "            y = start_y + j * v_spacing\n",
    "            layer_pos.append((x, y))\n",
    "        positions.append(layer_pos)\n",
    "    \n",
    "    # –í—Ä—ä–∑–∫–∏ –º–µ–∂–¥—É —Å–ª–æ–µ–≤–µ—Ç–µ\n",
    "    for i in range(n_layers - 1):\n",
    "        for pos1 in positions[i]:\n",
    "            for pos2 in positions[i + 1]:\n",
    "                ax.plot([pos1[0], pos2[0]], [pos1[1], pos2[1]], \n",
    "                       'gray', alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    # –ù–µ–≤—Ä–æ–Ω–∏—Ç–µ\n",
    "    colors = ['lightblue', 'lightgreen', 'lightgreen', 'coral']\n",
    "    for i, layer_pos in enumerate(positions):\n",
    "        color = colors[min(i, len(colors)-1)] if i < len(layer_sizes)-1 else colors[-1]\n",
    "        for pos in layer_pos:\n",
    "            circle = plt.Circle(pos, 0.3, color=color, ec='black', linewidth=2, zorder=4)\n",
    "            ax.add_patch(circle)\n",
    "    \n",
    "    # –ï—Ç–∏–∫–µ—Ç–∏\n",
    "    if layer_names:\n",
    "        for i, name in enumerate(layer_names):\n",
    "            x = i * h_spacing\n",
    "            ax.text(x, -1.2, name, ha='center', fontsize=11)\n",
    "    \n",
    "    ax.set_xlim(-1, (n_layers-1) * h_spacing + 1)\n",
    "    ax.set_ylim(-2, max_neurons * v_spacing)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: 4 ‚Üí 5 ‚Üí 3 ‚Üí 2\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "layer_sizes = [4, 5, 3, 2]\n",
    "layer_names = ['–í—Ö–æ–¥\\n(4 –ø—Ä–∏–∑–Ω–∞–∫–∞)', '–°–∫—Ä–∏—Ç —Å–ª–æ–π 1\\n(5 –Ω–µ–≤—Ä–æ–Ω–∞)', \n",
    "               '–°–∫—Ä–∏—Ç —Å–ª–æ–π 2\\n(3 –Ω–µ–≤—Ä–æ–Ω–∞)', '–ò–∑—Ö–æ–¥\\n(2 –∫–ª–∞—Å–∞)']\n",
    "draw_neural_network(ax, layer_sizes, layer_names)\n",
    "ax.set_title('Feedforward –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞: 4 ‚Üí 5 ‚Üí 3 ‚Üí 2', fontsize=14, pad=20)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä –ë—Ä–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:\")\n",
    "print(f\"   W¬π: 4√ó5 = 20, b¬π: 5 ‚Üí 25 –ø–∞—Ä–∞–º–µ—Ç—ä—Ä–∞\")\n",
    "print(f\"   W¬≤: 5√ó3 = 15, b¬≤: 3 ‚Üí 18 –ø–∞—Ä–∞–º–µ—Ç—ä—Ä–∞\")\n",
    "print(f\"   W¬≥: 3√ó2 = 6,  b¬≥: 2 ‚Üí 8 –ø–∞—Ä–∞–º–µ—Ç—ä—Ä–∞\")\n",
    "print(f\"   –û–±—â–æ: 51 –ø–∞—Ä–∞–º–µ—Ç—ä—Ä–∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. –ü—Ä–∞–≤–æ —Ä–∞–∑–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ —Å–∏–≥–Ω–∞–ª–∞ (Forward Pass)\n",
    "\n",
    "### –ö–∞–∫ –∏–∑—á–∏—Å–ª—è–≤–∞–º–µ –∏–∑—Ö–æ–¥–∞?\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∏:**\n",
    "\n",
    "$$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$\n",
    "$$a^{[l]} = f(z^{[l]})$$\n",
    "\n",
    "–ö—ä–¥–µ—Ç–æ $a^{[0]} = x$ (–≤—Ö–æ–¥—ä—Ç).\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä –∑–∞ 2-—Å–ª–æ–π–Ω–∞ –º—Ä–µ–∂–∞:**\n",
    "1. $z^{[1]} = W^{[1]} x + b^{[1]}$, $a^{[1]} = \\text{ReLU}(z^{[1]})$\n",
    "2. $z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$, $\\hat{y} = \\sigma(z^{[2]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-pass-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ forward pass\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def forward_pass(x, weights, biases):\n",
    "    \"\"\"\n",
    "    Forward pass –∑–∞ feedforward –º—Ä–µ–∂–∞.\n",
    "    \n",
    "    Args:\n",
    "        x: –≤—Ö–æ–¥, shape (n_features,)\n",
    "        weights: —Å–ø–∏—Å—ä–∫ –æ—Ç —Ç–µ–≥–ª–∞ [W1, W2, ...]\n",
    "        biases: —Å–ø–∏—Å—ä–∫ –æ—Ç biases [b1, b2, ...]\n",
    "    \n",
    "    Returns:\n",
    "        activations: —Å–ø–∏—Å—ä–∫ –æ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ [a0, a1, ...]\n",
    "        pre_activations: —Å–ø–∏—Å—ä–∫ –æ—Ç z —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ [z1, z2, ...]\n",
    "    \"\"\"\n",
    "    activations = [x]  # a[0] = x\n",
    "    pre_activations = []\n",
    "    \n",
    "    n_layers = len(weights)\n",
    "    \n",
    "    for l in range(n_layers):\n",
    "        z = np.dot(weights[l], activations[-1]) + biases[l]\n",
    "        pre_activations.append(z)\n",
    "        \n",
    "        # –ü–æ—Å–ª–µ–¥–Ω–∏—è—Ç —Å–ª–æ–π: sigmoid, –∏–Ω–∞—á–µ ReLU\n",
    "        if l == n_layers - 1:\n",
    "            a = sigmoid(z)\n",
    "        else:\n",
    "            a = relu(z)\n",
    "        \n",
    "        activations.append(a)\n",
    "    \n",
    "    return activations, pre_activations\n",
    "\n",
    "print(\"‚úì Forward pass –∏–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–Ω–∫—Ä–µ—Ç–µ–Ω –ø—Ä–∏–º–µ—Ä: –º—Ä–µ–∂–∞ 3 ‚Üí 4 ‚Üí 2 ‚Üí 1\n",
    "np.random.seed(42)\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–º–µ —Ç–µ–≥–ª–∞ (–º–∞–ª–∫–∏ —Å–ª—É—á–∞–π–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏)\n",
    "W1 = np.random.randn(4, 3) * 0.5  # 3 –≤—Ö–æ–¥–∞ ‚Üí 4 –Ω–µ–≤—Ä–æ–Ω–∞\n",
    "b1 = np.zeros(4)\n",
    "W2 = np.random.randn(2, 4) * 0.5  # 4 ‚Üí 2\n",
    "b2 = np.zeros(2)\n",
    "W3 = np.random.randn(1, 2) * 0.5  # 2 ‚Üí 1 (–∏–∑—Ö–æ–¥)\n",
    "b3 = np.zeros(1)\n",
    "\n",
    "weights = [W1, W2, W3]\n",
    "biases = [b1, b2, b3]\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä–µ–Ω –≤—Ö–æ–¥\n",
    "x = np.array([0.5, -0.2, 0.8])\n",
    "\n",
    "# Forward pass\n",
    "activations, pre_activations = forward_pass(x, weights, biases)\n",
    "\n",
    "print(\"üìä Forward Pass —Å—Ç—ä–ø–∫–∞ –ø–æ —Å—Ç—ä–ø–∫–∞:\\n\")\n",
    "print(f\"–í—Ö–æ–¥ x = {x}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç: {x.shape}\\n\")\n",
    "\n",
    "for l in range(len(weights)):\n",
    "    print(f\"--- –°–ª–æ–π {l+1} ---\")\n",
    "    print(f\"W{l+1} shape: {weights[l].shape}\")\n",
    "    print(f\"z{l+1} = W{l+1} ¬∑ a{l} + b{l+1}\")\n",
    "    print(f\"z{l+1} = {pre_activations[l].round(3)}\")\n",
    "    act_name = 'sigmoid' if l == len(weights)-1 else 'ReLU'\n",
    "    print(f\"a{l+1} = {act_name}(z{l+1}) = {activations[l+1].round(3)}\")\n",
    "    print()\n",
    "\n",
    "print(f\"üéØ –ò–∑—Ö–æ–¥ ≈∑ = {activations[-1][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏—Ç–µ\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 4))\n",
    "\n",
    "layer_names = ['–í—Ö–æ–¥ x', '–°–ª–æ–π 1 (ReLU)', '–°–ª–æ–π 2 (ReLU)', '–ò–∑—Ö–æ–¥ (Sigmoid)']\n",
    "\n",
    "for i, (ax, a, name) in enumerate(zip(axes, activations, layer_names)):\n",
    "    bars = ax.bar(range(len(a)), a, color='steelblue', edgecolor='black')\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('–ù–µ–≤—Ä–æ–Ω')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('–°—Ç–æ–π–Ω–æ—Å—Ç')\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑–≤–∞–º–µ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏—Ç–µ\n",
    "    for bar, val in zip(bars, a):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "               f'{val:.2f}', ha='center', fontsize=9)\n",
    "    \n",
    "    ax.set_ylim(min(a.min(), 0) - 0.1, max(a.max(), 0) + 0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ó–∞–±–µ–ª–µ–∂–µ—Ç–µ: ReLU –∑–∞–Ω—É–ª—è–≤–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª–Ω–∏—Ç–µ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ (–≤–∏–¥–∏–º–æ –≤ –°–ª–æ–π 1 –∏ 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-processing",
   "metadata": {},
   "source": [
    "### Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞: –ï—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç —á—Ä–µ–∑ –º–∞—Ç—Ä–∏—Ü–∏\n",
    "\n",
    "–í–º–µ—Å—Ç–æ –¥–∞ –æ–±—Ä–∞–±–æ—Ç–≤–∞–º–µ –ø—Ä–∏–º–µ—Ä–∏—Ç–µ –µ–¥–∏–Ω –ø–æ –µ–¥–∏–Ω, –∏–∑–ø–æ–ª–∑–≤–∞–º–µ **–º–∞—Ç—Ä–∏—á–Ω–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏**.\n",
    "\n",
    "**–ï–¥–∏–Ω –ø—Ä–∏–º–µ—Ä:** $z = Wx + b$\n",
    "\n",
    "**Batch –æ—Ç m –ø—Ä–∏–º–µ—Ä–∞:** $Z = WX^T + b$ (broadcasting)\n",
    "\n",
    "–ö—ä–¥–µ—Ç–æ $X$ –µ –º–∞—Ç—Ä–∏—Ü–∞ $(m, n)$ ‚Äî m –ø—Ä–∏–º–µ—Ä–∞ —Å n –ø—Ä–∏–∑–Ω–∞–∫–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch forward pass\n",
    "def forward_pass_batch(X, weights, biases):\n",
    "    \"\"\"\n",
    "    Forward pass –∑–∞ batch –æ—Ç –ø—Ä–∏–º–µ—Ä–∏.\n",
    "    X: shape (m, n_features)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    activations = [X]  # (m, n_features)\n",
    "    pre_activations = []\n",
    "    \n",
    "    n_layers = len(weights)\n",
    "    \n",
    "    for l in range(n_layers):\n",
    "        # Z = X @ W.T + b, shape: (m, n_neurons)\n",
    "        Z = np.dot(activations[-1], weights[l].T) + biases[l]\n",
    "        pre_activations.append(Z)\n",
    "        \n",
    "        if l == n_layers - 1:\n",
    "            A = sigmoid(Z)\n",
    "        else:\n",
    "            A = relu(Z)\n",
    "        \n",
    "        activations.append(A)\n",
    "    \n",
    "    return activations, pre_activations\n",
    "\n",
    "# Batch –æ—Ç 5 –ø—Ä–∏–º–µ—Ä–∞\n",
    "X_batch = np.random.randn(5, 3)  # 5 –ø—Ä–∏–º–µ—Ä–∞, 3 –ø—Ä–∏–∑–Ω–∞–∫–∞\n",
    "\n",
    "activations_batch, _ = forward_pass_batch(X_batch, weights, biases)\n",
    "\n",
    "print(\"üìä Batch Forward Pass:\\n\")\n",
    "print(f\"–í—Ö–æ–¥ X shape: {X_batch.shape}\")\n",
    "print(f\"–ò–∑—Ö–æ–¥ ≈∑ shape: {activations_batch[-1].shape}\")\n",
    "print(f\"\\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞ 5-—Ç–µ –ø—Ä–∏–º–µ—Ä–∞:\")\n",
    "print(activations_batch[-1].flatten().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Loss —Ñ—É–Ω–∫—Ü–∏–∏: –ö–∞–∫–≤–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–∞–º–µ?\n",
    "\n",
    "### –¶–µ–ª –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ\n",
    "\n",
    "**–ò—Å–∫–∞–º–µ:** –ú–æ–¥–µ–ª, –∫–æ–π—Ç–æ –º–∏–Ω–∏–º–∏–∑–∏—Ä–∞ –≥—Ä–µ—à–∫–∞—Ç–∞ –≤—ä—Ä—Ö—É –¥–∞–Ω–Ω–∏—Ç–µ.\n",
    "\n",
    "**–§–æ—Ä–º–∞–ª–Ω–æ:** $\\min_\\theta \\mathcal{L}(\\theta)$\n",
    "\n",
    "–ö—ä–¥–µ—Ç–æ $\\theta = \\{W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, ...\\}$ —Å–∞ –≤—Å–∏—á–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss-functions",
   "metadata": {},
   "source": [
    "### Binary Cross-Entropy (BCE)\n",
    "\n",
    "–ó–∞ **–±–∏–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è** (spam/–Ω–µ-spam, –ø–æ–ª–æ–∂–∏—Ç–µ–ª–Ω–æ/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª–Ω–æ):\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)}) \\right]$$\n",
    "\n",
    "**–ò–Ω—Ç—É–∏—Ü–∏—è:**\n",
    "- –ê–∫–æ $y=1$: –∏—Å–∫–∞–º–µ $\\hat{y} \\to 1$, $\\log(\\hat{y}) \\to 0$\n",
    "- –ê–∫–æ $y=0$: –∏—Å–∫–∞–º–µ $\\hat{y} \\to 0$, $\\log(1-\\hat{y}) \\to 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy Loss\n",
    "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
    "    \"\"\"\n",
    "    BCE loss.\n",
    "    epsilon –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç—è–≤–∞ log(0).\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä–∏\n",
    "print(\"üìä Binary Cross-Entropy Loss:\\n\")\n",
    "\n",
    "examples = [\n",
    "    (1, 0.9, \"–í—è—Ä–Ω–æ positive, –≤–∏—Å–æ–∫–∞ —É–≤–µ—Ä–µ–Ω–æ—Å—Ç\"),\n",
    "    (1, 0.5, \"–í—è—Ä–Ω–æ positive, –Ω–µ—Å–∏–≥—É—Ä–Ω–æ\"),\n",
    "    (1, 0.1, \"–í—è—Ä–Ω–æ positive, –≥—Ä–µ—à–∫–∞!\"),\n",
    "    (0, 0.1, \"–í—è—Ä–Ω–æ negative, –≤–∏—Å–æ–∫–∞ —É–≤–µ—Ä–µ–Ω–æ—Å—Ç\"),\n",
    "    (0, 0.9, \"–í—è—Ä–Ω–æ negative, –≥—Ä–µ—à–∫–∞!\"),\n",
    "]\n",
    "\n",
    "print(f\"{'y_true':>8} {'≈∑':>8} {'Loss':>10} –û–ø–∏—Å–∞–Ω–∏–µ\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for y_true, y_pred, desc in examples:\n",
    "    loss = binary_cross_entropy(np.array([y_true]), np.array([y_pred]))\n",
    "    print(f\"{y_true:>8} {y_pred:>8.1f} {loss:>10.4f} {desc}\")\n",
    "\n",
    "print(\"\\nüí° Loss –µ –Ω–∏—Å—ä–∫ –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª–Ω–∏, —É–≤–µ—Ä–µ–Ω–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ BCE loss\n",
    "y_pred_range = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# Loss –∑–∞ y=1 –∏ y=0\n",
    "loss_y1 = -np.log(y_pred_range)  # –∫–æ–≥–∞—Ç–æ y=1\n",
    "loss_y0 = -np.log(1 - y_pred_range)  # –∫–æ–≥–∞—Ç–æ y=0\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_pred_range, loss_y1, label='y = 1 (positive)', linewidth=2)\n",
    "plt.plot(y_pred_range, loss_y0, label='y = 0 (negative)', linewidth=2)\n",
    "\n",
    "# –ê–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "plt.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.annotate('–ù–µ—Å–∏–≥—É—Ä–Ω–æ—Å—Ç', xy=(0.5, 0.7), fontsize=10)\n",
    "\n",
    "plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ≈∑', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Binary Cross-Entropy: –Ω–∞–∫–∞–∑–≤–∞ –≥—Ä–µ—à–Ω–∏ —É–≤–µ—Ä–µ–Ω–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.ylim(0, 5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ü—Ä–∏ y=1: loss ‚Üí ‚àû –∫–æ–≥–∞—Ç–æ ≈∑ ‚Üí 0 (—É–≤–µ—Ä–µ–Ω–∞ –≥—Ä–µ—à–∫–∞)\")\n",
    "print(\"   –ü—Ä–∏ y=0: loss ‚Üí ‚àû –∫–æ–≥–∞—Ç–æ ≈∑ ‚Üí 1 (—É–≤–µ—Ä–µ–Ω–∞ –≥—Ä–µ—à–∫–∞)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "softmax-ce",
   "metadata": {},
   "source": [
    "### Softmax –∏ Categorical Cross-Entropy\n",
    "\n",
    "–ó–∞ **–º–Ω–æ–≥–æ–∫–ª–∞—Å–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**:\n",
    "\n",
    "**Softmax:** –ü—Ä–µ–≤—Ä—ä—â–∞ logits –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "$$\\hat{y}_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}$$\n",
    "\n",
    "**Cross-Entropy Loss:**\n",
    "$$\\mathcal{L} = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{K} y_j^{(i)} \\log(\\hat{y}_j^{(i)})$$\n",
    "\n",
    "**–í–∞–∂–Ω–æ –∑–∞ LLM:** Next-token prediction –∏–∑–ø–æ–ª–∑–≤–∞ —Ç–æ—á–Ω–æ —Ç–∞–∑–∏ loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "softmax-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax —Ñ—É–Ω–∫—Ü–∏—è\n",
    "def softmax(z):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    z_shifted = z - np.max(z, axis=-1, keepdims=True)\n",
    "    exp_z = np.exp(z_shifted)\n",
    "    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä: –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ —Ç–µ–º–∞ (4 –∫–ª–∞—Å–∞)\n",
    "classes = ['sports', 'politics', 'tech', 'science']\n",
    "logits = np.array([2.0, 1.0, 0.5, -1.0])  # \"—Å—É—Ä–æ–≤\" –∏–∑—Ö–æ–¥ –æ—Ç –º—Ä–µ–∂–∞—Ç–∞\n",
    "\n",
    "probs = softmax(logits)\n",
    "\n",
    "print(\"üìä Softmax —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è:\\n\")\n",
    "print(f\"{'–ö–ª–∞—Å':<12} {'Logit z':>10} {'P(–∫–ª–∞—Å)':>10}\")\n",
    "print(\"-\" * 35)\n",
    "for cls, z, p in zip(classes, logits, probs):\n",
    "    bar = '‚ñà' * int(p * 20)\n",
    "    print(f\"{cls:<12} {z:>10.2f} {p:>10.3f} {bar}\")\n",
    "\n",
    "print(f\"\\n   –°—É–º–∞ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏—Ç–µ: {probs.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "softmax-temp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ softmax (preview –∑–∞ generation –≤ LLM)\n",
    "def softmax_with_temp(z, temperature=1.0):\n",
    "    return softmax(z / temperature)\n",
    "\n",
    "logits = np.array([2.0, 1.0, 0.5, -1.0])\n",
    "temperatures = [0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "width = 0.2\n",
    "\n",
    "for i, temp in enumerate(temperatures):\n",
    "    probs = softmax_with_temp(logits, temp)\n",
    "    ax.bar(x + i*width, probs, width, label=f'T={temp}')\n",
    "\n",
    "ax.set_xticks(x + 1.5*width)\n",
    "ax.set_xticklabels(classes)\n",
    "ax.set_ylabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç')\n",
    "ax.set_title('Softmax —Å —Ä–∞–∑–ª–∏—á–Ω–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–∞ '–æ—Å—Ç—Ä–æ—Ç–∞—Ç–∞' –Ω–∞ —Ä–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ—Ç–æ:\")\n",
    "print(\"   ‚Ä¢ T < 1: –ø–æ-—É–≤–µ—Ä–µ–Ω–∏ (–ø–æ-–æ—Å—Ç—Ä–∏) –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\")\n",
    "print(\"   ‚Ä¢ T > 1: –ø–æ-—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞–∑–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\")\n",
    "print(\"   ‚Üí –ò–∑–ø–æ–ª–∑–≤–∞ —Å–µ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ —Ç–µ–∫—Å—Ç –æ—Ç LLM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. –û–±—Ä–∞—Ç–Ω–æ —Ä–∞–∑–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ —Å–∏–≥–Ω–∞–ª–∞ (Backpropagation)\n",
    "\n",
    "### –ò–¥–µ—è—Ç–∞: –ö–∞–∫ –¥–∞ –æ–±–Ω–æ–≤–∏–º —Ç–µ–≥–ª–∞—Ç–∞?\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º:** –ò–º–∞–º–µ loss $\\mathcal{L}$. –ö–∞–∫ –¥–∞ –Ω–∞–º–µ—Ä–∏–º $\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}$?\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ:** Chain rule (–≤–µ—Ä–∏–∂–Ω–æ –ø—Ä–∞–≤–∏–ª–æ)\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[L]}} \\cdot \\frac{\\partial a^{[L]}}{\\partial z^{[L]}} \\cdot \\frac{\\partial z^{[L]}}{\\partial a^{[L-1]}} \\cdots \\frac{\\partial z^{[1]}}{\\partial W^{[1]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backprop-intuition",
   "metadata": {},
   "source": [
    "### –ò–Ω—Ç—É–∏—Ü–∏—è: –ì—Ä–∞–¥–∏–µ–Ω—Ç—ä—Ç \"—Ç–µ—á–µ\" –Ω–∞–∑–∞–¥\n",
    "\n",
    "```\n",
    "Forward:  x ‚Üí [W¬π] ‚Üí a¬π ‚Üí [W¬≤] ‚Üí a¬≤ ‚Üí [W¬≥] ‚Üí ≈∑ ‚Üí L\n",
    "                                                 ‚Üì\n",
    "Backward: ‚àÇL/‚àÇW¬π ‚Üê Œ¥¬π ‚Üê [W¬≤·µÄ] ‚Üê Œ¥¬≤ ‚Üê [W¬≥·µÄ] ‚Üê Œ¥¬≥ ‚Üê ‚àÇL/‚àÇ≈∑\n",
    "```\n",
    "\n",
    "–ö—ä–¥–µ—Ç–æ $\\delta^{[l]} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}}$ –µ \"–≥—Ä–µ—à–∫–∞—Ç–∞\" –Ω–∞ —Å–ª–æ–π $l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backprop-derivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∏ –Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    \"\"\"–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞ –Ω–∞ sigmoid: œÉ'(z) = œÉ(z)(1-œÉ(z)) = a(1-a)\"\"\"\n",
    "    return a * (1 - a)\n",
    "\n",
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
    "z = np.array([-2, -1, 0, 1, 2])\n",
    "a_sig = sigmoid(z)\n",
    "\n",
    "print(\"üìä –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∏:\\n\")\n",
    "print(f\"{'z':>6} {'ReLU(z)':>10} {'ReLU\\'(z)':>10} {'œÉ(z)':>10} {'œÉ\\'(z)':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(z)):\n",
    "    print(f\"{z[i]:>6} {relu(z)[i]:>10.2f} {relu_derivative(z)[i]:>10.2f} \"\n",
    "          f\"{a_sig[i]:>10.4f} {sigmoid_derivative(a_sig)[i]:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backprop-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—ä–ª–Ω–∞ –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ backpropagation\n",
    "def backward_pass(y_true, activations, pre_activations, weights):\n",
    "    \"\"\"\n",
    "    Backpropagation –∑–∞ feedforward –º—Ä–µ–∂–∞ —Å BCE loss.\n",
    "    \n",
    "    Returns:\n",
    "        dW: –≥—Ä–∞–¥–∏–µ–Ω—Ç–∏ –∑–∞ —Ç–µ–≥–ª–∞—Ç–∞\n",
    "        db: –≥—Ä–∞–¥–∏–µ–Ω—Ç–∏ –∑–∞ bias-–∏—Ç–µ\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]  # batch size\n",
    "    n_layers = len(weights)\n",
    "    \n",
    "    dW = [None] * n_layers\n",
    "    db = [None] * n_layers\n",
    "    \n",
    "    # –ò–∑—Ö–æ–¥–µ–Ω —Å–ª–æ–π: ‚àÇL/‚àÇz = ≈∑ - y (–∑–∞ sigmoid + BCE)\n",
    "    y_pred = activations[-1]\n",
    "    delta = y_pred - y_true.reshape(-1, 1)  # (m, 1)\n",
    "    \n",
    "    # Backprop –ø—Ä–µ–∑ —Å–ª–æ–µ–≤–µ—Ç–µ (–æ—Ç –ø–æ—Å–ª–µ–¥–µ–Ω –∫—ä–º –ø—ä—Ä–≤–∏)\n",
    "    for l in range(n_layers - 1, -1, -1):\n",
    "        a_prev = activations[l]  # (m, n_prev)\n",
    "        \n",
    "        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–∏ –∑–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—Ç–µ\n",
    "        dW[l] = np.dot(delta.T, a_prev) / m  # (n_current, n_prev)\n",
    "        db[l] = np.mean(delta, axis=0)  # (n_current,)\n",
    "        \n",
    "        # –ü—Ä–æ–ø–∞–≥–∏—Ä–∞–º–µ –≥—Ä–µ—à–∫–∞—Ç–∞ –Ω–∞–∑–∞–¥ (–∞–∫–æ –Ω–µ —Å–º–µ –Ω–∞ –ø—ä—Ä–≤–∏—è —Å–ª–æ–π)\n",
    "        if l > 0:\n",
    "            delta = np.dot(delta, weights[l]) * relu_derivative(pre_activations[l-1])\n",
    "    \n",
    "    return dW, db\n",
    "\n",
    "print(\"‚úì Backpropagation –∏–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient checking: –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ backprop –µ –ø—Ä–∞–≤–∏–ª–µ–Ω\n",
    "def numerical_gradient(X, y, weights, biases, layer_idx, param_type, i, j, epsilon=1e-5):\n",
    "    \"\"\"–ò–∑—á–∏—Å–ª—è–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç —á–∏—Å–ª–µ–Ω–æ (–∑–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞).\"\"\"\n",
    "    if param_type == 'W':\n",
    "        params = weights\n",
    "    else:\n",
    "        params = biases\n",
    "    \n",
    "    # –ó–∞–ø–∞–∑–≤–∞–º–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞—Ç–∞ —Å—Ç–æ–π–Ω–æ—Å—Ç\n",
    "    original = params[layer_idx].flat[i * params[layer_idx].shape[-1] + j] if param_type == 'W' \\\n",
    "               else params[layer_idx][i]\n",
    "    \n",
    "    # f(Œ∏ + Œµ)\n",
    "    if param_type == 'W':\n",
    "        params[layer_idx].flat[i * params[layer_idx].shape[-1] + j] = original + epsilon\n",
    "    else:\n",
    "        params[layer_idx][i] = original + epsilon\n",
    "    activations_plus, _ = forward_pass_batch(X, weights, biases)\n",
    "    loss_plus = binary_cross_entropy(y, activations_plus[-1].flatten())\n",
    "    \n",
    "    # f(Œ∏ - Œµ)\n",
    "    if param_type == 'W':\n",
    "        params[layer_idx].flat[i * params[layer_idx].shape[-1] + j] = original - epsilon\n",
    "    else:\n",
    "        params[layer_idx][i] = original - epsilon\n",
    "    activations_minus, _ = forward_pass_batch(X, weights, biases)\n",
    "    loss_minus = binary_cross_entropy(y, activations_minus[-1].flatten())\n",
    "    \n",
    "    # –í—ä–∑—Å—Ç–∞–Ω–æ–≤—è–≤–∞–º–µ\n",
    "    if param_type == 'W':\n",
    "        params[layer_idx].flat[i * params[layer_idx].shape[-1] + j] = original\n",
    "    else:\n",
    "        params[layer_idx][i] = original\n",
    "    \n",
    "    return (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "# –¢–µ—Å—Ç\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(10, 3)\n",
    "y_test = np.random.randint(0, 2, 10)\n",
    "\n",
    "# Forward –∏ backward\n",
    "activations, pre_activations = forward_pass_batch(X_test, weights, biases)\n",
    "dW, db = backward_pass(y_test, activations, pre_activations, weights)\n",
    "\n",
    "print(\"‚úì Gradient Checking:\\n\")\n",
    "print(f\"{'–ü–∞—Ä–∞–º–µ—Ç—ä—Ä':>15} {'Backprop':>12} {'–ß–∏—Å–ª–µ–Ωo':>12} {'–†–∞–∑–ª–∏–∫–∞':>12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –Ω—è–∫–æ–ª–∫–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\n",
    "for l in range(len(weights)):\n",
    "    bp_grad = dW[l][0, 0]\n",
    "    num_grad = numerical_gradient(X_test, y_test, weights, biases, l, 'W', 0, 0)\n",
    "    diff = abs(bp_grad - num_grad)\n",
    "    status = '‚úì' if diff < 1e-5 else '‚úó'\n",
    "    print(f\"W[{l}][0,0] {status:>8} {bp_grad:>12.6f} {num_grad:>12.6f} {diff:>12.2e}\")\n",
    "\n",
    "print(\"\\nüí° –ú–∞–ª–∫–∏—Ç–µ —Ä–∞–∑–ª–∏–∫–∏ (< 1e-5) –ø–æ—Ç–≤—ä—Ä–∂–¥–∞–≤–∞—Ç, —á–µ backprop –µ –ø—Ä–∞–≤–∏–ª–µ–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∏\n",
    "\n",
    "### Gradient Descent: –û—Å–Ω–æ–≤–Ω–∞—Ç–∞ –∏–¥–µ—è\n",
    "\n",
    "**–ü—Ä–∞–≤–∏–ª–æ –∑–∞ –æ–±–Ω–æ–≤—è–≤–∞–Ω–µ:**\n",
    "$$\\theta := \\theta - \\alpha \\nabla_\\theta \\mathcal{L}$$\n",
    "\n",
    "–ö—ä–¥–µ—Ç–æ $\\alpha$ –µ learning rate.\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∏:**\n",
    "- Learning rate: —Ç–≤—ä—Ä–¥–µ –º–∞–ª—ä–∫ ‚Üí –±–∞–≤–Ω–æ, —Ç–≤—ä—Ä–¥–µ –≥–æ–ª—è–º ‚Üí –¥–∏–≤–µ—Ä–≥–∏—Ä–∞\n",
    "- Noisy –≥—Ä–∞–¥–∏–µ–Ω—Ç–∏ –ø—Ä–∏ –º–∞–ª–∫–∏ batch-–æ–≤–µ\n",
    "- –ú–æ–∂–µ –¥–∞ –∑–∞—Å–µ–¥–Ω–µ –≤ –ª–æ–∫–∞–ª–Ω–∏ –º–∏–Ω–∏–º—É–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sgd-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "def sgd_update(params, grads, learning_rate):\n",
    "    \"\"\"SGD: Œ∏ = Œ∏ - Œ±‚àáL\"\"\"\n",
    "    updated = []\n",
    "    for p, g in zip(params, grads):\n",
    "        updated.append(p - learning_rate * g)\n",
    "    return updated\n",
    "\n",
    "print(\"‚úì SGD –∏–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "momentum-section",
   "metadata": {},
   "source": [
    "### SGD —Å Momentum\n",
    "\n",
    "**–ò–¥–µ—è:** –î–æ–±–∞–≤—è–º–µ \"–∏–Ω–µ—Ä—Ü–∏—è\" ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç—ä—Ç —Å–µ –Ω–∞—Ç—Ä—É–ø–≤–∞.\n",
    "\n",
    "$$v := \\beta v + (1-\\beta) \\nabla_\\theta \\mathcal{L}$$\n",
    "$$\\theta := \\theta - \\alpha v$$\n",
    "\n",
    "**–ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞:**\n",
    "- –ò–∑–≥–ª–∞–∂–¥–∞ —à—É–º–Ω–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∏\n",
    "- –£—Å–∫–æ—Ä—è–≤–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏—è—Ç–∞ –≤ \"–ø–ª–æ—Å–∫–∏\" –ø–æ—Å–æ–∫–∏\n",
    "- –ü–æ–º–∞–≥–∞ –¥–∞ —Å–µ –∏–∑–±—è–≥–∞—Ç –ø–ª–∏—Ç–∫–∏ –ª–æ–∫–∞–ª–Ω–∏ –º–∏–Ω–∏–º—É–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "momentum-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD —Å Momentum\n",
    "class MomentumOptimizer:\n",
    "    def __init__(self, learning_rate=0.01, beta=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.beta = beta\n",
    "        self.velocities = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.velocities is None:\n",
    "            self.velocities = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        updated = []\n",
    "        for i, (p, g) in enumerate(zip(params, grads)):\n",
    "            self.velocities[i] = self.beta * self.velocities[i] + (1 - self.beta) * g\n",
    "            updated.append(p - self.lr * self.velocities[i])\n",
    "        \n",
    "        return updated\n",
    "\n",
    "print(\"‚úì Momentum –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adam-section",
   "metadata": {},
   "source": [
    "### Adam: Adaptive Moment Estimation\n",
    "\n",
    "**–ö–æ–º–±–∏–Ω–∏—Ä–∞:** Momentum + –∞–¥–∞–ø—Ç–∏–≤–µ–Ω learning rate –∑–∞ –≤—Å–µ–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—ä—Ä.\n",
    "\n",
    "$$m := \\beta_1 m + (1-\\beta_1) \\nabla_\\theta \\mathcal{L}$$ (–ø—ä—Ä–≤–∏ –º–æ–º–µ–Ω—Ç)\n",
    "$$v := \\beta_2 v + (1-\\beta_2) (\\nabla_\\theta \\mathcal{L})^2$$ (–≤—Ç–æ—Ä–∏ –º–æ–º–µ–Ω—Ç)\n",
    "$$\\hat{m} = \\frac{m}{1-\\beta_1^t}, \\quad \\hat{v} = \\frac{v}{1-\\beta_2^t}$$ (bias –∫–æ—Ä–µ–∫—Ü–∏—è)\n",
    "$$\\theta := \\theta - \\alpha \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon}$$\n",
    "\n",
    "**–¢–∏–ø–∏—á–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏:** $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\alpha = 0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adam-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam Optimizer\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None  # First moment\n",
    "        self.v = None  # Second moment\n",
    "        self.t = 0     # Time step\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        self.t += 1\n",
    "        updated = []\n",
    "        \n",
    "        for i, (p, g) in enumerate(zip(params, grads)):\n",
    "            # Update moments\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (g ** 2)\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            updated.append(p - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon))\n",
    "        \n",
    "        return updated\n",
    "\n",
    "print(\"‚úì Adam –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimizer-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∏ –Ω–∞ 2D —Ñ—É–Ω–∫—Ü–∏—è\n",
    "def rosenbrock(x, y):\n",
    "    \"\"\"Rosenbrock —Ñ—É–Ω–∫—Ü–∏—è - —Ç—Ä—É–¥–Ω–∞ –∑–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è.\"\"\"\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_grad(x, y):\n",
    "    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dy = 200 * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# –°—Ç–∞—Ä—Ç–æ–≤–∞ —Ç–æ—á–∫–∞\n",
    "start = np.array([-1.0, 1.0])\n",
    "\n",
    "# –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∑–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∏\n",
    "def optimize_trajectory(start, optimizer_name, n_steps=200, lr=0.001):\n",
    "    pos = start.copy()\n",
    "    trajectory = [pos.copy()]\n",
    "    \n",
    "    if optimizer_name == 'SGD':\n",
    "        for _ in range(n_steps):\n",
    "            grad = rosenbrock_grad(pos[0], pos[1])\n",
    "            pos = pos - lr * grad\n",
    "            trajectory.append(pos.copy())\n",
    "    \n",
    "    elif optimizer_name == 'Momentum':\n",
    "        v = np.zeros(2)\n",
    "        beta = 0.9\n",
    "        for _ in range(n_steps):\n",
    "            grad = rosenbrock_grad(pos[0], pos[1])\n",
    "            v = beta * v + (1 - beta) * grad\n",
    "            pos = pos - lr * v\n",
    "            trajectory.append(pos.copy())\n",
    "    \n",
    "    elif optimizer_name == 'Adam':\n",
    "        m = np.zeros(2)\n",
    "        v = np.zeros(2)\n",
    "        beta1, beta2, eps = 0.9, 0.999, 1e-8\n",
    "        for t in range(1, n_steps + 1):\n",
    "            grad = rosenbrock_grad(pos[0], pos[1])\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "            m_hat = m / (1 - beta1 ** t)\n",
    "            v_hat = v / (1 - beta2 ** t)\n",
    "            pos = pos - lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            trajectory.append(pos.copy())\n",
    "    \n",
    "    return np.array(trajectory)\n",
    "\n",
    "# –ò–∑—á–∏—Å–ª—è–≤–∞–º–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏\n",
    "traj_sgd = optimize_trajectory(start, 'SGD', lr=0.0001)\n",
    "traj_momentum = optimize_trajectory(start, 'Momentum', lr=0.0001)\n",
    "traj_adam = optimize_trajectory(start, 'Adam', lr=0.01)\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# –ö–æ–Ω—Ç—É—Ä–Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∞ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è—Ç–∞\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "y_range = np.linspace(-1, 3, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = rosenbrock(X, Y)\n",
    "\n",
    "ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.7)\n",
    "\n",
    "# –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏\n",
    "ax.plot(traj_sgd[:, 0], traj_sgd[:, 1], 'r.-', label='SGD', alpha=0.8, markersize=3)\n",
    "ax.plot(traj_momentum[:, 0], traj_momentum[:, 1], 'b.-', label='Momentum', alpha=0.8, markersize=3)\n",
    "ax.plot(traj_adam[:, 0], traj_adam[:, 1], 'g.-', label='Adam', alpha=0.8, markersize=3)\n",
    "\n",
    "# –ú–∞—Ä–∫–µ—Ä–∏\n",
    "ax.plot(start[0], start[1], 'ko', markersize=10, label='Start')\n",
    "ax.plot(1, 1, 'r*', markersize=15, label='Optimum')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∏ (Rosenbrock —Ñ—É–Ω–∫—Ü–∏—è)')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Adam –∫–æ–Ω–≤–µ—Ä–≥–∏—Ä–∞ –Ω–∞–π-–±—ä—Ä–∑–æ –∫—ä–º –æ–ø—Ç–∏–º—É–º–∞ (1, 1).\")\n",
    "print(\"   SGD –µ –Ω–∞–π-–±–∞–≤–µ–Ω, Momentum –µ –º–µ–∂–¥–∏–Ω–µ–Ω.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimizer-summary",
   "metadata": {},
   "source": [
    "### –ö–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞–º?\n",
    "\n",
    "| –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä | –ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞ | –ù–µ–¥–æ—Å—Ç–∞—Ç—ä—Ü–∏ | –ö–æ–≥–∞? |\n",
    "|-------------|------------|-------------|-------|\n",
    "| **SGD** | –ü—Ä–æ—Å—Ç, –¥–æ–±—Ä–∞ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è | –ë–∞–≤–µ–Ω, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫—ä–º LR | –ò–∑—Å–ª–µ–¥–≤–∞–Ω–∏—è, –Ω—è–∫–æ–∏ CV –∑–∞–¥–∞—á–∏ |\n",
    "| **SGD+Momentum** | –ü–æ-–±—ä—Ä–∑ –æ—Ç SGD | –î–æ–ø—ä–ª–Ω–∏—Ç–µ–ª–µ–Ω —Ö–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—ä—Ä | Computer vision |\n",
    "| **Adam** | –ë—ä—Ä–∑, –∞–¥–∞–ø—Ç–∏–≤–µ–Ω, —Ä–∞–±–æ—Ç–∏ \"out of the box\" | –ü–æ-–≥–æ–ª—è–º–∞ –ø–∞–º–µ—Ç | **–ü—Ä–µ–ø–æ—Ä—ä—á–∏—Ç–µ–ª–µ–Ω –∑–∞ NLP/LLM** |\n",
    "| **AdamW** | Adam + –ø—Ä–∞–≤–∏–ª–µ–Ω weight decay | - | **GPT, BERT, LLaMA** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. –ü—Ä–∞–∫—Ç–∏–∫–∞ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ\n",
    "\n",
    "### –ü—ä–ª–µ–Ω training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—ä–ª–µ–Ω training loop\n",
    "def train_network(X_train, y_train, X_val, y_val, layer_sizes, \n",
    "                  epochs=100, batch_size=32, learning_rate=0.001,\n",
    "                  optimizer='adam', verbose=True):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–∞–≤–∞ feedforward –º—Ä–µ–∂–∞.\n",
    "    \n",
    "    Args:\n",
    "        layer_sizes: —Å–ø–∏—Å—ä–∫ [n_input, n_hidden1, ..., n_output]\n",
    "    \n",
    "    Returns:\n",
    "        weights, biases, history\n",
    "    \"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_layers = len(layer_sizes) - 1\n",
    "    \n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ç–µ–≥–ª–∞ (Xavier)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(n_layers):\n",
    "        n_in, n_out = layer_sizes[l], layer_sizes[l+1]\n",
    "        W = np.random.randn(n_out, n_in) * np.sqrt(2.0 / n_in)\n",
    "        b = np.zeros(n_out)\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    \n",
    "    # Optimizer\n",
    "    if optimizer == 'adam':\n",
    "        opt_W = AdamOptimizer(learning_rate)\n",
    "        opt_b = AdamOptimizer(learning_rate)\n",
    "    else:\n",
    "        opt_W = MomentumOptimizer(learning_rate)\n",
    "        opt_b = MomentumOptimizer(learning_rate)\n",
    "    \n",
    "    # –ò—Å—Ç–æ—Ä–∏—è\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # Forward\n",
    "            activations, pre_activations = forward_pass_batch(X_batch, weights, biases)\n",
    "            \n",
    "            # Backward\n",
    "            dW, db = backward_pass(y_batch, activations, pre_activations, weights)\n",
    "            \n",
    "            # Update\n",
    "            weights = opt_W.update(weights, dW)\n",
    "            biases = opt_b.update(biases, db)\n",
    "        \n",
    "        # –û—Ü–µ–Ω–∫–∞ —Å–ª–µ–¥ –≤—Å—è–∫–∞ –µ–ø–æ—Ö–∞\n",
    "        train_act, _ = forward_pass_batch(X_train, weights, biases)\n",
    "        val_act, _ = forward_pass_batch(X_val, weights, biases)\n",
    "        \n",
    "        train_loss = binary_cross_entropy(y_train, train_act[-1].flatten())\n",
    "        val_loss = binary_cross_entropy(y_val, val_act[-1].flatten())\n",
    "        \n",
    "        train_pred = (train_act[-1].flatten() > 0.5).astype(int)\n",
    "        val_pred = (val_act[-1].flatten() > 0.5).astype(int)\n",
    "        train_acc = np.mean(train_pred == y_train)\n",
    "        val_acc = np.mean(val_pred == y_val)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n",
    "                  f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    "    \n",
    "    return weights, biases, history\n",
    "\n",
    "print(\"‚úì Training loop –∏–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—ä–∑–¥–∞–≤–∞–º–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏ –¥–∞–Ω–Ω–∏ –∑–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_synth, y_synth = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train_s, X_val_s, y_train_s, y_val_s = train_test_split(\n",
    "    X_synth, y_synth, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä –°–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏ –¥–∞–Ω–Ω–∏ (two moons):\")\n",
    "print(f\"   Train: {X_train_s.shape[0]} –ø—Ä–∏–º–µ—Ä–∞\")\n",
    "print(f\"   Val: {X_val_s.shape[0]} –ø—Ä–∏–º–µ—Ä–∞\")\n",
    "print(f\"   –ü—Ä–∏–∑–Ω–∞—Ü–∏: {X_train_s.shape[1]}\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_synth[y_synth==0, 0], X_synth[y_synth==0, 1], alpha=0.6, label='–ö–ª–∞—Å 0')\n",
    "plt.scatter(X_synth[y_synth==1, 0], X_synth[y_synth==1, 1], alpha=0.6, label='–ö–ª–∞—Å 1')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Two Moons Dataset (–Ω–µ–ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–∞–≤–∞–º–µ –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞\n",
    "print(\"üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞ (2 ‚Üí 16 ‚Üí 8 ‚Üí 1):\\n\")\n",
    "\n",
    "weights_trained, biases_trained, history = train_network(\n",
    "    X_train_s, y_train_s, X_val_s, y_val_s,\n",
    "    layer_sizes=[2, 16, 8, 1],\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.01,\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì –§–∏–Ω–∞–ª–Ω–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏:\")\n",
    "print(f\"   Train accuracy: {history['train_acc'][-1]:.3f}\")\n",
    "print(f\"   Val accuracy: {history['val_acc'][-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training –∏ Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training –∏ Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
    "print(\"   ‚Ä¢ Train –∏ Val loss –Ω–∞–º–∞–ª—è–≤–∞—Ç –∑–∞–µ–¥–Ω–æ ‚Üí –¥–æ–±—Ä–æ –æ–±—É—á–µ–Ω–∏–µ\")\n",
    "print(\"   ‚Ä¢ –ö—Ä–∏–≤–∏—Ç–µ —Å–∞ –±–ª–∏–∑–∫–∏ ‚Üí –Ω—è–º–∞ —Å–∏–ª–Ω–æ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decision-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ decision boundary\n",
    "def plot_decision_boundary(X, y, weights, biases, title='Decision Boundary'):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    activations, _ = forward_pass_batch(X_grid, weights, biases)\n",
    "    Z = activations[-1].reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.8)\n",
    "    plt.colorbar(label='P(class=1)')\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='red', edgecolors='black', s=50, label='–ö–ª–∞—Å 0')\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='blue', edgecolors='black', s=50, label='–ö–ª–∞—Å 1')\n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_synth, y_synth, weights_trained, biases_trained,\n",
    "                       '–ù–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞: –ù–µ–ª–∏–Ω–µ–π–Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ')\n",
    "\n",
    "print(\"üí° –ù–µ–≤—Ä–æ–Ω–Ω–∞—Ç–∞ –º—Ä–µ–∂–∞ —Å—ä–∑–¥–∞–≤–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞, –∫–æ—è—Ç–æ —Ä–∞–∑–¥–µ–ª—è –¥–≤–µ—Ç–µ '–ª—É–Ω–∏'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overfitting-section",
   "metadata": {},
   "source": [
    "### –ü—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "**–°–∏–º–ø—Ç–æ–º–∏ –Ω–∞ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ:**\n",
    "- Train loss –Ω–∞–º–∞–ª—è–≤–∞, Val loss —Ä–∞—Å—Ç–µ\n",
    "- Train accuracy >> Val accuracy\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏—è:**\n",
    "- **Dropout:** –ò–∑–∫–ª—é—á–≤–∞ —Å–ª—É—á–∞–π–Ω–∏ –Ω–µ–≤—Ä–æ–Ω–∏ –ø–æ –≤—Ä–µ–º–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ\n",
    "- **Weight decay (L2):** –î–æ–±–∞–≤—è $\\lambda ||\\theta||^2$ –∫—ä–º loss\n",
    "- **Early stopping:** –°–ø–∏—Ä–∞, –∫–æ–≥–∞—Ç–æ val loss —Å–ø—Ä–µ –¥–∞ –Ω–∞–º–∞–ª—è–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overfitting-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ —Å –º–∞–ª—ä–∫ dataset\n",
    "# –í–∑–∏–º–∞–º–µ —Å–∞–º–æ 100 –ø—Ä–∏–º–µ—Ä–∞\n",
    "X_small = X_train_s[:100]\n",
    "y_small = y_train_s[:100]\n",
    "\n",
    "print(\"üìä –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ (100 –ø—Ä–∏–º–µ—Ä–∞, –≥–æ–ª—è–º–∞ –º—Ä–µ–∂–∞):\\n\")\n",
    "\n",
    "# –ì–æ–ª—è–º–∞ –º—Ä–µ–∂–∞ –∑–∞ –º–∞–ª—ä–∫ dataset\n",
    "weights_overfit, biases_overfit, history_overfit = train_network(\n",
    "    X_small, y_small, X_val_s, y_val_s,\n",
    "    layer_sizes=[2, 64, 32, 16, 1],  # –ú–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ 100 –ø—Ä–∏–º–µ—Ä–∞\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.01,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(history_overfit['train_loss'], label='Train Loss', linewidth=2)\n",
    "ax.plot(history_overfit['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax.axvline(x=np.argmin(history_overfit['val_loss']), color='red', linestyle='--', \n",
    "           label='–û–ø—Ç–∏–º–∞–ª–Ω–∞ —Ç–æ—á–∫–∞', alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('–ü—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ: Train loss –Ω–∞–º–∞–ª—è–≤–∞, Val loss —Ä–∞—Å—Ç–µ')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "best_epoch = np.argmin(history_overfit['val_loss'])\n",
    "print(f\"üí° –ü—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ –∑–∞–ø–æ—á–≤–∞ –æ–∫–æ–ª–æ epoch {best_epoch}\")\n",
    "print(f\"   Train acc: {history_overfit['train_acc'][-1]:.3f}, Val acc: {history_overfit['val_acc'][-1]:.3f}\")\n",
    "print(f\"   ‚Üí Early stopping –±–∏ —Å–ø—Ä—è–ª–æ –Ω–∞ epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. –ù–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏ –∑–∞ —Ç–µ–∫—Å—Ç–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "\n",
    "### –û—Ç —Ç–µ–∫—Å—Ç –¥–æ –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞\n",
    "\n",
    "```\n",
    "–¢–µ–∫—Å—Ç ‚Üí –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ‚Üí Embeddings ‚Üí Pooling ‚Üí Dense layers ‚Üí –ò–∑—Ö–æ–¥\n",
    "                      (–õ–µ–∫—Ü–∏—è 2)            (–¥–Ω–µ—Å)\n",
    "```\n",
    "\n",
    "**Pooling —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:**\n",
    "- **Mean pooling:** –°—Ä–µ–¥–Ω–æ –Ω–∞ –≤—Å–∏—á–∫–∏ token embeddings\n",
    "- **Max pooling:** –ú–∞–∫—Å–∏–º—É–º –ø–æ –≤—Å—è–∫–æ –∏–∑–º–µ—Ä–µ–Ω–∏–µ\n",
    "- **[CLS] token:** –°–ø–µ—Ü–∏–∞–ª–µ–Ω token –∑–∞ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-text-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞—Ä–µ–∂–¥–∞–º–µ 20 Newsgroups (–±–∏–Ω–∞—Ä–Ω–∞ –∑–∞–¥–∞—á–∞ –∑–∞ –ø—Ä–æ—Å—Ç–æ—Ç–∞)\n",
    "categories = ['sci.space', 'rec.sport.baseball']\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "texts = newsgroups.data\n",
    "labels = newsgroups.target\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª—è–º–µ\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "X_train_text, X_val_text, y_train_text, y_val_text = train_test_split(\n",
    "    X_train_text, y_train_text, test_size=0.2, random_state=42, stratify=y_train_text\n",
    ")\n",
    "\n",
    "print(f\"üìö 20 Newsgroups (binary classification):\")\n",
    "print(f\"   –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {categories}\")\n",
    "print(f\"   Train: {len(X_train_text)}, Val: {len(X_val_text)}, Test: {len(X_test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vectorize-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n",
    "vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train_text).toarray()\n",
    "X_val_vec = vectorizer.transform(X_val_text).toarray()\n",
    "X_test_vec = vectorizer.transform(X_test_text).toarray()\n",
    "\n",
    "print(f\"üìä TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è:\")\n",
    "print(f\"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç: {X_train_vec.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–∞\")\n",
    "print(f\"   Train shape: {X_train_vec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-logreg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train_vec, y_train_text)\n",
    "\n",
    "logreg_train_acc = logreg.score(X_train_vec, y_train_text)\n",
    "logreg_val_acc = logreg.score(X_val_vec, y_val_text)\n",
    "\n",
    "print(f\"üìä Baseline (Logistic Regression):\")\n",
    "print(f\"   Train accuracy: {logreg_train_acc:.3f}\")\n",
    "print(f\"   Val accuracy: {logreg_val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-text-nn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞ –∑–∞ —Ç–µ–∫—Å—Ç–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "print(\"üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞ –∑–∞ —Ç–µ–∫—Å—Ç–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:\\n\")\n",
    "print(f\"   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {X_train_vec.shape[1]} ‚Üí 256 ‚Üí 64 ‚Üí 1\\n\")\n",
    "\n",
    "weights_text, biases_text, history_text = train_network(\n",
    "    X_train_vec, y_train_text, X_val_vec, y_val_text,\n",
    "    layer_sizes=[X_train_vec.shape[1], 256, 64, 1],\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ:\")\n",
    "print(f\"   Logistic Regression val acc: {logreg_val_acc:.3f}\")\n",
    "print(f\"   Neural Network val acc:      {history_text['val_acc'][-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§–∏–Ω–∞–ª–Ω–∞ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—è –Ω–∞–±–æ—Ä\n",
    "test_act, _ = forward_pass_batch(X_test_vec, weights_text, biases_text)\n",
    "y_test_pred = (test_act[-1].flatten() > 0.5).astype(int)\n",
    "\n",
    "nn_test_acc = np.mean(y_test_pred == y_test_text)\n",
    "logreg_test_acc = logreg.score(X_test_vec, y_test_text)\n",
    "\n",
    "print(\"üìä –§–∏–Ω–∞–ª–Ω–∞ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ TEST –Ω–∞–±–æ—Ä:\\n\")\n",
    "print(f\"   Logistic Regression: {logreg_test_acc:.3f}\")\n",
    "print(f\"   Neural Network:      {nn_test_acc:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Neural Network Classification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test_text, y_test_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-nn-limitations",
   "metadata": {},
   "source": [
    "### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ —Ç–æ–∑–∏ –ø–æ–¥—Ö–æ–¥\n",
    "\n",
    "| –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ | –ü—Ä–æ–±–ª–µ–º | –†–µ—à–µ–Ω–∏–µ |\n",
    "|-------------|---------|--------|\n",
    "| **Bag-of-words** | –ì—É–±–∏ —Ä–µ–¥ –Ω–∞ –¥—É–º–∏—Ç–µ | RNN, Transformers (–õ–µ–∫—Ü–∏—è 5) |\n",
    "| **–§–∏–∫—Å–∏—Ä–∞–Ω —Ä–µ—á–Ω–∏–∫** | –ù–æ–≤–∏ –¥—É–º–∏ = OOV | Subword tokenization (–õ–µ–∫—Ü–∏—è 3) |\n",
    "| **–ë–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç** | \"bank\" = ? | –ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–Ω–∏ embeddings (BERT) |\n",
    "| **Pooling** | –°—Ä–µ–¥–Ω–æ—Ç–æ –≥—É–±–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è | Attention –º–µ—Ö–∞–Ω–∏–∑—ä–º |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. –û–±–æ–±—â–µ–Ω–∏–µ –∏ –º–æ—Å—Ç –∫—ä–º –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–∏—Ç–µ\n",
    "\n",
    "### –ö–ª—é—á–æ–≤–∏ –∏–∑–≤–æ–¥–∏ –æ—Ç –¥–Ω–µ—Å\n",
    "\n",
    "**1. –ù–µ–≤—Ä–æ–Ω–Ω–∏—Ç–µ –º—Ä–µ–∂–∏ –Ω–∞–¥–≥—Ä–∞–∂–¥–∞—Ç –ª–∏–Ω–µ–π–Ω–∏—Ç–µ –º–æ–¥–µ–ª–∏**\n",
    "- –ù–µ–ª–∏–Ω–µ–π–Ω–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–æ–∑–≤–æ–ª—è–≤–∞—Ç —Å–ª–æ–∂–Ω–∏ –≥—Ä–∞–Ω–∏—Ü–∏ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ\n",
    "- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –Ω–∞—É—á–∞–≤–∞—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è (feature learning)\n",
    "\n",
    "**2. Forward pass –∏ Backpropagation**\n",
    "- Forward: –∏–∑—á–∏—Å–ª—è–≤–∞–º–µ –∏–∑—Ö–æ–¥–∞ —Å–ª–æ–π –ø–æ —Å–ª–æ–π\n",
    "- Backward: –≥—Ä–∞–¥–∏–µ–Ω—Ç–∏—Ç–µ \"—Ç–µ–∫–∞—Ç\" –Ω–∞–∑–∞–¥ —á—Ä–µ–∑ chain rule\n",
    "\n",
    "**3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**\n",
    "- Adam –µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ä—Ç –∑–∞ NLP/LLM\n",
    "- Learning rate –µ –∫—Ä–∏—Ç–∏—á–µ–Ω —Ö–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—ä—Ä\n",
    "\n",
    "**4. Training curves —Ä–∞–∑–∫—Ä–∏–≤–∞—Ç –ø—Ä–æ–±–ª–µ–º–∏**\n",
    "- –ì–æ–ª—è–º–∞ —Ä–∞–∑–ª–∏–∫–∞ train/val ‚Üí –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ\n",
    "- Regularization: dropout, weight decay, early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-lecture",
   "metadata": {},
   "source": [
    "### –°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–∏\n",
    "\n",
    "**–í—ä–ø—Ä–æ—Å–∏, –Ω–∞ –∫–æ–∏—Ç–æ —â–µ –æ—Ç–≥–æ–≤–æ—Ä–∏–º:**\n",
    "\n",
    "- –ö–∞–∫ –¥–∞ –æ–±—Ä–∞–±–æ—Ç–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏, –∑–∞–ø–∞–∑–≤–∞–π–∫–∏ —Ä–µ–¥–∞?\n",
    "- –ö–∞–∫–≤–æ –µ –º–µ—Ö–∞–Ω–∏–∑–º—ä—Ç –Ω–∞ –≤–Ω–∏–º–∞–Ω–∏–µ—Ç–æ (attention)?\n",
    "- –ö–∞–∫ —Ä–∞–±–æ—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ç–∞ –Ω–∞ Transformer?\n",
    "- –ö–∞–∫–≤–æ –µ BERT –∏ –∫–∞–∫ —Å–µ —Ä–∞–∑–ª–∏—á–∞–≤–∞ –æ—Ç GPT?\n",
    "\n",
    "**–ó–∞—â–æ –µ –≤–∞–∂–Ω–æ:**\n",
    "- Transformers —Å–∞ –æ—Å–Ω–æ–≤–∞—Ç–∞ –Ω–∞ **–≤—Å–∏—á–∫–∏** —Å—ä–≤—Ä–µ–º–µ–Ω–Ω–∏ LLM\n",
    "- GPT, Claude, LLaMA, Gemini ‚Äî –≤—Å–∏—á–∫–∏ —Å–∞ Transformers\n",
    "\n",
    "```\n",
    "Feedforward NN (–¥–Ω–µ—Å) ‚Üí RNN ‚Üí Attention ‚Üí Transformer ‚Üí GPT/BERT\n",
    "                              ‚Üë\n",
    "                         –õ–µ–∫—Ü–∏—è 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "## –†–µ—Å—É—Ä—Å–∏\n",
    "\n",
    "### –ü—Ä–µ–ø–æ—Ä—ä—á–∏—Ç–µ–ª–Ω–æ —á–µ—Ç–µ–Ω–µ\n",
    "\n",
    "**–°—Ç–∞—Ç–∏–∏:**\n",
    "1. Rumelhart, Hinton, Williams (1986) - \"Learning representations by back-propagating errors\" (–æ—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞—Ç–∞ backprop —Å—Ç–∞—Ç–∏—è)\n",
    "2. Kingma & Ba (2014) - \"Adam: A Method for Stochastic Optimization\"\n",
    "3. Srivastava et al. (2014) - \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"\n",
    "\n",
    "**–£—á–µ–±–Ω–∏—Ü–∏:**\n",
    "1. \"Deep Learning\" - Goodfellow, Bengio, Courville, –ì–ª–∞–≤–∞ 6 (Feedforward Networks)\n",
    "2. \"Neural Networks and Deep Learning\" - Michael Nielsen (–±–µ–∑–ø–ª–∞—Ç–µ–Ω –æ–Ω–ª–∞–π–Ω)\n",
    "\n",
    "**Online:**\n",
    "1. Stanford CS231n - Convolutional Neural Networks\n",
    "2. 3Blue1Brown - Neural Networks video series (–æ—Ç–ª–∏—á–Ω–∞ –≤–∏–∑—É–∞–ª–Ω–∞ –∏–Ω—Ç—É–∏—Ü–∏—è)\n",
    "3. Andrej Karpathy - \"A Recipe for Training Neural Networks\" (–±–ª–æ–≥ –ø–æ—Å—Ç)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∑–∞ –≤–∫—ä—â–∏\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 1: –ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è –æ—Ç –Ω—É–ª–∞—Ç–∞**\n",
    "- –ò–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–π—Ç–µ backprop –∑–∞ 3-—Å–ª–æ–π–Ω–∞ –º—Ä–µ–∂–∞ –≤ NumPy\n",
    "- –î–æ–±–∞–≤–µ—Ç–µ L2 regularization\n",
    "- –í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–∞–π—Ç–µ —Å gradient checking\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∏**\n",
    "- –û–±—É—á–µ—Ç–µ –µ–¥–∏–Ω –∏ —Å—ä—â –º–æ–¥–µ–ª —Å SGD, Momentum, Adam\n",
    "- –ù–∞—á–µ—Ä—Ç–∞–π—Ç–µ training curves\n",
    "- –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–∞–π—Ç–µ —Å —Ä–∞–∑–ª–∏—á–Ω–∏ learning rates\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 3: –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–∞ training**\n",
    "- –î–∞–¥–µ–Ω–∏ —Å–∞ training curves —Å —Ä–∞–∑–ª–∏—á–Ω–∏ –ø—Ä–æ–±–ª–µ–º–∏\n",
    "- –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–∞–π—Ç–µ: –ø—Ä–µ–Ω–∞–≥–∞–∂–¥–∞–Ω–µ, —ä–Ω–¥—ä—Ä—Ñ–∏—Ç–≤–∞–Ω–µ, –ª–æ—à learning rate\n",
    "- –ü—Ä–µ–¥–ª–æ–∂–µ—Ç–µ —Ä–µ—à–µ–Ω–∏—è\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 4: –¢–µ–∫—Å—Ç–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞–π—Ç–µ IMDB reviews dataset\n",
    "- –°—Ä–∞–≤–Ω–µ—Ç–µ TF-IDF + NN —Å Word2Vec embeddings + NN\n",
    "- –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–π—Ç–µ –≥—Ä–µ—à–∫–∏—Ç–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "end",
   "metadata": {},
   "source": [
    "---\n",
    "## –ö—Ä–∞–π –Ω–∞ –õ–µ–∫—Ü–∏—è 4\n",
    "\n",
    "### –ë–ª–∞–≥–æ–¥–∞—Ä—è –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ—Ç–æ!\n",
    "\n",
    "**–í—ä–ø—Ä–æ—Å–∏?**\n",
    "\n",
    "---\n",
    "\n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–∏ (Attention, Self-Attention, BERT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
