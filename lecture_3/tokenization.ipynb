{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# –õ–µ–∫—Ü–∏—è 3: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "## –û—Ç —Ç–µ–∫—Å—Ç –∫—ä–º —Ç–æ–∫–µ–Ω–∏: BPE, Unigram –∏ —Å—ä–≤—Ä–µ–º–µ–Ω–Ω–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∏\n",
    "\n",
    "**–ü—Ä–æ–¥—ä–ª–∂–∏—Ç–µ–ª–Ω–æ—Å—Ç:** 2-2.5 —á–∞—Å–∞  \n",
    "**–ü—Ä–µ–¥–ø–æ—Å—Ç–∞–≤–∫–∏:** –õ–µ–∫—Ü–∏—è 2 (–ï–∑–∏–∫–æ–≤–∏ –º–æ–¥–µ–ª–∏, Word2Vec)  \n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –ù–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives",
   "metadata": {},
   "source": [
    "---\n",
    "## –¶–µ–ª–∏ –Ω–∞ –ª–µ–∫—Ü–∏—è—Ç–∞\n",
    "\n",
    "–°–ª–µ–¥ —Ç–∞–∑–∏ –ª–µ–∫—Ü–∏—è —â–µ –º–æ–∂–µ—Ç–µ:\n",
    "\n",
    "- –û–±—è—Å–Ω—è–≤–∞—Ç–µ –∑–∞—â–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è—Ç–∞ –µ –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª–µ–Ω –ø—Ä–æ–±–ª–µ–º\n",
    "- –°—Ä–∞–≤–Ω—è–≤–∞—Ç–µ word-level, character-level –∏ subword —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "- –ò–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞—Ç–µ BPE –∞–ª–≥–æ—Ä–∏—Ç—ä–º –æ—Ç –Ω—É–ª–∞—Ç–∞\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞—Ç–µ Unigram —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —Ä–∞–∑–±–∏—Ä–∞—Ç–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—Ç–∞ –º—É –æ—Å–Ω–æ–≤–∞\n",
    "- –†–∞–±–æ—Ç–∏—Ç–µ —Å HuggingFace tokenizers –∏ tiktoken –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∑–∞–¥–∞—á–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roadmap",
   "metadata": {},
   "source": [
    "### –ü—ä—Ç–Ω–∞ –∫–∞—Ä—Ç–∞ –∑–∞ –¥–Ω–µ—Å\n",
    "\n",
    "```\n",
    "–ö–∞–∫–≤–æ –µ \"–¥—É–º–∞\"? ‚Üí –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ‚Üí Word/Char ‚Üí BPE ‚Üí Unigram ‚Üí –ü—Ä–∞–∫—Ç–∏–∫–∞ ‚Üí LLM —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∏\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "print(\"‚úì NLTK –¥–∞–Ω–Ω–∏—Ç–µ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì –í—Å–∏—á–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏ —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. –ö–∞–∫–≤–æ –µ \"–¥—É–º–∞\"? –ú–æ—Ç–∏–≤–∞—Ü–∏—è –∑–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "### –ü—Ä–æ–±–ª–µ–º—ä—Ç\n",
    "\n",
    "–í –õ–µ–∫—Ü–∏—è 2 –∏–∑–ø–æ–ª–∑–≤–∞—Ö–º–µ Word2Vec, –∫–æ–π—Ç–æ –ø—Ä–∏—Å–≤–æ—è–≤–∞ –≤–µ–∫—Ç–æ—Ä –Ω–∞ –≤—Å—è–∫–∞ **–¥—É–º–∞**.\n",
    "\n",
    "–ù–æ –∫–∞–∫–≤–æ —Ç–æ—á–Ω–æ –µ \"–¥—É–º–∞\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "word-challenges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–¥–∏–∑–≤–∏–∫–∞—Ç–µ–ª—Å—Ç–≤–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª—è–Ω–µ –Ω–∞ \"–¥—É–º–∞\"\n",
    "examples = [\n",
    "    (\"New York\", \"–ï–¥–∏–Ω –≥—Ä–∞–¥ –∏–ª–∏ –¥–≤–µ –¥—É–º–∏?\"),\n",
    "    (\"don't\", \"–ï–¥–Ω–∞ –¥—É–º–∞ –∏–ª–∏ 'do' + 'not'?\"),\n",
    "    (\"COVID-19\", \"–ö–∞–∫ –¥–∞ —Ä–∞–∑–¥–µ–ª–∏–º?\"),\n",
    "    (\"state-of-the-art\", \"–ï–¥–Ω–æ –ø–æ–Ω—è—Ç–∏–µ, –º–Ω–æ–≥–æ —Ç–∏—Ä–µ—Ç–∞\"),\n",
    "    (\"Bundesausbildungsf√∂rderungsgesetz\", \"–ù–µ–º—Å–∫–∏ —Å—ä—Å—Ç–∞–≤–Ω–∏ –¥—É–º–∏\"),\n",
    "    (\"‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ\", \"–ö–∏—Ç–∞–π—Å–∫–∏: –Ω—è–º–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∏\"),\n",
    "]\n",
    "\n",
    "print(\"‚ùì –ö–∞–∫–≤–æ –µ '–¥—É–º–∞'?\\n\")\n",
    "for text, question in examples:\n",
    "    print(f\"   {text:40s} ‚Üí {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-matters",
   "metadata": {},
   "source": [
    "### –ó–∞—â–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è—Ç–∞ –µ –≤–∞–∂–Ω–∞?\n",
    "\n",
    "| –ü—Ä–æ–±–ª–µ–º | –ü–æ—Å–ª–µ–¥–∏—Ü–∞ |\n",
    "|---------|----------|\n",
    "| **OOV –¥—É–º–∏** | Word2Vec –Ω–µ –º–æ–∂–µ –¥–∞ –æ–±—Ä–∞–±–æ—Ç–∏ –Ω–æ–≤–∏ –¥—É–º–∏ |\n",
    "| **–û–≥—Ä–æ–º–µ–Ω —Ä–µ—á–Ω–∏–∫** | –ú–∏–ª–∏–æ–Ω–∏ —É–Ω–∏–∫–∞–ª–Ω–∏ –¥—É–º–∏ = –º–∏–ª–∏–æ–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏ |\n",
    "| **–õ–∏–ø—Å–∞ –Ω–∞ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è** | \"walk\", \"walks\", \"walked\" —Å–∞ —Ç—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∞ |\n",
    "| **–ú–Ω–æ–≥–æ–µ–∑–∏—á–Ω–æ—Å—Ç** | –†–∞–∑–ª–∏—á–Ω–∏ –µ–∑–∏—Ü–∏ = —Ä–∞–∑–ª–∏—á–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oov-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: OOV –ø—Ä–æ–±–ª–µ–º—ä—Ç\n",
    "vocabulary = {'the', 'cat', 'sat', 'on', 'mat', 'dog', 'runs'}\n",
    "test_sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog runs\",\n",
    "    \"the kitten sat quietly\",  # 'kitten' –∏ 'quietly' —Å–∞ OOV\n",
    "    \"COVID-19 affects everyone\",\n",
    "]\n",
    "\n",
    "print(\"üìö –†–µ—á–Ω–∏–∫:\", vocabulary)\n",
    "print(\"\\n‚ùì OOV (Out-of-Vocabulary) –ø—Ä–æ–±–ª–µ–º:\\n\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    words = sent.lower().split()\n",
    "    oov = [w for w in words if w not in vocabulary]\n",
    "    status = \"‚úì OK\" if not oov else f\"‚úó OOV: {oov}\"\n",
    "    print(f\"   '{sent}'\")\n",
    "    print(f\"   {status}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization-definition",
   "metadata": {},
   "source": [
    "### –î–µ—Ñ–∏–Ω–∏—Ü–∏—è –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "**–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è** –µ –ø—Ä–æ—Ü–µ—Å—ä—Ç –Ω–∞ —Ä–∞–∑–±–∏–≤–∞–Ω–µ –Ω–∞ —Ç–µ–∫—Å—Ç –Ω–∞ –ø–æ-–º–∞–ª–∫–∏ –µ–¥–∏–Ω–∏—Ü–∏ (—Ç–æ–∫–µ–Ω–∏).\n",
    "\n",
    "**–¢—Ä–∏ –æ—Å–Ω–æ–≤–Ω–∏ –ø–æ–¥—Ö–æ–¥–∞:**\n",
    "\n",
    "| –ü–æ–¥—Ö–æ–¥ | –¢–æ–∫–µ–Ω–∏ | –ü—Ä–∏–º–µ—Ä –∑–∞ \"playing\" |\n",
    "|--------|--------|--------------------|\n",
    "| **Word-level** | –¶–µ–ª–∏ –¥—É–º–∏ | `[\"playing\"]` |\n",
    "| **Character-level** | –°–∏–º–≤–æ–ª–∏ | `[\"p\",\"l\",\"a\",\"y\",\"i\",\"n\",\"g\"]` |\n",
    "| **Subword** | –ß–∞—Å—Ç–∏ –æ—Ç –¥—É–º–∏ | `[\"play\", \"ing\"]` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ —Ç–µ–∫—Å—Ç\n",
    "\n",
    "–ü—Ä–µ–¥–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –æ–±–∏–∫–Ω–æ–≤–µ–Ω–æ –ø—Ä–∏–ª–∞–≥–∞–º–µ **–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è** ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "case-normalization",
   "metadata": {},
   "source": [
    "### 2.1 Case –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "**–í—ä–ø—Ä–æ—Å:** \"The\" –∏ \"the\" –µ–¥–Ω–∞ –¥—É–º–∞ –ª–∏ —Å–∞?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "case-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case sensitivity –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
    "text = \"Apple released the new iPhone. I ate an apple.\"\n",
    "\n",
    "words_original = text.split()\n",
    "words_lower = text.lower().split()\n",
    "\n",
    "print(\"üìù –û—Ä–∏–≥–∏–Ω–∞–ª–µ–Ω —Ç–µ–∫—Å—Ç:\")\n",
    "print(f\"   {text}\\n\")\n",
    "\n",
    "print(\"üìä –£–Ω–∏–∫–∞–ª–Ω–∏ –¥—É–º–∏:\")\n",
    "print(f\"   Case-sensitive:   {len(set(words_original))} –¥—É–º–∏ ‚Üí {set(words_original)}\")\n",
    "print(f\"   Case-insensitive: {len(set(words_lower))} –¥—É–º–∏ ‚Üí {set(words_lower)}\")\n",
    "\n",
    "print(\"\\nüí° Lowercase –Ω–∞–º–∞–ª—è–≤–∞ —Ä–µ—á–Ω–∏–∫–∞, –Ω–æ –≥—É–±–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\")\n",
    "print(\"   'Apple' (–∫–æ–º–ø–∞–Ω–∏—è) ‚â† 'apple' (–ø–ª–æ–¥)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unicode-normalization",
   "metadata": {},
   "source": [
    "### 2.2 Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º:** –ï–¥–∏–Ω –∏ —Å—ä—â —Å–∏–º–≤–æ–ª –º–æ–∂–µ –¥–∞ –∏–º–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ Unicode –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unicode-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "# \"√©\" –º–æ–∂–µ –¥–∞ –±—ä–¥–µ: –µ–¥–∏–Ω —Å–∏–º–≤–æ–ª (U+00E9) –∏–ª–∏ 'e' + combining accent (U+0065 U+0301)\n",
    "\n",
    "e_composed = '√©'           # \\u00e9\n",
    "e_decomposed = '√©'        # e + \\u0301 (combining acute accent)\n",
    "\n",
    "# –°—ä–∑–¥–∞–≤–∞–º–µ decomposed –≤–µ—Ä—Å–∏—è\n",
    "e_decomposed = 'e\\u0301'\n",
    "\n",
    "print(\"üî§ Unicode –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è –Ω–∞ '√©':\\n\")\n",
    "print(f\"   Composed (NFC):   '{e_composed}' ‚Üí bytes: {e_composed.encode('utf-8')}\")\n",
    "print(f\"   Decomposed (NFD): '{e_decomposed}' ‚Üí bytes: {e_decomposed.encode('utf-8')}\")\n",
    "print(f\"\\n   –ï–¥–Ω–∞–∫–≤–∏ –≤–∏–∑—É–∞–ª–Ω–æ? {e_composed} == {e_decomposed}\")\n",
    "print(f\"   –ï–¥–Ω–∞–∫–≤–∏ –≤ Python? {e_composed == e_decomposed}\")\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "nfc = unicodedata.normalize('NFC', e_decomposed)\n",
    "print(f\"\\n   –°–ª–µ–¥ NFC –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: {nfc == e_composed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stemming-lemmatization",
   "metadata": {},
   "source": [
    "### 2.3 –°—Ç–µ–º–∏—Ä–∞–Ω–µ vs –õ–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "| –ú–µ—Ç–æ–¥ | –ü–æ–¥—Ö–æ–¥ | \"running\" | \"better\" |\n",
    "|-------|--------|-----------|----------|\n",
    "| **Stemming** | –ü—Ä–∞–≤–∏–ª–∞ (–æ—Ç—Ä—è–∑–≤–∞–Ω–µ) | \"run\" | \"better\" |\n",
    "| **Lemmatization** | –†–µ—á–Ω–∏–∫ | \"run\" | \"good\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stemming-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming vs Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['running', 'runs', 'ran', 'better', 'studies', 'studying']\n",
    "\n",
    "print(\"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: Stemming vs Lemmatization\\n\")\n",
    "print(f\"{'–î—É–º–∞':<12} {'Stem':<12} {'Lemma (verb)':<15}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for word in words:\n",
    "    stem = stemmer.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word, pos='v')  # verb\n",
    "    print(f\"{word:<12} {stem:<12} {lemma:<15}\")\n",
    "\n",
    "print(\"\\nüí° Stemming –µ –±—ä—Ä–∑, –Ω–æ –≥—Ä—É–±. Lemmatization –µ —Ç–æ—á–Ω–∞, –Ω–æ –±–∞–≤–Ω–∞.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stemming-problems",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–±–ª–µ–º–∏ —Å—ä—Å stemming\n",
    "problem_words = ['university', 'universe', 'organization', 'organ']\n",
    "\n",
    "print(\"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∏ —Å—ä—Å stemming (over-stemming):\\n\")\n",
    "for word in problem_words:\n",
    "    stem = stemmer.stem(word)\n",
    "    print(f\"   {word:15s} ‚Üí {stem}\")\n",
    "\n",
    "print(\"\\nüí° 'university' –∏ 'universe' –ø–æ–ª—É—á–∞–≤–∞—Ç —Å—ä—â–∏—è stem, –Ω–æ –∏–º–∞—Ç —Ä–∞–∑–ª–∏—á–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. –ü—Ä–æ—Å—Ç–∏ –ø–æ–¥—Ö–æ–¥–∏ –∑–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "### 3.1 Word-level —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "word-tokenization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–ª–∏—á–Ω–∏ –ø–æ–¥—Ö–æ–¥–∏ –∑–∞ word tokenization\n",
    "text = \"Hello, world! I'm learning NLP. It's state-of-the-art.\"\n",
    "\n",
    "# 1. Whitespace split\n",
    "whitespace_tokens = text.split()\n",
    "\n",
    "# 2. NLTK word_tokenize\n",
    "nltk_tokens = word_tokenize(text)\n",
    "\n",
    "# 3. Simple regex (\\w+)\n",
    "regex_tokens = re.findall(r'\\w+', text)\n",
    "\n",
    "print(f\"üìù –¢–µ–∫—Å—Ç: '{text}'\\n\")\n",
    "print(f\"Whitespace split:  {whitespace_tokens}\")\n",
    "print(f\"NLTK tokenize:     {nltk_tokens}\")\n",
    "print(f\"Regex (\\\\w+):       {regex_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "word-problems",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–±–ª–µ–º–∏ —Å word tokenization: vocabulary size\n",
    "# –°–∏–º—É–ª–∏—Ä–∞–º–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–µ–Ω –∫–æ—Ä–ø—É—Å\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Zipf's law: —á–µ—Å—Ç–æ—Ç–∞—Ç–∞ –Ω–∞ –¥—É–º–∏—Ç–µ —Å–ª–µ–¥–≤–∞ —Å—Ç–µ–ø–µ–Ω–µ–Ω –∑–∞–∫–æ–Ω\n",
    "# –¢–æ–ø 100 –¥—É–º–∏ –ø–æ–∫—Ä–∏–≤–∞—Ç ~50% –æ—Ç —Ç–µ–∫—Å—Ç–∞, –Ω–æ –∏–º–∞ –º–∏–ª–∏–æ–Ω–∏ —Ä–µ–¥–∫–∏ –¥—É–º–∏\n",
    "\n",
    "print(\"üìä Vocabulary Size –ø—Ä–æ–±–ª–µ–º:\\n\")\n",
    "print(\"   –¢–∏–ø–∏—á–Ω–∏ —Ä–∞–∑–º–µ—Ä–∏ –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞:\")\n",
    "print(\"   ‚Ä¢ English Wikipedia: ~5M —É–Ω–∏–∫–∞–ª–Ω–∏ –¥—É–º–∏\")\n",
    "print(\"   ‚Ä¢ Google Books: ~100M —É–Ω–∏–∫–∞–ª–Ω–∏ –¥—É–º–∏\")\n",
    "print(\"   ‚Ä¢ Word2Vec (Google News): 3M –¥—É–º–∏, 300D ‚Üí 3.6GB –ø–∞–º–µ—Ç!\")\n",
    "\n",
    "print(\"\\nüí° –ü—Ä–æ–±–ª–µ–º–∏ —Å word-level:\")\n",
    "print(\"   1. –û–≥—Ä–æ–º–µ–Ω —Ä–µ—á–Ω–∏–∫\")\n",
    "print(\"   2. OOV –∑–∞ –Ω–æ–≤–∏/—Ä–µ–¥–∫–∏ –¥—É–º–∏\")\n",
    "print(\"   3. –ù—è–º–∞ —Å–ø–æ–¥–µ–ª—è–Ω–µ –º–µ–∂–¥—É 'walk', 'walks', 'walked'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "char-tokenization-header",
   "metadata": {},
   "source": [
    "### 3.2 Character-level —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "char-tokenization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-level tokenization\n",
    "text = \"Hello NLP!\"\n",
    "\n",
    "char_tokens = list(text)\n",
    "\n",
    "print(f\"üìù –¢–µ–∫—Å—Ç: '{text}'\")\n",
    "print(f\"\\nCharacter tokens: {char_tokens}\")\n",
    "print(f\"–ë—Ä–æ–π —Ç–æ–∫–µ–Ω–∏: {len(char_tokens)}\")\n",
    "\n",
    "# Vocabulary size\n",
    "english_chars = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,!?')\n",
    "print(f\"\\nüìä –†–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞:\")\n",
    "print(f\"   ASCII –±—É–∫–≤–∏ + —Ü–∏—Ñ—Ä–∏ + punctuation: ~100 —Å–∏–º–≤–æ–ª–∞\")\n",
    "print(f\"   Unicode (–≤—Å–∏—á–∫–∏ –µ–∑–∏—Ü–∏): ~150,000 —Å–∏–º–≤–æ–ª–∞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "char-problems",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–±–ª–µ–º–∏ —Å character-level\n",
    "word_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "word_tokens = word_text.split()\n",
    "char_tokens = list(word_text)\n",
    "\n",
    "print(\"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ sequence lengths:\\n\")\n",
    "print(f\"   –¢–µ–∫—Å—Ç: '{word_text}'\")\n",
    "print(f\"   Word tokens: {len(word_tokens)} —Ç–æ–∫–µ–Ω–∞\")\n",
    "print(f\"   Char tokens: {len(char_tokens)} —Ç–æ–∫–µ–Ω–∞\")\n",
    "print(f\"\\n   –°—ä–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {len(char_tokens)/len(word_tokens):.1f}x –ø–æ-–¥—ä–ª–≥–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç\")\n",
    "\n",
    "print(\"\\nüí° –ü—Ä–æ–±–ª–µ–º–∏ —Å character-level:\")\n",
    "print(\"   1. –ú–Ω–æ–≥–æ –¥—ä–ª–≥–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏\")\n",
    "print(\"   2. –°–∏–º–≤–æ–ª–∏—Ç–µ –Ω—è–º–∞—Ç —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ\")\n",
    "print(\"   3. –ü–æ-—Ç—Ä—É–¥–Ω–æ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ (–¥—ä–ª–≥–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tradeoff-visualization",
   "metadata": {},
   "source": [
    "### 3.3 Trade-off –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tradeoff-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: Vocabulary size vs Sequence length\n",
    "approaches = ['Character', 'Subword\\n(BPE)', 'Word']\n",
    "vocab_sizes = [100, 50000, 1000000]  # –ø—Ä–∏–º–µ—Ä–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏\n",
    "seq_lengths = [50, 15, 10]  # –∑–∞ —Å—ä—â–∏—è —Ç–µ–∫—Å—Ç\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "colors = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "\n",
    "# Vocabulary size\n",
    "axes[0].bar(approaches, vocab_sizes, color=colors)\n",
    "axes[0].set_ylabel('–†–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞')\n",
    "axes[0].set_title('Vocabulary Size')\n",
    "axes[0].set_yscale('log')\n",
    "for i, v in enumerate(vocab_sizes):\n",
    "    axes[0].text(i, v*1.5, f'{v:,}', ha='center', fontsize=11)\n",
    "\n",
    "# Sequence length\n",
    "axes[1].bar(approaches, seq_lengths, color=colors)\n",
    "axes[1].set_ylabel('–ë—Ä–æ–π —Ç–æ–∫–µ–Ω–∏ (–∑–∞ ~10 –¥—É–º–∏)')\n",
    "axes[1].set_title('–î—ä–ª–∂–∏–Ω–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç—Ç–∞')\n",
    "for i, v in enumerate(seq_lengths):\n",
    "    axes[1].text(i, v+1, str(v), ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Subword —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –µ '–∑–ª–∞—Ç–Ω–∞—Ç–∞ —Å—Ä–µ–¥–∞':\")\n",
    "print(\"   ‚Ä¢ –£–º–µ—Ä–µ–Ω —Ä–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞ (~30K-100K)\")\n",
    "print(\"   ‚Ä¢ –†–∞–∑—É–º–Ω–∞ –¥—ä–ª–∂–∏–Ω–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏—Ç–µ\")\n",
    "print(\"   ‚Ä¢ –ù—è–º–∞ OOV (–º–æ–∂–µ –¥–∞ —Ä–∞–∑–±–∏–µ –≤—Å—è–∫–∞ –¥—É–º–∞)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Subword —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: BPE –∏ Unigram\n",
    "\n",
    "### –ö–ª—é—á–æ–≤–∞ –∏–¥–µ—è\n",
    "\n",
    "**Subword tokenization** —Ä–∞–∑–±–∏–≤–∞ –¥—É–º–∏—Ç–µ –Ω–∞ —Å–º–∏—Å–ª–µ–Ω–∏ —á–∞—Å—Ç–∏:\n",
    "- –ß–µ—Å—Ç–∏ –¥—É–º–∏ ‚Üí –µ–¥–∏–Ω —Ç–æ–∫–µ–Ω: \"the\", \"and\", \"is\"\n",
    "- –†–µ–¥–∫–∏ –¥—É–º–∏ ‚Üí –Ω—è–∫–æ–ª–∫–æ —Ç–æ–∫–µ–Ω–∞: \"un\" + \"happiness\", \"play\" + \"ing\"\n",
    "\n",
    "**–ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞:**\n",
    "- –§–∏–∫—Å–∏—Ä–∞–Ω —Ä–µ—á–Ω–∏–∫ (30K-100K)\n",
    "- –ù—è–º–∞ OOV ‚Äî –≤—Å–∏—á–∫–æ –º–æ–∂–µ –¥–∞ —Å–µ —Ä–∞–∑–±–∏–µ –¥–æ —Å–∏–º–≤–æ–ª–∏\n",
    "- –°–ø–æ–¥–µ–ª—è–Ω–µ: \"play\", \"playing\", \"replay\" —Å–ø–æ–¥–µ–ª—è—Ç \"play\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpe-header",
   "metadata": {},
   "source": [
    "### 4.1 Byte-Pair Encoding (BPE)\n",
    "\n",
    "**–§–∏–ª–æ—Å–æ—Ñ–∏—è:** Bottom-up, greedy, frequency-based\n",
    "\n",
    "**–ê–ª–≥–æ—Ä–∏—Ç—ä–º –∑–∞ –æ–±—É—á–µ–Ω–∏–µ:**\n",
    "1. –ó–∞–ø–æ—á–Ω–∏ —Å —Ä–µ—á–Ω–∏–∫ –æ—Ç –≤—Å–∏—á–∫–∏ —Å–∏–º–≤–æ–ª–∏\n",
    "2. –ù–∞–º–µ—Ä–∏ –Ω–∞–π-—á–µ—Å—Ç–∞—Ç–∞ –¥–≤–æ–π–∫–∞ —Å—ä—Å–µ–¥–Ω–∏ —Å–∏–º–≤–æ–ª–∏\n",
    "3. –û–±–µ–¥–∏–Ω–∏ –≥–∏ –≤ –Ω–æ–≤ —Å–∏–º–≤–æ–ª\n",
    "4. –ü–æ–≤—Ç–∞—Ä—è–π –¥–æ–∫–∞—Ç–æ —Ä–µ—á–Ω–∏–∫—ä—Ç –¥–æ—Å—Ç–∏–≥–Ω–µ –∂–µ–ª–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä–µ–Ω –∫–æ—Ä–ø—É—Å –∑–∞ BPE –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
    "corpus = [\n",
    "    \"low\", \"low\", \"low\", \"low\", \"low\",\n",
    "    \"lower\", \"lower\",\n",
    "    \"newest\", \"newest\", \"newest\", \"newest\", \"newest\", \"newest\",\n",
    "    \"widest\", \"widest\", \"widest\"\n",
    "]\n",
    "\n",
    "# –ß–µ—Å—Ç–æ—Ç–∞ –Ω–∞ –¥—É–º–∏—Ç–µ\n",
    "word_freq = Counter(corpus)\n",
    "\n",
    "print(\"üìö –ö–æ—Ä–ø—É—Å (—á–µ—Å—Ç–æ—Ç–∞ –Ω–∞ –¥—É–º–∏—Ç–µ):\\n\")\n",
    "for word, count in word_freq.items():\n",
    "    print(f\"   '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE –°—Ç—ä–ø–∫–∞ 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Äî —Ä–∞–∑–±–∏–≤–∞–º–µ –Ω–∞ —Å–∏–º–≤–æ–ª–∏\n",
    "def initialize_vocab(word_freq):\n",
    "    \"\"\"–†–∞–∑–±–∏–≤–∞ –≤—Å—è–∫–∞ –¥—É–º–∞ –Ω–∞ —Å–∏–º–≤–æ–ª–∏ —Å –º–∞—Ä–∫–µ—Ä –∑–∞ –∫—Ä–∞–π –Ω–∞ –¥—É–º–∞.\"\"\"\n",
    "    vocab = {}\n",
    "    for word, freq in word_freq.items():\n",
    "        # –î–æ–±–∞–≤—è–º–µ </w> –∫–∞—Ç–æ –º–∞—Ä–∫–µ—Ä –∑–∞ –∫—Ä–∞–π –Ω–∞ –¥—É–º–∞\n",
    "        chars = list(word) + ['</w>']\n",
    "        vocab[' '.join(chars)] = freq\n",
    "    return vocab\n",
    "\n",
    "vocab = initialize_vocab(word_freq)\n",
    "\n",
    "print(\"üìä –ù–∞—á–∞–ª–µ–Ω —Ä–µ—á–Ω–∏–∫ (—Å–∏–º–≤–æ–ª–∏):\\n\")\n",
    "for word, freq in vocab.items():\n",
    "    print(f\"   [{word}] : {freq}\")\n",
    "\n",
    "# –°–∏–º–≤–æ–ª–∏—Ç–µ –≤ —Ä–µ—á–Ω–∏–∫–∞\n",
    "all_symbols = set()\n",
    "for word in vocab:\n",
    "    all_symbols.update(word.split())\n",
    "\n",
    "print(f\"\\n   –£–Ω–∏–∫–∞–ª–Ω–∏ —Å–∏–º–≤–æ–ª–∏: {sorted(all_symbols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-get-pairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE –°—Ç—ä–ø–∫–∞ 2: –ù–∞–º–∏—Ä–∞–º–µ –Ω–∞–π-—á–µ—Å—Ç–∞—Ç–∞ –¥–≤–æ–π–∫–∞\n",
    "def get_pair_counts(vocab):\n",
    "    \"\"\"–ë—Ä–æ–∏ –≤—Å–∏—á–∫–∏ —Å—ä—Å–µ–¥–Ω–∏ –¥–≤–æ–π–∫–∏ —Å–∏–º–≤–æ–ª–∏.\"\"\"\n",
    "    pairs = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            pairs[pair] += freq\n",
    "    return pairs\n",
    "\n",
    "pairs = get_pair_counts(vocab)\n",
    "\n",
    "print(\"üìä –ß–µ—Å—Ç–æ—Ç–∞ –Ω–∞ –¥–≤–æ–π–∫–∏—Ç–µ:\\n\")\n",
    "for pair, count in pairs.most_common(10):\n",
    "    print(f\"   {pair}: {count}\")\n",
    "\n",
    "best_pair = pairs.most_common(1)[0]\n",
    "print(f\"\\n‚≠ê –ù–∞–π-—á–µ—Å—Ç–∞ –¥–≤–æ–π–∫–∞: {best_pair[0]} (—á–µ—Å—Ç–æ—Ç–∞: {best_pair[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-merge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE –°—Ç—ä–ø–∫–∞ 3: –û–±–µ–¥–∏–Ω—è–≤–∞–º–µ –Ω–∞–π-—á–µ—Å—Ç–∞—Ç–∞ –¥–≤–æ–π–∫–∞\n",
    "def merge_pair(pair, vocab):\n",
    "    \"\"\"–û–±–µ–¥–∏–Ω—è–≤–∞ –¥–≤–æ–π–∫–∞ —Å–∏–º–≤–æ–ª–∏ –Ω–∞–≤—Å—è–∫—ä–¥–µ –≤ —Ä–µ—á–Ω–∏–∫–∞.\"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = freq\n",
    "    \n",
    "    return new_vocab\n",
    "\n",
    "# –ò–∑–≤—ä—Ä—à–≤–∞–º–µ –ø—ä—Ä–≤–æ—Ç–æ —Å–ª–∏–≤–∞–Ω–µ\n",
    "best = pairs.most_common(1)[0][0]\n",
    "vocab = merge_pair(best, vocab)\n",
    "\n",
    "print(f\"üîó Merge #1: {best[0]} + {best[1]} ‚Üí {''.join(best)}\\n\")\n",
    "print(\"   –ù–æ–≤ —Ä–µ—á–Ω–∏–∫:\")\n",
    "for word, freq in vocab.items():\n",
    "    print(f\"   [{word}] : {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-full",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—ä–ª–Ω–∞ BPE –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è\n",
    "def train_bpe(word_freq, num_merges):\n",
    "    \"\"\"–û–±—É—á–∞–≤–∞ BPE tokenizer.\"\"\"\n",
    "    vocab = initialize_vocab(word_freq)\n",
    "    merges = []  # –ó–∞–ø–∏—Å–≤–∞–º–µ —Ä–µ–¥—ä—Ç –Ω–∞ —Å–ª–∏–≤–∞–Ω–∏—è—Ç–∞\n",
    "    \n",
    "    print(f\"üöÄ BPE –æ–±—É—á–µ–Ω–∏–µ ({num_merges} merge –æ–ø–µ—Ä–∞—Ü–∏–∏):\\n\")\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_pair_counts(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        best_pair = pairs.most_common(1)[0][0]\n",
    "        vocab = merge_pair(best_pair, vocab)\n",
    "        merges.append(best_pair)\n",
    "        \n",
    "        merged = ''.join(best_pair)\n",
    "        print(f\"   Merge #{i+1}: {best_pair[0]:8s} + {best_pair[1]:8s} ‚Üí {merged}\")\n",
    "    \n",
    "    return vocab, merges\n",
    "\n",
    "# –û–±—É—á–∞–≤–∞–º–µ —Å 10 —Å–ª–∏–≤–∞–Ω–∏—è\n",
    "vocab = initialize_vocab(word_freq)\n",
    "final_vocab, merge_rules = train_bpe(word_freq, num_merges=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-final-vocab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–µ–∑—É–ª—Ç–∞—Ç–µ–Ω —Ä–µ—á–Ω–∏–∫\n",
    "print(\"üìä –§–∏–Ω–∞–ª–µ–Ω BPE —Ä–µ—á–Ω–∏–∫:\\n\")\n",
    "for word, freq in final_vocab.items():\n",
    "    print(f\"   [{word}] : {freq}\")\n",
    "\n",
    "# –ò–∑–≤–ª–∏—á–∞–º–µ –≤—Å–∏—á–∫–∏ —Ç–æ–∫–µ–Ω–∏\n",
    "all_tokens = set()\n",
    "for word in final_vocab:\n",
    "    all_tokens.update(word.split())\n",
    "\n",
    "print(f\"\\nüìö –¢–æ–∫–µ–Ω–∏ –≤ —Ä–µ—á–Ω–∏–∫–∞ ({len(all_tokens)}):\")\n",
    "print(f\"   {sorted(all_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-encode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE Encoding: –ü—Ä–∏–ª–∞–≥–∞–º–µ merge –ø—Ä–∞–≤–∏–ª–∞—Ç–∞ –≤—ä—Ä—Ö—É –Ω–æ–≤ —Ç–µ–∫—Å—Ç\n",
    "def bpe_encode(word, merges):\n",
    "    \"\"\"–¢–æ–∫–µ–Ω–∏–∑–∏—Ä–∞ –¥—É–º–∞ —Å BPE merge rules.\"\"\"\n",
    "    # –ó–∞–ø–æ—á–≤–∞–º–µ —Å—ä—Å —Å–∏–º–≤–æ–ª–∏\n",
    "    tokens = list(word) + ['</w>']\n",
    "    \n",
    "    # –ü—Ä–∏–ª–∞–≥–∞–º–µ merge –ø—Ä–∞–≤–∏–ª–∞—Ç–∞ –≤ —Ä–µ–¥–∞, –≤ –∫–æ–π—Ç–æ —Å–∞ –Ω–∞—É—á–µ–Ω–∏\n",
    "    for pair in merges:\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            if tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                tokens = tokens[:i] + [''.join(pair)] + tokens[i+2:]\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# –¢–µ—Å—Ç–≤–∞–º–µ encoding\n",
    "test_words = ['low', 'lower', 'lowest', 'newest', 'wider', 'unknown']\n",
    "\n",
    "print(\"üî§ BPE Encoding:\\n\")\n",
    "for word in test_words:\n",
    "    tokens = bpe_encode(word, merge_rules)\n",
    "    print(f\"   {word:12s} ‚Üí {tokens}\")\n",
    "\n",
    "print(\"\\nüí° 'lowest' —Å–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–∞ –≤—ä–ø—Ä–µ–∫–∏ —á–µ –Ω–µ –µ –≤ –∫–æ—Ä–ø—É—Å–∞!\")\n",
    "print(\"   BPE –º–æ–∂–µ –¥–∞ –æ–±—Ä–∞–±–æ—Ç–∏ –≤—Å—è–∫–∞ –¥—É–º–∞ —á—Ä–µ–∑ subword —Ä–∞–∑–±–∏–≤–∫–∞.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpe-visualization",
   "metadata": {},
   "source": [
    "### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ BPE merge –ø—Ä–æ—Ü–µ—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-merge-tree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ merge –¥—ä—Ä–≤–æ –∑–∞ –µ–¥–Ω–∞ –¥—É–º–∞\n",
    "def visualize_bpe_encoding(word, merges):\n",
    "    \"\"\"–ü–æ–∫–∞–∑–≤–∞ —Å—Ç—ä–ø–∫–∏—Ç–µ –Ω–∞ BPE encoding.\"\"\"\n",
    "    tokens = list(word) + ['</w>']\n",
    "    steps = [tokens.copy()]\n",
    "    \n",
    "    for pair in merges:\n",
    "        changed = False\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            if tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                tokens = tokens[:i] + [''.join(pair)] + tokens[i+2:]\n",
    "                changed = True\n",
    "            else:\n",
    "                i += 1\n",
    "        if changed:\n",
    "            steps.append(tokens.copy())\n",
    "    \n",
    "    return steps\n",
    "\n",
    "word = \"newest\"\n",
    "steps = visualize_bpe_encoding(word, merge_rules)\n",
    "\n",
    "print(f\"üå≥ BPE encoding —Å—Ç—ä–ø–∫–∏ –∑–∞ '{word}':\\n\")\n",
    "for i, step in enumerate(steps):\n",
    "    prefix = \"   \" if i == 0 else \"   ‚Üí \"\n",
    "    print(f\"{prefix}{step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-vocab-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: –ö–∞–∫ —Ä–∞—Å—Ç–µ —Ä–µ—á–Ω–∏–∫—ä—Ç —Å merge –æ–ø–µ—Ä–∞—Ü–∏–∏—Ç–µ\n",
    "def track_vocab_growth(word_freq, num_merges):\n",
    "    \"\"\"–°–ª–µ–¥–∏ —Ä–∞—Å—Ç–µ–∂–∞ –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞.\"\"\"\n",
    "    vocab = initialize_vocab(word_freq)\n",
    "    \n",
    "    # –ù–∞—á–∞–ª–µ–Ω –±—Ä–æ–π —Å–∏–º–≤–æ–ª–∏\n",
    "    initial_symbols = set()\n",
    "    for word in vocab:\n",
    "        initial_symbols.update(word.split())\n",
    "    \n",
    "    sizes = [len(initial_symbols)]\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_pair_counts(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = pairs.most_common(1)[0][0]\n",
    "        vocab = merge_pair(best_pair, vocab)\n",
    "        sizes.append(sizes[-1] + 1)  # –í—Å–µ–∫–∏ merge –¥–æ–±–∞–≤—è 1 –Ω–æ–≤ —Ç–æ–∫–µ–Ω\n",
    "    \n",
    "    return sizes\n",
    "\n",
    "sizes = track_vocab_growth(word_freq, 10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(sizes)), sizes, 'b-o', linewidth=2, markersize=8)\n",
    "plt.xlabel('–ë—Ä–æ–π merge –æ–ø–µ—Ä–∞—Ü–∏–∏')\n",
    "plt.ylabel('–†–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞')\n",
    "plt.title('BPE: –†–∞—Å—Ç–µ–∂ –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# –ê–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "plt.annotate('–ù–∞—á–∞–ª–Ω–∏ —Å–∏–º–≤–æ–ª–∏', xy=(0, sizes[0]), \n",
    "             xytext=(1, sizes[0]+2), fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üí° BPE vocabulary = base_chars + num_merges\")\n",
    "print(f\"   GPT-2: 256 bytes + 50,000 merges = 50,256 —Ç–æ–∫–µ–Ω–∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unigram-header",
   "metadata": {},
   "source": [
    "### 4.2 Unigram Language Model Tokenization\n",
    "\n",
    "**–§–∏–ª–æ—Å–æ—Ñ–∏—è:** Top-down, probabilistic, likelihood-based\n",
    "\n",
    "**–†–∞–∑–ª–∏–∫–∞ –æ—Ç BPE:**\n",
    "- BPE: greedy, –¥–æ–±–∞–≤—è —Ç–æ–∫–µ–Ω–∏\n",
    "- Unigram: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–Ω, –ø—Ä–µ–º–∞—Ö–≤–∞ —Ç–æ–∫–µ–Ω–∏\n",
    "\n",
    "**–ê–ª–≥–æ—Ä–∏—Ç—ä–º:**\n",
    "1. –ó–∞–ø–æ—á–Ω–∏ —Å –≥–æ–ª—è–º —Ä–µ—á–Ω–∏–∫ (–≤—Å–∏—á–∫–∏ —Å–∏–º–≤–æ–ª–∏ + —á–µ—Å—Ç–∏ substrings)\n",
    "2. –û–±—É—á–∏ unigram LM: $P(\\text{token})$ –∑–∞ –≤—Å–µ–∫–∏ —Ç–æ–∫–µ–Ω\n",
    "3. –ü—Ä–µ–º–∞—Ö–Ω–∏ —Ç–æ–∫–µ–Ω–∏—Ç–µ, –∫–æ–∏—Ç–æ –Ω–∞–π-–º–∞–ª–∫–æ –≤–ª–∏—è—è—Ç –Ω–∞ likelihood\n",
    "4. –ü–æ–≤—Ç–∞—Ä—è–π –¥–æ–∫–∞—Ç–æ —Ä–µ—á–Ω–∏–∫—ä—Ç —Å—Ç–∞–Ω–µ –¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –º–∞–ª—ä–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unigram-segmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—ä–∑–º–æ–∂–Ω–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
    "# –¢–æ–≤–∞ –µ –∫–ª—é—á–æ–≤–∞ —Ä–∞–∑–ª–∏–∫–∞ –æ—Ç BPE!\n",
    "\n",
    "word = \"unhappiness\"\n",
    "\n",
    "# –í—ä–∑–º–æ–∂–Ω–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (–ø—Ä–∏–º–µ—Ä–Ω–∏)\n",
    "segmentations = [\n",
    "    [\"un\", \"happiness\"],\n",
    "    [\"un\", \"happ\", \"iness\"],\n",
    "    [\"un\", \"happy\", \"ness\"],\n",
    "    [\"unhapp\", \"iness\"],\n",
    "    [\"u\", \"n\", \"h\", \"a\", \"p\", \"p\", \"i\", \"n\", \"e\", \"s\", \"s\"],\n",
    "]\n",
    "\n",
    "print(f\"üî§ –î—É–º–∞: '{word}'\\n\")\n",
    "print(\"–í—ä–∑–º–æ–∂–Ω–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏:\")\n",
    "for seg in segmentations:\n",
    "    print(f\"   {seg}\")\n",
    "\n",
    "print(\"\\nüí° Unigram –∏–∑–±–∏—Ä–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è—Ç–∞ —Å –Ω–∞–π-–≤–∏—Å–æ–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unigram-math",
   "metadata": {},
   "source": [
    "### Unigram –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞\n",
    "\n",
    "**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è:**\n",
    "$$P(x_1, x_2, ..., x_n) = \\prod_{i=1}^{n} P(x_i)$$\n",
    "\n",
    "**–ù–∞–π-–≤–µ—Ä–æ—è—Ç–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è:** (Viterbi –∞–ª–≥–æ—Ä–∏—Ç—ä–º)\n",
    "$$x^* = \\arg\\max_x P(x)$$\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "- P(\"un\") = 0.01, P(\"happiness\") = 0.001\n",
    "- P([\"un\", \"happiness\"]) = 0.01 √ó 0.001 = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unigram-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–∏–º—É–ª–∞—Ü–∏—è –Ω–∞ Unigram probability\n",
    "# (–†–µ–∞–ª–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ –±–∏—Ö–∞ –±–∏–ª–∏ –æ—Ç –æ–±—É—á–µ–Ω –º–æ–¥–µ–ª)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä–Ω–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "token_probs = {\n",
    "    'un': 0.01,\n",
    "    'happiness': 0.001,\n",
    "    'happy': 0.005,\n",
    "    'ness': 0.008,\n",
    "    'happ': 0.0001,\n",
    "    'iness': 0.0002,\n",
    "    'unhapp': 0.00001,\n",
    "}\n",
    "\n",
    "def segmentation_prob(tokens, probs):\n",
    "    \"\"\"–ò–∑—á–∏—Å–ª—è–≤–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è.\"\"\"\n",
    "    prob = 1.0\n",
    "    for t in tokens:\n",
    "        prob *= probs.get(t, 0.0001)  # fallback –∑–∞ –Ω–µ–ø–æ–∑–Ω–∞—Ç–∏\n",
    "    return prob\n",
    "\n",
    "segmentations = [\n",
    "    [\"un\", \"happiness\"],\n",
    "    [\"un\", \"happy\", \"ness\"],\n",
    "    [\"un\", \"happ\", \"iness\"],\n",
    "    [\"unhapp\", \"iness\"],\n",
    "]\n",
    "\n",
    "print(\"üìä Unigram –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏:\\n\")\n",
    "print(f\"{'–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è':<30} {'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç':>15} {'Log-prob':>12}\")\n",
    "print(\"-\" * 58)\n",
    "\n",
    "best_prob = 0\n",
    "best_seg = None\n",
    "\n",
    "for seg in segmentations:\n",
    "    prob = segmentation_prob(seg, token_probs)\n",
    "    log_prob = np.log(prob) if prob > 0 else float('-inf')\n",
    "    print(f\"{str(seg):<30} {prob:>15.2e} {log_prob:>12.2f}\")\n",
    "    \n",
    "    if prob > best_prob:\n",
    "        best_prob = prob\n",
    "        best_seg = seg\n",
    "\n",
    "print(f\"\\n‚≠ê –ù–∞–π-–≤–µ—Ä–æ—è—Ç–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {best_seg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpe-vs-unigram",
   "metadata": {},
   "source": [
    "### 4.3 BPE vs Unigram: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "\n",
    "| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | BPE | Unigram |\n",
    "|----------------|-----|--------|\n",
    "| **–ü–æ–¥—Ö–æ–¥** | Bottom-up (–¥–æ–±–∞–≤—è) | Top-down (–ø—Ä–µ–º–∞—Ö–≤–∞) |\n",
    "| **–ö—Ä–∏—Ç–µ—Ä–∏–π** | –ß–µ—Å—Ç–æ—Ç–∞ | Likelihood |\n",
    "| **–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ–Ω** | –î–∞ | –ù–µ (–º–æ–∂–µ –¥–∞ —Å–µ–º–ø–ª–∏—Ä–∞) |\n",
    "| **–ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏** | –ù–µ | –î–∞ |\n",
    "| **–ò–∑–ø–æ–ª–∑–≤–∞ —Å–µ –æ—Ç** | GPT, LLaMA | T5, ALBERT |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-unigram-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–Ω–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –ø–æ–¥—Ö–æ–¥–∏—Ç–µ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# BPE: Bottom-up\n",
    "ax1 = axes[0]\n",
    "levels = ['a b c d e', 'a b c de', 'a bc de', 'abc de', 'abcde']\n",
    "y_pos = range(len(levels))\n",
    "\n",
    "for i, level in enumerate(levels):\n",
    "    ax1.text(0.5, i, level, ha='center', va='center', fontsize=14,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    if i < len(levels) - 1:\n",
    "        ax1.annotate('', xy=(0.5, i+0.7), xytext=(0.5, i+0.3),\n",
    "                    arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(-0.5, len(levels)-0.5)\n",
    "ax1.set_title('BPE: Bottom-up (merge)', fontsize=14)\n",
    "ax1.axis('off')\n",
    "ax1.text(0.5, -0.3, '‚Üë Merge –Ω–∞–π-—á–µ—Å—Ç–∞—Ç–∞ –¥–≤–æ–π–∫–∞', ha='center', fontsize=11)\n",
    "\n",
    "# Unigram: Top-down\n",
    "ax2 = axes[1]\n",
    "levels = ['abcde + abcd + abc + ab + ...', 'abcde + abc + ab + ...', \n",
    "          'abcde + ab + ...', 'abcde + ...', 'abcde']\n",
    "\n",
    "for i, level in enumerate(levels):\n",
    "    color = 'lightgreen' if i == len(levels)-1 else 'lightyellow'\n",
    "    ax2.text(0.5, i, level, ha='center', va='center', fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor=color, alpha=0.8))\n",
    "    if i < len(levels) - 1:\n",
    "        ax2.annotate('', xy=(0.5, i+0.7), xytext=(0.5, i+0.3),\n",
    "                    arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(-0.5, len(levels)-0.5)\n",
    "ax2.set_title('Unigram: Top-down (prune)', fontsize=14)\n",
    "ax2.axis('off')\n",
    "ax2.text(0.5, -0.3, '‚Üë –ü—Ä–µ–º–∞—Ö–Ω–∏ —Ç–æ–∫–µ–Ω —Å –Ω–∞–π-–º–∞–ª—ä–∫ likelihood impact', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wordpiece-header",
   "metadata": {},
   "source": [
    "### 4.4 WordPiece (–∫—Ä–∞—Ç–∫–æ)\n",
    "\n",
    "**–ò–∑–ø–æ–ª–∑–≤–∞ —Å–µ –æ—Ç:** BERT, DistilBERT, ELECTRA\n",
    "\n",
    "**–ü–æ–¥–æ–±–µ–Ω –Ω–∞ BPE, –Ω–æ:**\n",
    "- –ò–∑–±–∏—Ä–∞ merge –≤—ä–∑ –æ—Å–Ω–æ–≤–∞ –Ω–∞ likelihood gain, –Ω–µ —á–µ—Å—Ç–æ—Ç–∞\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞ `##` –ø—Ä–µ—Ñ–∏–∫—Å –∑–∞ continuation tokens\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:** \"playing\" ‚Üí [\"play\", \"##ing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wordpiece-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordPiece –ø—Ä–∏–º–µ—Ä (—Å–∏–º—É–ª–∞—Ü–∏—è)\n",
    "print(\"üî§ WordPiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (BERT —Å—Ç–∏–ª):\\n\")\n",
    "\n",
    "examples = [\n",
    "    (\"playing\", [\"play\", \"##ing\"]),\n",
    "    (\"unhappiness\", [\"un\", \"##happiness\"]),\n",
    "    (\"tokenization\", [\"token\", \"##ization\"]),\n",
    "    (\"embeddings\", [\"em\", \"##bed\", \"##ding\", \"##s\"]),\n",
    "]\n",
    "\n",
    "for word, tokens in examples:\n",
    "    print(f\"   {word:15s} ‚Üí {tokens}\")\n",
    "\n",
    "print(\"\\nüí° '##' –æ–∑–Ω–∞—á–∞–≤–∞, —á–µ —Ç–æ–∫–µ–Ω—ä—Ç –ø—Ä–æ–¥—ä–ª–∂–∞–≤–∞ –ø—Ä–µ–¥–∏—à–µ–Ω (–Ω–µ –µ –Ω–∞—á–∞–ª–æ –Ω–∞ –¥—É–º–∞)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "### 5.1 –°–ø–µ—Ü–∏–∞–ª–Ω–∏ —Ç–æ–∫–µ–Ω–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-tokens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–ø–µ—Ü–∏–∞–ª–Ω–∏ —Ç–æ–∫–µ–Ω–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω–∏ –º–æ–¥–µ–ª–∏\n",
    "special_tokens = {\n",
    "    'BERT': {\n",
    "        '[CLS]': '–ù–∞—á–∞–ª–æ –Ω–∞ input (–∑–∞ classification)',\n",
    "        '[SEP]': '–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª –º–µ–∂–¥—É –∏–∑—Ä–µ—á–µ–Ω–∏—è',\n",
    "        '[PAD]': 'Padding –¥–æ –µ–¥–Ω–∞–∫–≤–∞ –¥—ä–ª–∂–∏–Ω–∞',\n",
    "        '[MASK]': '–ú–∞—Å–∫–∏—Ä–∞–Ω —Ç–æ–∫–µ–Ω (–∑–∞ MLM)',\n",
    "        '[UNK]': '–ù–µ–ø–æ–∑–Ω–∞—Ç —Ç–æ–∫–µ–Ω',\n",
    "    },\n",
    "    'GPT': {\n",
    "        '<|endoftext|>': '–ö—Ä–∞–π –Ω–∞ —Ç–µ–∫—Å—Ç',\n",
    "        '<|startoftext|>': '–ù–∞—á–∞–ª–æ –Ω–∞ —Ç–µ–∫—Å—Ç (GPT-2)',\n",
    "    },\n",
    "    'T5': {\n",
    "        '<pad>': 'Padding',\n",
    "        '</s>': '–ö—Ä–∞–π –Ω–∞ sequence',\n",
    "        '<unk>': '–ù–µ–ø–æ–∑–Ω–∞—Ç —Ç–æ–∫–µ–Ω',\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üè∑Ô∏è –°–ø–µ—Ü–∏–∞–ª–Ω–∏ —Ç–æ–∫–µ–Ω–∏ –ø–æ –º–æ–¥–µ–ª:\\n\")\n",
    "for model, tokens in special_tokens.items():\n",
    "    print(f\"  {model}:\")\n",
    "    for token, desc in tokens.items():\n",
    "        print(f\"      {token:20s} ‚Üí {desc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "huggingface-header",
   "metadata": {},
   "source": [
    "### 5.2 HuggingFace Tokenizers\n",
    "\n",
    "–ù–∞–π-–ø–æ–ø—É–ª—è—Ä–Ω–∞—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∑–∞ —Ä–∞–±–æ—Ç–∞ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-tokenizers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω–µ (–∞–∫–æ –µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ)\n",
    "# !pip install transformers tokenizers tiktoken\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    print(\"‚úì HuggingFace transformers –µ –Ω–∞–ª–∏—á–µ–Ω!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è –ò–Ω—Å—Ç–∞–ª–∏—Ä–∞–π—Ç–µ —Å: pip install transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-bert-tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞—Ä–µ–∂–¥–∞–º–µ BERT tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(f\"üìö BERT tokenizer:\")\n",
    "print(f\"   –¢–∏–ø: WordPiece\")\n",
    "print(f\"   –†–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞: {bert_tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert-tokenize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "text = \"I love machine learning and natural language processing!\"\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "tokens = bert_tokenizer.tokenize(text)\n",
    "token_ids = bert_tokenizer.encode(text)\n",
    "\n",
    "print(f\"üìù –¢–µ–∫—Å—Ç: '{text}'\\n\")\n",
    "print(f\"–¢–æ–∫–µ–Ω–∏: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"–ë—Ä–æ–π —Ç–æ–∫–µ–Ω–∏: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert-decode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode –æ–±—Ä–∞—Ç–Ω–æ –∫—ä–º —Ç–µ–∫—Å—Ç\n",
    "decoded = bert_tokenizer.decode(token_ids)\n",
    "\n",
    "print(f\"üîÑ Decode:\")\n",
    "print(f\"   Token IDs: {token_ids}\")\n",
    "print(f\"   Decoded:   '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-gpt2-tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 tokenizer (BPE)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(f\"üìö GPT-2 tokenizer:\")\n",
    "print(f\"   –¢–∏–ø: BPE (byte-level)\")\n",
    "print(f\"   –†–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞: {gpt2_tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-tokenizers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: BERT vs GPT-2\n",
    "text = \"Tokenization is fundamental to NLP.\"\n",
    "\n",
    "bert_tokens = bert_tokenizer.tokenize(text)\n",
    "gpt2_tokens = gpt2_tokenizer.tokenize(text)\n",
    "\n",
    "print(f\"üìù –¢–µ–∫—Å—Ç: '{text}'\\n\")\n",
    "print(f\"BERT (WordPiece):  {bert_tokens}\")\n",
    "print(f\"GPT-2 (BPE):       {gpt2_tokens}\")\n",
    "\n",
    "print(f\"\\nüìä –ë—Ä–æ–π —Ç–æ–∫–µ–Ω–∏:\")\n",
    "print(f\"   BERT:  {len(bert_tokens)}\")\n",
    "print(f\"   GPT-2: {len(gpt2_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø–æ–≤–µ—á–µ –ø—Ä–∏–º–µ—Ä–∏\n",
    "examples = [\n",
    "    \"Hello world!\",\n",
    "    \"Machine learning is amazing.\",\n",
    "    \"Unhappiness leads to introspection.\",\n",
    "    \"COVID-19 pandemic affected everyone.\",\n",
    "    \"Supercalifragilisticexpialidocious\",\n",
    "    \"‰ªäÂ§©Â§©Ê∞îÂæàÂ•Ω\",  # Chinese\n",
    "]\n",
    "\n",
    "print(\"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ BERT vs GPT-2:\\n\")\n",
    "print(f\"{'–¢–µ–∫—Å—Ç':<40} {'BERT':<8} {'GPT-2':<8}\")\n",
    "print(\"-\" * 58)\n",
    "\n",
    "for text in examples:\n",
    "    bert_len = len(bert_tokenizer.tokenize(text))\n",
    "    gpt2_len = len(gpt2_tokenizer.tokenize(text))\n",
    "    display_text = text[:37] + \"...\" if len(text) > 40 else text\n",
    "    print(f\"{display_text:<40} {bert_len:<8} {gpt2_len:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tiktoken-header",
   "metadata": {},
   "source": [
    "### 5.3 tiktoken (OpenAI)\n",
    "\n",
    "–ë—ä—Ä–∑ BPE tokenizer, –∏–∑–ø–æ–ª–∑–≤–∞–Ω –æ—Ç GPT-3.5/4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tiktoken-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tiktoken\n",
    "    \n",
    "    # –ó–∞—Ä–µ–∂–¥–∞–º–µ encoding –∑–∞ GPT-4\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    text = \"Hello, how are you doing today?\"\n",
    "    tokens = enc.encode(text)\n",
    "    \n",
    "    print(f\"üìö tiktoken (GPT-4 encoding):\\n\")\n",
    "    print(f\"   –¢–µ–∫—Å—Ç: '{text}'\")\n",
    "    print(f\"   Token IDs: {tokens}\")\n",
    "    print(f\"   –ë—Ä–æ–π —Ç–æ–∫–µ–Ω–∏: {len(tokens)}\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑–≤–∞–º–µ –∫–∞–∫–≤–æ –µ –≤—Å–µ–∫–∏ —Ç–æ–∫–µ–Ω\n",
    "    print(f\"\\n   –¢–æ–∫–µ–Ω–∏:\")\n",
    "    for tid in tokens:\n",
    "        token_bytes = enc.decode_single_token_bytes(tid)\n",
    "        print(f\"      {tid:6d} ‚Üí {token_bytes}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è tiktoken –Ω–µ –µ –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω. –ò–Ω—Å—Ç–∞–ª–∏—Ä–∞–π—Ç–µ —Å: pip install tiktoken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤ —Å—ä–≤—Ä–µ–º–µ–Ω–Ω–∏ LLM\n",
    "\n",
    "### 6.1 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ LLM —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-tokenizers-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–∞–±–ª–∏—Ü–∞ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∏ –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω–∏ –º–æ–¥–µ–ª–∏\n",
    "llm_tokenizers = {\n",
    "    'GPT-2': {'type': 'BPE', 'vocab': 50257, 'note': 'Byte-level'},\n",
    "    'GPT-3/3.5/4': {'type': 'BPE', 'vocab': 100000, 'note': 'cl100k_base'},\n",
    "    'BERT': {'type': 'WordPiece', 'vocab': 30522, 'note': 'Uncased'},\n",
    "    'T5': {'type': 'Unigram', 'vocab': 32000, 'note': 'SentencePiece'},\n",
    "    'LLaMA': {'type': 'BPE', 'vocab': 32000, 'note': 'SentencePiece'},\n",
    "    'LLaMA 2': {'type': 'BPE', 'vocab': 32000, 'note': 'SentencePiece'},\n",
    "    'Mistral': {'type': 'BPE', 'vocab': 32000, 'note': 'SentencePiece'},\n",
    "    'Claude': {'type': 'BPE', 'vocab': '~100k', 'note': 'Byte-level'},\n",
    "}\n",
    "\n",
    "print(\"üìö –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∏ –≤ –ø–æ–ø—É–ª—è—Ä–Ω–∏ LLM:\\n\")\n",
    "print(f\"{'–ú–æ–¥–µ–ª':<15} {'–¢–∏–ø':<12} {'Vocab Size':<12} {'–ë–µ–ª–µ–∂–∫–∞'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for model, info in llm_tokenizers.items():\n",
    "    vocab = str(info['vocab'])\n",
    "    print(f\"{model:<15} {info['type']:<12} {vocab:<12} {info['note']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocab-size-tradeoff",
   "metadata": {},
   "source": [
    "### 6.2 Vocabulary size trade-off\n",
    "\n",
    "| –ü–æ-–º–∞–ª—ä–∫ —Ä–µ—á–Ω–∏–∫ (30K) | –ü–æ-–≥–æ–ª—è–º —Ä–µ—á–Ω–∏–∫ (100K) |\n",
    "|----------------------|------------------------|\n",
    "| –ü–æ-–¥—ä–ª–≥–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏ | –ü–æ-–∫—Ä–∞—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏ |\n",
    "| –ü–æ-–º–∞–ª–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤ embedding | –ü–æ–≤–µ—á–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ |\n",
    "| –ü–æ-–¥–æ–±—Ä–∞ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è | –ü–æ-–¥–æ–±—Ä–æ –ø–æ–∫—Ä–∏—Ç–∏–µ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocab-size-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: Impact –Ω–∞ vocabulary size\n",
    "vocab_sizes = [1000, 10000, 30000, 50000, 100000]\n",
    "embedding_dim = 768  # BERT-base\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤ embedding layer = vocab_size √ó embedding_dim\n",
    "params = [v * embedding_dim / 1e6 for v in vocab_sizes]  # –≤ –º–∏–ª–∏–æ–Ω–∏\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä–Ω–∞ –¥—ä–ª–∂–∏–Ω–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç (–æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–Ω–∞)\n",
    "seq_lengths = [100, 40, 25, 20, 15]  # –ø—Ä–∏–º–µ—Ä–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Embedding parameters\n",
    "axes[0].bar([f'{v//1000}K' for v in vocab_sizes], params, color='steelblue')\n",
    "axes[0].set_xlabel('Vocabulary Size')\n",
    "axes[0].set_ylabel('Embedding –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–º–∏–ª–∏–æ–Ω–∏)')\n",
    "axes[0].set_title('–ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤ Embedding Layer')\n",
    "\n",
    "# Sequence length\n",
    "axes[1].bar([f'{v//1000}K' for v in vocab_sizes], seq_lengths, color='coral')\n",
    "axes[1].set_xlabel('Vocabulary Size')\n",
    "axes[1].set_ylabel('–°—Ä–µ–¥–Ω–∞ –¥—ä–ª–∂–∏–Ω–∞ –Ω–∞ sequence')\n",
    "axes[1].set_title('–î—ä–ª–∂–∏–Ω–∞ –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–∞–Ω —Ç–µ–∫—Å—Ç')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° –ü–æ-–≥–æ–ª—è–º —Ä–µ—á–Ω–∏–∫ = –ø–æ–≤–µ—á–µ –ø–∞–º–µ—Ç –∑–∞ embeddings, –Ω–æ –ø–æ-–∫—Ä–∞—Ç–∫–∏ sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-tokens",
   "metadata": {},
   "source": [
    "### 6.3 Token limits –∏ context windows\n",
    "\n",
    "**–í–∞–∂–Ω–æ:** –ö–æ–≥–∞—Ç–æ API –∫–∞–∑–≤–∞ \"128K tokens\", —Ç–æ–≤–∞ —Å–∞ **—Ç–æ–∫–µ–Ω–∏**, –Ω–µ –¥—É–º–∏!\n",
    "\n",
    "- –°—Ä–µ–¥–Ω–æ 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞ (–∞–Ω–≥–ª–∏–π—Å–∫–∏)\n",
    "- ~75 –¥—É–º–∏ ‚âà 100 —Ç–æ–∫–µ–Ω–∞\n",
    "- –ó–∞ –¥—Ä—É–≥–∏ –µ–∑–∏—Ü–∏ (–∫–∏—Ç–∞–π—Å–∫–∏, –±—ä–ª–≥–∞—Ä—Å–∫–∏) —Å—ä–æ—Ç–Ω–æ—à–µ–Ω–∏–µ—Ç–æ –µ —Ä–∞–∑–ª–∏—á–Ω–æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "token-cost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: –¢–æ–∫–µ–Ω–∏ vs –¥—É–º–∏\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming industries.\",\n",
    "    \"–ó–¥—Ä–∞–≤–µ–π—Ç–µ, –∫–∞–∫ —Å—Ç–µ –¥–Ω–µ—Å?\",  # Bulgarian\n",
    "    \"‰ªäÂ§©Â§©Ê∞îÈùûÂ∏∏Â•Ω„ÄÇ\",  # Chinese\n",
    "    \"def fibonacci(n): return n if n < 2 else fibonacci(n-1) + fibonacci(n-2)\",\n",
    "]\n",
    "\n",
    "print(\"üìä –¢–æ–∫–µ–Ω–∏ vs —Å–∏–º–≤–æ–ª–∏ vs –¥—É–º–∏:\\n\")\n",
    "print(f\"{'–¢–µ–∫—Å—Ç':<50} {'Chars':>6} {'Words':>6} {'Tokens':>7} {'Chars/Tok':>10}\")\n",
    "print(\"-\" * 82)\n",
    "\n",
    "for text in sample_texts:\n",
    "    chars = len(text)\n",
    "    words = len(text.split())\n",
    "    tokens = len(gpt2_tokenizer.tokenize(text))\n",
    "    ratio = chars / tokens if tokens > 0 else 0\n",
    "    \n",
    "    display = text[:47] + \"...\" if len(text) > 50 else text\n",
    "    print(f\"{display:<50} {chars:>6} {words:>6} {tokens:>7} {ratio:>10.1f}\")\n",
    "\n",
    "print(\"\\nüí° –ö–∏—Ç–∞–π—Å–∫–∏ –∏ –±—ä–ª–≥–∞—Ä—Å–∫–∏ –∏–∑–ø–æ–ª–∑–≤–∞—Ç –ø–æ–≤–µ—á–µ —Ç–æ–∫–µ–Ω–∏ –Ω–∞ –¥—É–º–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. –ü—Ä–æ–±–ª–µ–º–∏ –∏ pitfalls\n",
    "\n",
    "### 7.1 Multilingual bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multilingual-bias",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilingual tokenization bias\n",
    "sentences = {\n",
    "    'English': \"Hello, how are you today?\",\n",
    "    'Bulgarian': \"–ó–¥—Ä–∞–≤–µ–π—Ç–µ, –∫–∞–∫ —Å—Ç–µ –¥–Ω–µ—Å?\",\n",
    "    'Chinese': \"‰Ω†Â•ΩÔºå‰ªäÂ§©ÊÄé‰πàÊ†∑Ôºü\",\n",
    "    'Arabic': \"ŸÖÿ±ÿ≠ÿ®ÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉ ÿßŸÑŸäŸàŸÖÿü\",\n",
    "}\n",
    "\n",
    "print(\"üåç Multilingual Tokenization Bias:\\n\")\n",
    "print(f\"{'–ï–∑–∏–∫':<12} {'–°–∏–º–≤–æ–ª–∏':>10} {'GPT-2 —Ç–æ–∫–µ–Ω–∏':>15} {'Ratio':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for lang, text in sentences.items():\n",
    "    chars = len(text)\n",
    "    tokens = len(gpt2_tokenizer.tokenize(text))\n",
    "    ratio = tokens / chars\n",
    "    print(f\"{lang:<12} {chars:>10} {tokens:>15} {ratio:>10.2f}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è –ù–µ-–ª–∞—Ç–∏–Ω—Å–∫–∏ –µ–∑–∏—Ü–∏ —Å–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–∞—Ç –ø–æ-'—Å–∫—ä–ø–æ':\")\n",
    "print(\"   ‚Ä¢ –ü–æ–≤–µ—á–µ —Ç–æ–∫–µ–Ω–∏ –∑–∞ —Å—ä—â–æ—Ç–æ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ\")\n",
    "print(\"   ‚Ä¢ –ü–æ-–±—ä—Ä–∑–æ –∏–∑—á–µ—Ä–ø–≤–∞–Ω–µ –Ω–∞ context window\")\n",
    "print(\"   ‚Ä¢ –ü–æ-–≤–∏—Å–æ–∫–∞ —Ü–µ–Ω–∞ –ø—Ä–∏ API calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "byte-level-bpe",
   "metadata": {},
   "source": [
    "### 7.2 Byte-level BPE\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º:** –ö–∞–∫ –¥–∞ –æ–±—Ä–∞–±–æ—Ç–∏–º –≤—Å–µ–∫–∏ Unicode —Å–∏–º–≤–æ–ª?\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ (GPT-2):** Byte-level BPE\n",
    "- –†–∞–±–æ—Ç–∏ –≤—ä—Ä—Ö—É bytes (256 –≤—ä–∑–º–æ–∂–Ω–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏)\n",
    "- –ú–æ–∂–µ –¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–∞ –≤—Å–∏—á–∫–æ\n",
    "- –ù–∏–∫–æ–≥–∞ –Ω–µ –≤—Ä—ä—â–∞ [UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "byte-level-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte-level BPE: –º–æ–∂–µ –¥–∞ –æ–±—Ä–∞–±–æ—Ç–∏ –≤—Å–∏—á–∫–æ\n",
    "weird_texts = [\n",
    "    \"Hello üåç World! üéâ\",\n",
    "    \"Math: ‚àë‚à´‚àö‚àû\",\n",
    "    \"Ê∑∑Âêà text with Êó•Êú¨Ë™û\",\n",
    "    \"<script>alert('xss')</script>\",\n",
    "]\n",
    "\n",
    "print(\"üî§ Byte-level BPE: –æ–±—Ä–∞–±–æ—Ç–≤–∞ –≤—Å–∏—á–∫–æ!\\n\")\n",
    "for text in weird_texts:\n",
    "    tokens = gpt2_tokenizer.tokenize(text)\n",
    "    print(f\"   '{text}'\")\n",
    "    print(f\"   ‚Üí {tokens}\\n\")\n",
    "\n",
    "print(\"üí° GPT-2 –Ω–∏–∫–æ–≥–∞ –Ω–µ –≤—Ä—ä—â–∞ [UNK] ‚Äî –≤—Å–∏—á–∫–æ —Å–µ —Ä–∞–∑–±–∏–≤–∞ –¥–æ bytes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization-pitfalls",
   "metadata": {},
   "source": [
    "### 7.3 –ß–µ—Å—Ç–∏ –≥—Ä–µ—à–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pitfall-spaces",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 1: Trailing spaces\n",
    "text1 = \"Hello\"\n",
    "text2 = \"Hello \"\n",
    "text3 = \" Hello\"\n",
    "\n",
    "print(\"‚ö†Ô∏è Pitfall: Spaces matter!\\n\")\n",
    "for text in [text1, text2, text3]:\n",
    "    tokens = gpt2_tokenizer.tokenize(text)\n",
    "    print(f\"   '{text}' ({len(text)} chars) ‚Üí {tokens}\")\n",
    "\n",
    "print(\"\\nüí° –í–æ–¥–µ—â/—Å–ª–µ–¥–≤–∞—â space –º–æ–∂–µ –¥–∞ –ø—Ä–æ–º–µ–Ω–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è—Ç–∞!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pitfall-case",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 2: Case sensitivity\n",
    "texts = [\"Hello\", \"hello\", \"HELLO\"]\n",
    "\n",
    "print(\"‚ö†Ô∏è Pitfall: Case sensitivity\\n\")\n",
    "print(\"GPT-2 (case-sensitive):\")\n",
    "for text in texts:\n",
    "    tokens = gpt2_tokenizer.tokenize(text)\n",
    "    print(f\"   '{text}' ‚Üí {tokens}\")\n",
    "\n",
    "print(\"\\nBERT uncased (case-insensitive):\")\n",
    "for text in texts:\n",
    "    tokens = bert_tokenizer.tokenize(text)\n",
    "    print(f\"   '{text}' ‚Üí {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pitfall-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 3: Numbers are tricky\n",
    "numbers = [\"123\", \"1234\", \"12345\", \"123456\", \"1000000\"]\n",
    "\n",
    "print(\"‚ö†Ô∏è Pitfall: –ß–∏—Å–ª–∞—Ç–∞ —Å–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–∞—Ç –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–∏–º–æ\\n\")\n",
    "for num in numbers:\n",
    "    tokens = gpt2_tokenizer.tokenize(num)\n",
    "    print(f\"   '{num}' ‚Üí {tokens} ({len(tokens)} —Ç–æ–∫–µ–Ω–∞)\")\n",
    "\n",
    "print(\"\\nüí° LLM –º–æ–¥–µ–ª–∏—Ç–µ –Ω–µ '–≤–∏–∂–¥–∞—Ç' —á–∏—Å–ª–∞ –∫–∞—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏,\")\n",
    "print(\"   –∞ –∫–∞—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç –æ—Ç —Ç–æ–∫–µ–Ω–∏!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. –û–±–æ–±—â–µ–Ω–∏–µ –∏ –º–æ—Å—Ç –∫—ä–º —Å–ª–µ–¥–≤–∞—â–∞—Ç–∞ –ª–µ–∫—Ü–∏—è\n",
    "\n",
    "### –ö–ª—é—á–æ–≤–∏ –∏–∑–≤–æ–¥–∏\n",
    "\n",
    "**1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è—Ç–∞ –µ –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª–µ–Ω –ø—Ä–æ–±–ª–µ–º**\n",
    "- Word-level: –æ–≥—Ä–æ–º–µ–Ω —Ä–µ—á–Ω–∏–∫, OOV –ø—Ä–æ–±–ª–µ–º\n",
    "- Character-level: –¥—ä–ª–≥–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏\n",
    "- Subword: –±–∞–ª–∞–Ω—Å–∏—Ä–∞–Ω–æ —Ä–µ—à–µ–Ω–∏–µ\n",
    "\n",
    "**2. BPE –∏ Unigram —Å–∞ –æ—Å–Ω–æ–≤–Ω–∏—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∏**\n",
    "- BPE: greedy, bottom-up, –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ–Ω\n",
    "- Unigram: probabilistic, top-down, –º–æ–∂–µ –¥–∞ —Å–µ–º–ø–ª–∏—Ä–∞\n",
    "\n",
    "**3. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Å—ä–æ–±—Ä–∞–∂–µ–Ω–∏—è**\n",
    "- –¢–æ–∫–µ–Ω–∏ ‚â† –¥—É–º–∏ (–≤–∞–∂–Ω–æ –∑–∞ API costs!)\n",
    "- Multilingual bias –µ —Ä–µ–∞–ª–µ–Ω –ø—Ä–æ–±–ª–µ–º\n",
    "- Spaces –∏ case –º–æ–≥–∞—Ç –¥–∞ –æ–±—ä—Ä–∫–∞—Ç –Ω–µ—â–∞—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-lecture",
   "metadata": {},
   "source": [
    "### –°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è: –ù–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏\n",
    "\n",
    "**–í—ä–ø—Ä–æ—Å–∏, –Ω–∞ –∫–æ–∏—Ç–æ —â–µ –æ—Ç–≥–æ–≤–æ—Ä–∏–º:**\n",
    "\n",
    "- –ö–∞–∫ –Ω–µ–≤—Ä–æ–Ω–Ω–∏—Ç–µ –º—Ä–µ–∂–∏ –æ–±—Ä–∞–±–æ—Ç–≤–∞—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ—Å—Ç–∏ –æ—Ç —Ç–æ–∫–µ–Ω–∏?\n",
    "- –ö–∞–∫–≤–æ –µ embedding layer –∏ –∫–∞–∫ —Ä–∞–±–æ—Ç–∏?\n",
    "- –ö–∞–∫ —Å–µ –æ–±—É—á–∞–≤–∞ –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞ (backpropagation)?\n",
    "\n",
    "**–ó–∞—â–æ –µ –≤–∞–∂–Ω–æ:**\n",
    "- –¢–æ–∫–µ–Ω–∏—Ç–µ + embeddings = –≤—Ö–æ–¥—ä—Ç –∫—ä–º –º–æ–¥–µ–ª–∏—Ç–µ\n",
    "- –†–∞–∑–±–∏—Ä–∞–Ω–µ—Ç–æ –Ω–∞ NN –µ –∫–ª—é—á–æ–≤–æ –∑–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–∏—Ç–µ (–õ–µ–∫—Ü–∏—è 5)\n",
    "\n",
    "```\n",
    "–¢–µ–∫—Å—Ç ‚Üí –¢–æ–∫–µ–Ω–∏ ‚Üí Token IDs ‚Üí Embeddings ‚Üí Neural Network ‚Üí Output\n",
    "       ^^^^^^^^\n",
    "       (–¥–Ω–µ—Å)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "## –†–µ—Å—É—Ä—Å–∏\n",
    "\n",
    "### –ü—Ä–µ–ø–æ—Ä—ä—á–∏—Ç–µ–ª–Ω–æ —á–µ—Ç–µ–Ω–µ\n",
    "\n",
    "**–°—Ç–∞—Ç–∏–∏:**\n",
    "1. Sennrich et al. (2016) - \"Neural Machine Translation of Rare Words with Subword Units\" (BPE)\n",
    "2. Kudo (2018) - \"Subword Regularization: Improving Neural Network Translation Models\" (Unigram)\n",
    "3. Radford et al. (2019) - \"Language Models are Unsupervised Multitask Learners\" (GPT-2 byte-level BPE)\n",
    "\n",
    "**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:**\n",
    "1. HuggingFace Tokenizers: https://huggingface.co/docs/tokenizers/\n",
    "2. tiktoken: https://github.com/openai/tiktoken\n",
    "3. SentencePiece: https://github.com/google/sentencepiece\n",
    "\n",
    "**–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏:**\n",
    "1. OpenAI Tokenizer: https://platform.openai.com/tokenizer\n",
    "2. HuggingFace tokenizer playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∑–∞ –≤–∫—ä—â–∏\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 1: –ò–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–∞–π—Ç–µ BPE –æ—Ç –Ω—É–ª–∞—Ç–∞**\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞–π—Ç–µ –ø–æ-–≥–æ–ª—è–º –∫–æ—Ä–ø—É—Å (–Ω–∞–ø—Ä. WikiText)\n",
    "- –û–±—É—á–µ—Ç–µ BPE —Å 1000 merge –æ–ø–µ—Ä–∞—Ü–∏–∏\n",
    "- –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–∞–π—Ç–µ –Ω–æ–≤ —Ç–µ–∫—Å—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–π—Ç–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∞\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 2: –°—Ä–∞–≤–Ω–µ—Ç–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∏**\n",
    "- –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–∞–π—Ç–µ –µ–¥–∏–Ω –∏ —Å—ä—â —Ç–µ–∫—Å—Ç —Å BERT, GPT-2, T5\n",
    "- –°—Ä–∞–≤–Ω–µ—Ç–µ –±—Ä–æ—è —Ç–æ–∫–µ–Ω–∏ –∏ –∫–∞–∫–≤–∏ —Å–∞ —Ç–µ\n",
    "- –ö–æ–π —Ä–∞–±–æ—Ç–∏ –Ω–∞–π-–¥–æ–±—Ä–µ –∑–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∏ —Ç–µ–∫—Å—Ç?\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 3: –ê–Ω–∞–ª–∏–∑ –Ω–∞ multilingual bias**\n",
    "- –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–∞–π—Ç–µ –µ–¥–Ω–∞–∫–≤–æ –¥—ä–ª–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤–µ –Ω–∞ 5 –µ–∑–∏–∫–∞\n",
    "- –ò–∑—á–∏—Å–ª–µ—Ç–µ —Å—Ä–µ–¥–Ω–∏—è –±—Ä–æ–π —Ç–æ–∫–µ–Ω–∏ –Ω–∞ —Å–∏–º–≤–æ–ª\n",
    "- –ö–∞–∫–≤–∏ —Å–∞ –ø–æ—Å–ª–µ–¥–∏—Ü–∏—Ç–µ –∑–∞ API costs?\n",
    "\n",
    "**–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 4: Train custom tokenizer**\n",
    "- –ò–∑–ø–æ–ª–∑–≤–∞–π—Ç–µ HuggingFace tokenizers –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ç–∞\n",
    "- –û–±—É—á–µ—Ç–µ BPE tokenizer –Ω–∞ –¥–æ–º–µ–π–Ω-—Å–ø–µ—Ü–∏—Ñ–∏—á–µ–Ω —Ç–µ–∫—Å—Ç (–∫–æ–¥, –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏, –ø—Ä–∞–≤–µ–Ω)\n",
    "- –°—Ä–∞–≤–Ω–µ—Ç–µ —Å GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "end",
   "metadata": {},
   "source": [
    "---\n",
    "## –ö—Ä–∞–π –Ω–∞ –õ–µ–∫—Ü–∏—è 3\n",
    "\n",
    "### –ë–ª–∞–≥–æ–¥–∞—Ä—è –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ—Ç–æ!\n",
    "\n",
    "**–í—ä–ø—Ä–æ—Å–∏?**\n",
    "\n",
    "---\n",
    "\n",
    "**–°–ª–µ–¥–≤–∞—â–∞ –ª–µ–∫—Ü–∏—è:** –ù–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏ (Neurons, Layers, Backpropagation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
