---
marp: true
theme: default
paginate: true
math: mathjax
---

# Ğ›ĞµĞºÑ†Ğ¸Ñ 9: Ğ›Ğ¾ĞºĞ°Ğ»Ğ½Ğ¸ LLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

## Quantization, Hardware Ğ¸ Deployment

---

# Ğ¦ĞµĞ»Ğ¸ Ğ½Ğ° Ğ»ĞµĞºÑ†Ğ¸ÑÑ‚Ğ°

- Ğ—Ğ°Ñ‰Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»Ğ½Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” privacy, Ñ†ĞµĞ½Ğ°, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»
- Quantization â€” ĞºĞ°Ğº Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ‚Ğµ Ğ¿Ğ¾-Ğ¼Ğ°Ğ»ĞºĞ¸
- Hardware â€” ĞºĞ°ĞºĞ²Ğ¾ Ğ½Ğ¸ Ñ‚Ñ€ÑĞ±Ğ²Ğ° Ğ·Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
- Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¸ â€” Ollama, vLLM, llama.cpp
- Ğ˜Ğ·Ğ±Ğ¾Ñ€ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ» ÑĞ¿Ğ¾Ñ€ĞµĞ´ use case

---

# Ğ§Ğ°ÑÑ‚ 1: ĞŸÑ€ĞµĞ´Ğ¸Ğ·Ğ²Ğ¸ĞºĞ°Ñ‚ĞµĞ»ÑÑ‚Ğ²Ğ¾Ñ‚Ğ¾

---

# Ğ—Ğ°Ñ‰Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»Ğ½Ğ¸ LLM?

| ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° | ĞĞ±ÑÑĞ½ĞµĞ½Ğ¸Ğµ |
|---------|-----------|
| **Privacy** | Ğ”Ğ°Ğ½Ğ½Ğ¸Ñ‚Ğµ Ğ½Ğµ Ğ½Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ°Ñ‚Ğ° |
| **Ğ¦ĞµĞ½Ğ°** | Ğ‘ĞµĞ· API Ñ‚Ğ°ĞºÑĞ¸ Ğ¿Ñ€Ğ¸ Ğ³Ğ¾Ğ»ÑĞ¼ Ğ¾Ğ±ĞµĞ¼ |
| **Latency** | ĞÑĞ¼Ğ° network overhead |
| **ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»** | Ğ‘ĞµĞ· content filters |
| **Offline** | Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚ |

---

# Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»Ğ½Ğ¸ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼

```
LLaMA 70B (full precision):
  70B Ã— 4 bytes = 280 GB RAM

Ğ¢Ğ¸Ğ¿Ğ¸Ñ‡ĞµĞ½ laptop:
  16 GB RAM

Gap: 17.5x ğŸ˜±
```

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:** Quantization + Hardware optimization

---

# Ğ¢Ñ€Ğ¸ ÑÑ‚ÑŠĞ»Ğ±Ğ° Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ĞµĞ½ inference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Local LLM                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Quantizationâ”‚  Efficient  â”‚    Hardware     â”‚
â”‚             â”‚  Inference  â”‚   Utilization   â”‚
â”‚  (Ğ¿Ğ¾-Ğ¼Ğ°Ğ»ĞºĞ¸  â”‚ (Ğ¿Ğ¾-Ğ±ÑŠÑ€Ğ·Ğ¾   â”‚  (Ğ¸Ğ·Ğ¿Ğ¾Ğ»Ğ·Ğ²Ğ°Ğ¹     â”‚
â”‚   Ñ‚ĞµĞ³Ğ»Ğ°)    â”‚  Ğ¸Ğ·Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ)â”‚   Ğ²ÑĞ¸Ñ‡ĞºĞ¾)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Ğ§Ğ°ÑÑ‚ 2: Memory Requirements

---

# ĞšÑŠĞ´Ğµ Ğ¾Ñ‚Ğ¸Ğ²Ğ° Ğ¿Ğ°Ğ¼ĞµÑ‚Ñ‚Ğ°?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Total Memory                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Weights     â”‚     KV Cache          â”‚
â”‚    (static)    â”‚    (dynamic)          â”‚
â”‚                â”‚                       â”‚
â”‚  Ğ—Ğ°Ğ²Ğ¸ÑĞ¸ Ğ¾Ñ‚     â”‚  Ğ Ğ°ÑÑ‚Ğµ Ñ context      â”‚
â”‚  model size    â”‚  length               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ĞŸÑ€ĞµÑĞ¼ÑÑ‚Ğ°Ğ½Ğµ Ğ½Ğ° Ğ¿Ğ°Ğ¼ĞµÑ‚ Ğ·Ğ° weights

**Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°:**
```
Memory = Parameters Ã— Bytes per Parameter
```

| Model | FP32 | FP16 | INT8 | INT4 |
|-------|------|------|------|------|
| 7B    | 28 GB | 14 GB | 7 GB | 3.5 GB |
| 13B   | 52 GB | 26 GB | 13 GB | 6.5 GB |
| 70B   | 280 GB | 140 GB | 70 GB | 35 GB |

INT4 = 8x Ğ¿Ğ¾-Ğ¼Ğ°Ğ»ĞºĞ¾ Ğ¾Ñ‚ FP32!

---

# KV Cache: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ°Ñ‚Ğ° Ñ‡Ğ°ÑÑ‚

$$\text{KV Cache} = 2 \times L \times H \times D \times S \times B$$

- $L$ = layers (Ğ½Ğ°Ğ¿Ñ€. 32)
- $H$ = heads (Ğ½Ğ°Ğ¿Ñ€. 32)
- $D$ = head dimension (Ğ½Ğ°Ğ¿Ñ€. 128)
- $S$ = sequence length
- $B$ = batch size

**7B Ğ¼Ğ¾Ğ´ĞµĞ», 4096 tokens:** ~4 GB Ğ´Ğ¾Ğ¿ÑŠĞ»Ğ½Ğ¸Ñ‚ĞµĞ»Ğ½Ğ¾

---

# Memory Bandwidth Bottleneck

**ĞšĞ»ÑÑ‡Ğ¾Ğ² insight:** LLM inference Ğµ **memory-bound**, Ğ½Ğµ compute-bound

```
Tokens/sec â‰ˆ Memory Bandwidth / Model Size
```

| Hardware | Bandwidth | 7B INT4 | 70B INT4 |
|----------|-----------|---------|----------|
| DDR4 RAM | 50 GB/s   | 14 t/s  | 1.4 t/s  |
| Apple M2 | 100 GB/s  | 28 t/s  | 2.8 t/s  |
| RTX 4090 | 1000 GB/s | 285 t/s | 28 t/s   |

---

# Ğ§Ğ°ÑÑ‚ 3: Quantization

---

# ĞšĞ°ĞºĞ²Ğ¾ Ğµ Quantization?

Mapping Ğ¾Ñ‚ high-precision ĞºÑŠĞ¼ low-precision:

```
FP32:  -3.14159265...  (32 bits)
  â†“
INT8:  -3             (8 bits)
```

**Ğ¦ĞµĞ»:** Ğ—Ğ°Ğ¿Ğ°Ğ·Ğ¸ ĞºĞ¾Ğ»ĞºĞ¾Ñ‚Ğ¾ ÑĞµ Ğ¼Ğ¾Ğ¶Ğµ Ğ¿Ğ¾Ğ²ĞµÑ‡Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾-Ğ¼Ğ°Ğ»ĞºĞ¾ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğµ

---

# Linear Quantization

**Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°:**
$$x_q = \text{round}\left(\frac{x - z}{s}\right)$$

- $s$ = scale factor
- $z$ = zero point

**Dequantization:**
$$\hat{x} = s \cdot x_q + z$$

---

# Quantization Granularity

```
Per-Tensor:        Ğ•Ğ´Ğ¸Ğ½ scale Ğ·Ğ° Ñ†ĞµĞ»Ğ¸Ñ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€
                   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
                         scale = 0.5

Per-Channel:       Scale Ğ·Ğ° Ğ²ÑĞµĞºĞ¸ ĞºĞ°Ğ½Ğ°Ğ»
                   [â–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆ]
                    s=0.3  s=0.5  s=0.7

Per-Group:         Scale Ğ·Ğ° Ğ³Ñ€ÑƒĞ¿Ğ° ÑÑ‚Ğ¾Ğ¹Ğ½Ğ¾ÑÑ‚Ğ¸
                   [â–ˆâ–ˆ][â–ˆâ–ˆ] [â–ˆâ–ˆ][â–ˆâ–ˆ] [â–ˆâ–ˆ][â–ˆâ–ˆ]
                   s1  s2   s3  s4   s5  s6
```

Per-group Ğµ Ğ½Ğ°Ğ¹-Ñ‚Ğ¾Ñ‡Ğ½Ğ¾, Ğ½Ğ¾ Ğ¿Ğ¾-Ğ±Ğ°Ğ²Ğ½Ğ¾

---

# Bit Levels Ğ¸ Trade-offs

| Bits | Ğ Ğ°Ğ·Ğ¼ĞµÑ€ | ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ | Use case |
|------|--------|----------|----------|
| FP16 | 2x Ğ¿Ğ¾-Ğ¼Ğ°Ğ»ĞºĞ¾ | ~100% | Training, inference |
| INT8 | 4x Ğ¿Ğ¾-Ğ¼Ğ°Ğ»ĞºĞ¾ | ~99% | Production |
| INT4 | 8x Ğ¿Ğ¾-Ğ¼Ğ°Ğ»ĞºĞ¾ | ~95-98% | Consumer hardware |
| INT3 | 10.7x | ~90-95% | Edge devices |
| INT2 | 16x | ~80-90% | Experimental |

**Sweet spot:** INT4 (Q4) Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµÑ‡ĞµÑ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¸

---

# Advanced Quantization Methods

**GPTQ (2022)**
- Post-training quantization
- Layer-by-layer, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ° reconstruction error
- ĞŸĞ¾Ğ¿ÑƒĞ»ÑÑ€ĞµĞ½ Ğ·Ğ° 4-bit Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

**AWQ (2023)**
- Activation-aware
- ĞŸĞ°Ğ·Ğ¸ "Ğ²Ğ°Ğ¶Ğ½Ğ¸Ñ‚Ğµ" weights Ñ Ğ¿Ğ¾-Ğ²Ğ¸ÑĞ¾ĞºĞ° precision
- ĞŸĞ¾-Ğ´Ğ¾Ğ±Ñ€Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ GPTQ

---

# GGUF K-quants

llama.cpp Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ mixed precision:

| Quant | Bits | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|-------|------|----------|
| Q4_K_S | ~4.5 | Smallest 4-bit |
| Q4_K_M | ~4.8 | Medium 4-bit (recommended) |
| Q5_K_S | ~5.5 | Smallest 5-bit |
| Q5_K_M | ~5.8 | Medium 5-bit |
| Q6_K | ~6.6 | Best quality |

**K = Ğ²Ğ°Ğ¶Ğ½Ğ¸Ñ‚Ğµ ÑĞ»Ğ¾ĞµĞ²Ğµ ÑĞ° Ğ¿Ğ¾-Ğ¼Ğ°Ğ»ĞºĞ¾ quantized**

---

# ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ vs Ğ Ğ°Ğ·Ğ¼ĞµÑ€

```
Perplexity â†‘
    â”‚
    â”‚  Q2 â—
    â”‚
    â”‚      Q3 â—
    â”‚
    â”‚           Q4 â—
    â”‚              Q5 â—
    â”‚                 Q6 â— Q8 â— FP16 â—
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Size
```

**Q4_K_M** Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»Ğ½Ğ¸ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸Ñ Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµÑ‡ĞµÑ‚Ğ¾

---

# ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑŠĞ²ĞµÑ‚Ğ¸

| Use case | ĞŸÑ€ĞµĞ¿Ğ¾Ñ€ÑŠÑ‡Ğ¸Ñ‚ĞµĞ»Ğ½Ğ¾ |
|----------|----------------|
| ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ĞµĞ½ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ | Q2_K, Q3_K |
| Consumer GPU (8GB) | Q4_K_M |
| Consumer GPU (16GB+) | Q5_K_M, Q6_K |
| Server | Q8_0, FP16 |
| Code completion | Q5+ (precision matters) |

---

# Ğ§Ğ°ÑÑ‚ 4: Ğ”Ñ€ÑƒĞ³Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

---

# KV Cache Optimization

**Grouped Query Attention (GQA)**
- Ğ¡Ğ¿Ğ¾Ğ´ĞµĞ»Ñ KV heads Ğ¼ĞµĞ¶Ğ´Ñƒ Q heads
- 8x Ğ¿Ğ¾-Ğ¼Ğ°Ğ»ÑŠĞº KV cache

**Sliding Window Attention**
- Ğ’Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ°Ğ¼Ğ¾ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ‚Ğµ N Ñ‚Ğ¾ĞºĞµĞ½Ğ¸
- ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚ĞµĞ½ KV cache size

**PagedAttention (vLLM)**
- Virtual memory Ğ·Ğ° KV cache
- ĞŸĞ¾-Ğ´Ğ¾Ğ±Ñ€Ğ¾ batch utilization

---

# Flash Attention

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼:** Standard attention Ğµ $O(n^2)$ memory

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:** Tiling + recomputation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Standard: Load all Q,K,Vâ”‚
â”‚ Compute full attention  â”‚
â”‚ Store full result       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flash: Load blocks      â”‚
â”‚ Compute partial         â”‚
â”‚ Accumulate on-the-fly   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

2-4x Ğ¿Ğ¾-Ğ±ÑŠÑ€Ğ·Ğ¾, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾-Ğ¼Ğ°Ğ»ĞºĞ¾ memory

---

# Speculative Decoding

```
Draft Model (Ğ±ÑŠÑ€Ğ·, Ğ¼Ğ°Ğ»ÑŠĞº)     Target Model (Ğ±Ğ°Ğ²ĞµĞ½, Ğ³Ğ¾Ğ»ÑĞ¼)
         â”‚                              â”‚
    Generate N                     Verify N
    tokens Ğ±ÑŠÑ€Ğ·Ğ¾                   tokens Ğ½Ğ°Ğ²ĞµĞ´Ğ½ÑŠĞ¶
         â”‚                              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              Keep valid tokens
              Regenerate from error
```

**Speedup:** 2-3x ĞºĞ¾Ğ³Ğ°Ñ‚Ğ¾ acceptance rate Ğµ Ğ²Ğ¸ÑĞ¾Ğº

---

# Mixture of Experts (MoE)

```
Input â†’ Router â†’ Expert 1 â”€â”
                 Expert 2 â”€â”€â”¼â†’ Output
                 Expert 3 â”€â”€â”˜
                 Expert 4  (inactive)
                 ...
```

- Ğ¡Ğ°Ğ¼Ğ¾ 2-4 experts Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸ Ğ¾Ñ‚ 8-64 total
- ĞŸĞ¾Ğ²ĞµÑ‡Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸, ÑÑŠÑ‰Ğ¸ÑÑ‚ compute
- ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: Mixtral 8x7B = 47B params, 13B active

---

# Ğ§Ğ°ÑÑ‚ 5: Hardware

---

# ĞšĞ»ÑÑ‡Ğ¾Ğ²Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ

```
Priority:
1. Memory (VRAM/RAM)   â† ĞœĞ¾Ğ´ĞµĞ» Ñ‚Ñ€ÑĞ±Ğ²Ğ° Ğ´Ğ° ÑĞµ ÑÑŠĞ±ĞµÑ€Ğµ
2. Bandwidth           â† ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ tokens/sec
3. Compute            â† Ğ ÑĞ´ĞºĞ¾ Ğµ bottleneck
```

**ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ¾:** ĞŸÑŠÑ€Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸ Ğ´Ğ°Ğ»Ğ¸ ÑĞµ ÑÑŠĞ±Ğ¸Ñ€Ğ°,
Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ¹ Ğ·Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚

---

# CPU-Only Inference

**ĞšĞ¾Ğ³Ğ°:**
- ĞÑĞ¼Ğ°Ñˆ GPU
- ĞœĞ¾Ğ´ĞµĞ» Ğ½Ğµ ÑĞµ ÑÑŠĞ±Ğ¸Ñ€Ğ° Ğ² VRAM
- Background tasks

**ĞÑ‡Ğ°ĞºĞ²Ğ°Ğ½Ğ¸Ñ:**
- 7B Q4: 5-15 tokens/sec
- 13B Q4: 2-8 tokens/sec
- 70B Q4: 0.5-2 tokens/sec

**Tools:** llama.cpp, Ollama

---

# Consumer GPUs

| GPU | VRAM | Max Model (Q4) | Speed |
|-----|------|----------------|-------|
| RTX 3060 | 12GB | 7B | 30-50 t/s |
| RTX 3090 | 24GB | 13B | 40-60 t/s |
| RTX 4090 | 24GB | 13B | 80-120 t/s |

**Hybrid offloading:** Part GPU + Part CPU
- ĞŸĞ¾-Ğ±Ğ°Ğ²Ğ½Ğ¾ Ğ¾Ñ‚ pure GPU
- ĞĞ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞ²Ğ° Ğ¿Ğ¾-Ğ³Ğ¾Ğ»ĞµĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

---

# Apple Silicon

**ĞŸÑ€ĞµĞ´Ğ¸Ğ¼ÑÑ‚Ğ²Ğ¾:** Unified Memory Architecture (UMA)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Unified Memory       â”‚
â”‚   CPU â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ GPU   â”‚
â”‚         (shared)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Chip | RAM | Max Model | Speed |
|------|-----|-----------|-------|
| M1 | 16GB | 7B Q4 | 15-25 t/s |
| M2 Pro | 32GB | 13B Q4 | 20-35 t/s |
| M3 Max | 128GB | 70B Q4 | 15-25 t/s |

---

# Server Hardware

| GPU | VRAM | Use Case |
|-----|------|----------|
| A100 | 40/80GB | Training + Inference |
| H100 | 80GB | State of the art |
| RTX A6000 | 48GB | Inference focused |

**Multi-GPU:** Tensor parallelism Ğ·Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»ĞµĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

---

# Hardware Selection Guide

```
Budget Model?           Use Case?
    â”‚                      â”‚
    â”œâ”€ < $500 â”€â”€â†’ CPU only (Ollama)
    â”‚
    â”œâ”€ $500-1500 â”€â”€â†’ RTX 3060/4060 (7B)
    â”‚
    â”œâ”€ $1500-3000 â”€â”€â†’ RTX 4090 (13B)
    â”‚
    â””â”€ $3000+ â”€â”€â†’ Mac Studio Ğ¸Ğ»Ğ¸ Server
```

---

# Ğ§Ğ°ÑÑ‚ 6: Model Sizes Ğ¸ Use Cases

---

# Ğ¡Ğ¿ĞµĞºÑ‚ÑŠÑ€ÑŠÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚Ğµ

```
 1B         7B         13B        34B        70B       405B
  â”‚          â”‚          â”‚          â”‚          â”‚          â”‚
Small    Medium     Medium+    Large     X-Large   Frontier
  â”‚          â”‚          â”‚          â”‚          â”‚          â”‚
Fast     General    Better     Strong    Premium   Best
cheap    purpose    quality    reasoning quality   available
```

---

# Small Models (1-3B)

**Ğ¡Ğ¸Ğ»Ğ½Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸:**
- ĞœĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑŠÑ€Ğ·Ğ¸ (100+ t/s Ğ½Ğ° GPU)
- Ğ Ğ°Ğ±Ğ¾Ñ‚ÑÑ‚ Ğ½Ğ° Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ¸
- Ğ”Ğ¾Ğ±Ñ€Ğ¸ Ğ·Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸

**Use cases:**
- Code completion (Ğ² IDE)
- Embeddings
- Classification
- Edge deployment

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ğ¸:** Phi-2, TinyLlama, Gemma 2B

---

# Medium Models (7-13B)

**Ğ¡Ğ¸Ğ»Ğ½Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸:**
- Ğ”Ğ¾Ğ±ÑŠÑ€ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾/ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚
- Ğ Ğ°Ğ±Ğ¾Ñ‚ÑÑ‚ Ğ½Ğ° consumer hardware
- General purpose

**Use cases:**
- RAG chatbots
- Summarization
- Translation
- General assistance

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ğ¸:** LLaMA 3 8B, Mistral 7B, Qwen2 7B

---

# Large Models (34-70B)

**Ğ¡Ğ¸Ğ»Ğ½Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸:**
- Ğ¡Ğ¸Ğ»Ğ½Ğ¾ reasoning
- Ğ¡Ğ»ĞµĞ´Ğ²Ğ°Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸
- ĞŸĞ¾-Ğ¼Ğ°Ğ»ĞºĞ¾ hallucinations

**Use cases:**
- Code generation
- Complex analysis
- Enterprise applications
- ĞšĞ¾Ğ³Ğ°Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ñ‚Ğ¾ Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ğ¸:** LLaMA 3 70B, Qwen2 72B, DeepSeek 67B

---

# ĞšĞ°Ğº Ğ´Ğ° Ğ¸Ğ·Ğ±ĞµÑ€ĞµĞ¼?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ‚Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°?                           â”‚
â”‚       â”‚                                     â”‚
â”‚   Ğ”Ğ° â†â”´â†’ ĞĞµ                                 â”‚
â”‚   â”‚      â”‚                                  â”‚
â”‚  1-7B    ĞÑƒĞ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğµ reasoning?              â”‚
â”‚              â”‚                              â”‚
â”‚          Ğ”Ğ° â†â”´â†’ ĞĞµ                          â”‚
â”‚          â”‚      â”‚                           â”‚
â”‚       34-70B   7-13B                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Quality vs Latency Trade-off

| ĞœĞ¾Ğ´ĞµĞ» | First Token | Tokens/sec | Quality |
|-------|-------------|------------|---------|
| 3B Q4 | 50ms | 100+ | â­â­ |
| 7B Q4 | 100ms | 50-80 | â­â­â­ |
| 13B Q4 | 200ms | 30-50 | â­â­â­â­ |
| 70B Q4 | 500ms | 10-20 | â­â­â­â­â­ |

Ğ—Ğ° interactive: Ğ¿ÑŠÑ€Ğ²Ğ¸Ñ‚Ğµ ÑĞ° Ğ¿Ğ¾-Ğ´Ğ¾Ğ±Ñ€Ğ¸
Ğ—Ğ° batch processing: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ñ‚Ğ¾ Ğ¸Ğ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚

---

# Ğ§Ğ°ÑÑ‚ 7: Deployment Tools

---

# llama.cpp

**ĞšĞ°ĞºĞ²Ğ¾ Ğµ:** C++ inference engine Ğ·Ğ° GGUF Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

**ĞšĞ¾Ğ³Ğ° Ğ´Ğ° Ğ³Ğ¾ Ğ¸Ğ·Ğ¿Ğ¾Ğ»Ğ·Ğ²Ğ°Ñˆ:**
- ĞÑƒĞ¶ĞµĞ½ Ğµ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ĞµĞ½ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»
- Custom integration
- Ğ ĞµÑÑƒÑ€ÑĞ½Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ° ÑÑ€ĞµĞ´Ğ°

```bash
./main -m model.gguf -p "Hello" -n 100
```

---

# Ollama

**ĞšĞ°ĞºĞ²Ğ¾ Ğµ:** Docker-like experience Ğ·Ğ° LLMs

```bash
# Install
curl -fsSL https://ollama.ai/install.sh | sh

# Pull Ğ¼Ğ¾Ğ´ĞµĞ»
ollama pull llama3:8b

# Run
ollama run llama3:8b

# API
curl http://localhost:11434/api/generate \
  -d '{"model": "llama3:8b", "prompt": "Hello"}'
```

---

# Ollama: Ğ—Ğ°Ñ‰Ğ¾ Ğµ Ğ´Ğ¾Ğ±ÑŠÑ€?

âœ… Ğ›ĞµÑĞµĞ½ setup (ĞµĞ´Ğ½Ğ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°)
âœ… Model management (pull, rm, list)
âœ… OpenAI-compatible API
âœ… Automatic GPU detection
âœ… Model library (ollama.ai/library)

âŒ ĞĞµ Ğµ Ğ·Ğ° production throughput
âŒ ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ batch capabilities

---

# vLLM

**ĞšĞ°ĞºĞ²Ğ¾ Ğµ:** High-throughput inference server

**ĞšĞ¾Ğ³Ğ° Ğ´Ğ° Ğ³Ğ¾ Ğ¸Ğ·Ğ¿Ğ¾Ğ»Ğ·Ğ²Ğ°Ñˆ:**
- Production deployment
- ĞœĞ½Ğ¾Ğ³Ğ¾ concurrent requests
- ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ĞµĞ½ throughput

```python
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-3-8B")
outputs = llm.generate(["Hello"], SamplingParams())
```

---

# vLLM Features

- **PagedAttention:** Ğ•Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ memory management
- **Continuous batching:** ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ĞµĞ½ GPU utilization
- **Tensor parallelism:** Multi-GPU support
- **OpenAI-compatible API**

```bash
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3-8B
```

---

# Tool Selection Guide

| ĞÑƒĞ¶Ğ´Ğ° | Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ |
|-------|------------|
| Personal use | **Ollama** |
| Integration | **llama.cpp** |
| Production API | **vLLM** |
| Apple Silicon | **Ollama** Ğ¸Ğ»Ğ¸ **mlx** |
| Maximum control | **llama.cpp** |

---

# Ğ§Ğ°ÑÑ‚ 8: Deployment Patterns

---

# Pattern 1: Single User Local

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Your Machine            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Ollama  â”‚ â†â†’ â”‚  App    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:** Full privacy, no cost, offline
**Cons:** Limited by your hardware

---

# Pattern 2: Team Server

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GPU Server              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  vLLM   â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”˜
                              â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚           â”‚            â”‚            â”‚
   User 1     User 2      User 3      User N
```

**Pros:** Better utilization, consistent experience
**Cons:** Need server management

---

# Pattern 3: Hybrid Local + Cloud

```
Simple queries    Complex queries
     â”‚                  â”‚
     â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Local   â”‚      â”‚  Cloud API  â”‚
â”‚ 7B      â”‚      â”‚  (GPT-4)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                  â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
       Router Logic
```

**Pros:** Cost optimization, best of both worlds
**Cons:** More complex architecture

---

# Pattern 4: Model Routing

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Incoming   â”€â”€â”€â”€â”€â”€â”€â†’ â”‚   Router    â”‚
Request             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚               â”‚               â”‚
           â–¼               â–¼               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Simple  â”‚    â”‚ Medium  â”‚    â”‚ Complex â”‚
      â”‚  3B     â”‚    â”‚  13B    â”‚    â”‚  70B    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ĞšĞ»Ğ°ÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ°Ğ¹ Ğ·Ğ°ÑĞ²ĞºĞ°Ñ‚Ğ° â†’ Ğ˜Ğ·Ğ¿Ñ€Ğ°Ñ‚Ğ¸ Ğ´Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ Ğ¼Ğ¾Ğ´ĞµĞ»

---

# ĞĞ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ

---

# ĞšĞ»ÑÑ‡Ğ¾Ğ²Ğ¸ Ğ¸Ğ´ĞµĞ¸

1. **Quantization Ğµ ĞºĞ»ÑÑ‡ÑŠÑ‚**
   - Q4 Ğ¿Ñ€Ğ°Ğ²Ğ¸ 70B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑŠĞ¿Ğ½Ğ¸ Ğ½Ğ° consumer hardware
   - Trade-off ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾/Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼

2. **VRAM/RAM Ğµ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ñ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ**
   - Memory bandwidth Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ñ‚Ğ°

3. **Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ» ÑĞ¿Ğ¾Ñ€ĞµĞ´ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ‚Ğ°**
   - Small Ğ·Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚, large Ğ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾

4. **Tools:** Ollama Ğ·Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ°, vLLM Ğ·Ğ° production

---

# ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Quick Start

```bash
# 1. Ğ˜Ğ½ÑÑ‚Ğ°Ğ»Ğ¸Ñ€Ğ°Ğ¹ Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Ğ˜Ğ·Ñ‚ĞµĞ³Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»
ollama pull llama3:8b

# 3. ĞŸÑ€Ğ¾Ğ±Ğ²Ğ°Ğ¹
ollama run llama3:8b "Explain quantum computing"

# 4. Ğ˜Ğ·Ğ¿Ğ¾Ğ»Ğ·Ğ²Ğ°Ğ¹ API
curl http://localhost:11434/api/generate \
  -d '{"model":"llama3:8b","prompt":"Hello"}'
```

---

# Ğ¡Ğ»ĞµĞ´Ğ²Ğ°Ñ‰Ğ° Ğ»ĞµĞºÑ†Ğ¸Ñ

## Ğ›ĞµĞºÑ†Ğ¸Ñ 10: Advanced Prompting Ğ¸ Reasoning Models

- Chain-of-Thought Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¸
- Few-shot vs Zero-shot
- Reasoning models (o1, DeepSeek R1)
- Prompt engineering best practices

---

# Ğ ĞµÑÑƒÑ€ÑĞ¸

**Papers:**
- Frantar et al. (2022) â€” GPTQ
- Lin et al. (2023) â€” AWQ
- Kwon et al. (2023) â€” vLLM/PagedAttention

**Tools:**
- ollama.ai
- github.com/ggerganov/llama.cpp
- github.com/vllm-project/vllm

---

# Ğ’ÑŠĞ¿Ñ€Ğ¾ÑĞ¸?

